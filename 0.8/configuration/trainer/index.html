
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
  
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Declarative machine learning: End-to-end machine learning pipelines using data-driven configurations.">
      
      
        <meta name="author" content="Piero Molino">
      
      
        <link rel="canonical" href="https://ludwig.ai/latest/configuration/trainer/">
      
      
        <link rel="prev" href="../combiner/">
      
      
        <link rel="next" href="../hyperparameter_optimization/">
      
      <link rel="icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.14">
    
  <meta content="http://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/og-image.jpg"
    property="og:image">
  <meta content="https://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/og-image.jpg"
    property="og:image:secure_url">

    
      
        <title>Trainer - Ludwig</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.113286f1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/monokai.css">
    
      <link rel="stylesheet" href="../../stylesheets/colorful.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-H8VVJF9L6G"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-H8VVJF9L6G",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-H8VVJF9L6G",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>

  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Ludwig" class="md-header__button md-logo" aria-label="Ludwig" data-md-component="logo">
      
<img alt="logo" src="../../images/ludwig_logo.svg"
     style="height:1rem;width:4rem;">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ludwig
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Trainer
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="grey" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ludwig-ai/ludwig" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
      </div>
    
  </nav>
  
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Ludwig" class="md-nav__button md-logo" aria-label="Ludwig" data-md-component="logo">
      
<img alt="logo" src="../../images/ludwig_logo.svg"
     style="height:1rem;width:4rem;">

    </a>
    Ludwig
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ludwig-ai/ludwig" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Ludwig
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../getting_started/">🚀 Getting Started</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          🚀 Getting Started
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/prepare_data/" class="md-nav__link">
        Dataset preparation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/train/" class="md-nav__link">
        Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/evaluate/" class="md-nav__link">
        Prediction and Evaluation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/hyperopt/" class="md-nav__link">
        Hyperopt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/serve/" class="md-nav__link">
        Serving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/ray/" class="md-nav__link">
        Distributed training on Ray
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/llm_finetuning/" class="md-nav__link">
        LLM Fine-tuning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/docker/" class="md-nav__link">
        Ludwig with Docker
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../user_guide/">📖 User Guide</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          📖 User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/what_is_ludwig/" class="md-nav__link">
        What is Ludwig?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/how_ludwig_works/" class="md-nav__link">
        How Ludwig Works
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/command_line_interface/" class="md-nav__link">
        Command Line Interface
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Python API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Python API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/api/LudwigModel/" class="md-nav__link">
        LudwigModel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/api/visualization/" class="md-nav__link">
        Visualization
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          Datasets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Datasets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/supported_formats/" class="md-nav__link">
        Supported Formats
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/data_preprocessing/" class="md-nav__link">
        Data Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/data_postprocessing/" class="md-nav__link">
        Data Postprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/dataset_zoo/" class="md-nav__link">
        Dataset Zoo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
        
          
            
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../user_guide/llms/">Large Language Models</a>
          
            <label for="__nav_3_7">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Large Language Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/llms/finetuning/" class="md-nav__link">
        Fine-Tuning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/llms/in_context_learning/" class="md-nav__link">
        In-Context Learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/gpus/" class="md-nav__link">
        GPUs
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
        
          
            
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../user_guide/distributed_training/">Distributed Training</a>
          
            <label for="__nav_3_9">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          Distributed Training
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/distributed_training/finetuning/" class="md-nav__link">
        Fine-Tuning Pretrained Models
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/hyperopt/" class="md-nav__link">
        Hyperparameter Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/cloud_storage/" class="md-nav__link">
        Cloud Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/automl/" class="md-nav__link">
        AutoML
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/visualizations/" class="md-nav__link">
        Visualizations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/model_export/" class="md-nav__link">
        Model Export
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/serving/" class="md-nav__link">
        Serving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/integrations/" class="md-nav__link">
        Third-Party Integrations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../">📚 Configuration</a>
          
            <label for="__nav_4">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          📚 Configuration
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../model_type/" class="md-nav__link">
        Model Types
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../large_language_model/" class="md-nav__link">
        Large Language Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          Features
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/supported_data_types/" class="md-nav__link">
        Supported Data Types
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/input_features/" class="md-nav__link">
        Input Features (↑)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/output_features/" class="md-nav__link">
        Output Features (↓)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/binary_features/" class="md-nav__link">
        ⇅ Binary Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/number_features/" class="md-nav__link">
        ⇅ Number Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/category_features/" class="md-nav__link">
        ⇅ Category Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/bag_features/" class="md-nav__link">
        ⇅ Bag Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/set_features/" class="md-nav__link">
        ⇅ Set Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/sequence_features/" class="md-nav__link">
        ⇅ Sequence Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/text_features/" class="md-nav__link">
        ⇅ Text Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/vector_features/" class="md-nav__link">
        ⇅ Vector Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/audio_features/" class="md-nav__link">
        ↑ Audio Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/date_features/" class="md-nav__link">
        ↑ Date Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/h3_features/" class="md-nav__link">
        ↑ H3 Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/image_features/" class="md-nav__link">
        ↑ Image Features
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../features/time_series_features/" class="md-nav__link">
        ↑ Time Series Features
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../defaults/" class="md-nav__link">
        Defaults
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../combiner/" class="md-nav__link">
        Combiner
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Trainer
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Trainer
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
    <nav class="md-nav" aria-label="Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#trainer-parameters" class="md-nav__link">
    Trainer parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-parameters" class="md-nav__link">
    Optimizer parameters
  </a>
  
    <nav class="md-nav" aria-label="Optimizer parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    sgd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd_8bit" class="md-nav__link">
    sgd_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lbfgs" class="md-nav__link">
    lbfgs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    adam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam_8bit" class="md-nav__link">
    adam_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adam" class="md-nav__link">
    paged_adam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adam_8bit" class="md-nav__link">
    paged_adam_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw" class="md-nav__link">
    adamw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw_8bit" class="md-nav__link">
    adamw_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adamw" class="md-nav__link">
    paged_adamw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adamw_8bit" class="md-nav__link">
    paged_adamw_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adadelta" class="md-nav__link">
    adadelta
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adagrad" class="md-nav__link">
    adagrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adagrad_8bit" class="md-nav__link">
    adagrad_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamax" class="md-nav__link">
    adamax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nadam" class="md-nav__link">
    nadam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop" class="md-nav__link">
    rmsprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop_8bit" class="md-nav__link">
    rmsprop_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lamb" class="md-nav__link">
    lamb
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lamb_8bit" class="md-nav__link">
    lamb_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lars" class="md-nav__link">
    lars
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lars_8bit" class="md-nav__link">
    lars_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lion" class="md-nav__link">
    lion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lion_8bit" class="md-nav__link">
    lion_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_lion" class="md-nav__link">
    paged_lion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_lion_8bit" class="md-nav__link">
    paged_lion_8bit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-length" class="md-nav__link">
    Training length
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#early-stopping" class="md-nav__link">
    Early stopping
  </a>
  
    <nav class="md-nav" aria-label="Early stopping">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-early-stopping-works-in-ludwig" class="md-nav__link">
    How early stopping works in Ludwig
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#changing-the-metric-early-stopping-metrics" class="md-nav__link">
    Changing the metric early stopping metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disabling-early-stopping" class="md-nav__link">
    Disabling early stopping
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkpoint-evaluation-frequency" class="md-nav__link">
    Checkpoint-evaluation frequency
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#increasing-throughput-on-gpus" class="md-nav__link">
    Increasing throughput on GPUs
  </a>
  
    <nav class="md-nav" aria-label="Increasing throughput on GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#increase-batch-size" class="md-nav__link">
    Increase batch size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-mixed-precision" class="md-nav__link">
    Use mixed precision
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../hyperparameter_optimization/" class="md-nav__link">
        Hyperopt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../backend/" class="md-nav__link">
        Backend
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
            
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../examples/">💡 Examples</a>
          
            <label for="__nav_5">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          💡 Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          LLMs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          LLMs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_classification/" class="md-nav__link">
        Fine-tuning for classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_finetuning/" class="md-nav__link">
        Instruction-tuning llama-2-7b
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_finetuning_deepspeed/" class="md-nav__link">
        Adapter-based encoder fine-tuning for text classification with deepspeed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_text_generation/" class="md-nav__link">
        Adapter-based fine-tuning for text generation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_zero_shot_text_generation/" class="md-nav__link">
        Zero-shot batch inference for text generation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_zero_shot_batch_inference/" class="md-nav__link">
        Zero-shot batch inference for text classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_few_shot_batch_inference/" class="md-nav__link">
        Few-shot batch inference for text classification (RAG)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_tabular_zero_shot_batch_inference/" class="md-nav__link">
        Zero-shot batch inference for tabular classification (TabLLM)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_tabular_classification/" class="md-nav__link">
        Fine-tuning for tabular classification (TabLLM)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Supervised ML
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Supervised ML
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/text_classification/" class="md-nav__link">
        Text Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/adult_census_income/" class="md-nav__link">
        Tabular Data Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/mnist/" class="md-nav__link">
        Image Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multimodal_classification/" class="md-nav__link">
        Multimodal Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/hyperopt/" class="md-nav__link">
        Hyperparameter Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/gbm_fraud/" class="md-nav__link">
        Fraud with GBMs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/sentiment_analysis/" class="md-nav__link">
        Sentiment Analysis
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
          Use Cases
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          Use Cases
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/ner_tagging/" class="md-nav__link">
        Named Entity Recognition Tagging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/nlu/" class="md-nav__link">
        Natural Language Understanding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/machine_translation/" class="md-nav__link">
        Machine Translation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/seq2seq/" class="md-nav__link">
        Chit-Chat Dialogue Modeling through Sequence2Sequence
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/sentiment_analysis/" class="md-nav__link">
        Sentiment Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/oneshot/" class="md-nav__link">
        One-shot Learning with Siamese Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/visual_qa/" class="md-nav__link">
        Visual Question Answering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/speech_recognition/" class="md-nav__link">
        Spoken Digit Speech Recognition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/speaker_verification/" class="md-nav__link">
        Speaker Verification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/titanic/" class="md-nav__link">
        Binary Classification (Titanic)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/forecasting/" class="md-nav__link">
        Timeseries forecasting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/weather/" class="md-nav__link">
        Timeseries forecasting (Weather)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/movie_ratings/" class="md-nav__link">
        Movie rating prediction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multi_label/" class="md-nav__link">
        Multi-label classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multi_task/" class="md-nav__link">
        Multi-Task Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/fuel_efficiency/" class="md-nav__link">
        Simple Regression - Fuel Efficiency Prediction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/fraud/" class="md-nav__link">
        Fraud Detection
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../developer_guide/">🛠️ Developer Guide</a>
          
            <label for="__nav_6">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          🛠️ Developer Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/contributing/" class="md-nav__link">
        How to Contribute
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/codebase_structure/" class="md-nav__link">
        Codebase Structure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/api_annotations/" class="md-nav__link">
        Ludwig API Guarantees
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_an_encoder/" class="md-nav__link">
        Add an Encoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_combiner/" class="md-nav__link">
        Add a Combiner
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_decoder/" class="md-nav__link">
        Add a Decoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_feature_type/" class="md-nav__link">
        Add a Feature Type
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_metric/" class="md-nav__link">
        Add a Metric
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_loss_function/" class="md-nav__link">
        Add a Loss Function
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_tokenizer/" class="md-nav__link">
        Add a Tokenizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_hyperopt/" class="md-nav__link">
        Add a Hyperopt Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_pretrained_model/" class="md-nav__link">
        Add a Pretrained Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_an_integration/" class="md-nav__link">
        Add an Integration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_dataset/" class="md-nav__link">
        Add a Dataset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/style_guidelines_and_tests/" class="md-nav__link">
        Style Guidelines and Tests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/unit_test_design_guidelines/" class="md-nav__link">
        Unit Test Design Guidelines
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/run_tests_on_gpu_using_ray/" class="md-nav__link">
        Run Tests on GPU Using Ray
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/release_process/" class="md-nav__link">
        Release Process
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../community/" class="md-nav__link">
        👋 Community
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        ❓ FAQ
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
    <nav class="md-nav" aria-label="Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#trainer-parameters" class="md-nav__link">
    Trainer parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-parameters" class="md-nav__link">
    Optimizer parameters
  </a>
  
    <nav class="md-nav" aria-label="Optimizer parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    sgd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd_8bit" class="md-nav__link">
    sgd_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lbfgs" class="md-nav__link">
    lbfgs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    adam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam_8bit" class="md-nav__link">
    adam_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adam" class="md-nav__link">
    paged_adam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adam_8bit" class="md-nav__link">
    paged_adam_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw" class="md-nav__link">
    adamw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamw_8bit" class="md-nav__link">
    adamw_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adamw" class="md-nav__link">
    paged_adamw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_adamw_8bit" class="md-nav__link">
    paged_adamw_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adadelta" class="md-nav__link">
    adadelta
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adagrad" class="md-nav__link">
    adagrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adagrad_8bit" class="md-nav__link">
    adagrad_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adamax" class="md-nav__link">
    adamax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nadam" class="md-nav__link">
    nadam
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop" class="md-nav__link">
    rmsprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop_8bit" class="md-nav__link">
    rmsprop_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lamb" class="md-nav__link">
    lamb
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lamb_8bit" class="md-nav__link">
    lamb_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lars" class="md-nav__link">
    lars
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lars_8bit" class="md-nav__link">
    lars_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lion" class="md-nav__link">
    lion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lion_8bit" class="md-nav__link">
    lion_8bit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_lion" class="md-nav__link">
    paged_lion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paged_lion_8bit" class="md-nav__link">
    paged_lion_8bit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-length" class="md-nav__link">
    Training length
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#early-stopping" class="md-nav__link">
    Early stopping
  </a>
  
    <nav class="md-nav" aria-label="Early stopping">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-early-stopping-works-in-ludwig" class="md-nav__link">
    How early stopping works in Ludwig
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#changing-the-metric-early-stopping-metrics" class="md-nav__link">
    Changing the metric early stopping metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disabling-early-stopping" class="md-nav__link">
    Disabling early stopping
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkpoint-evaluation-frequency" class="md-nav__link">
    Checkpoint-evaluation frequency
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#increasing-throughput-on-gpus" class="md-nav__link">
    Increasing throughput on GPUs
  </a>
  
    <nav class="md-nav" aria-label="Increasing throughput on GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#increase-batch-size" class="md-nav__link">
    Increase batch size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-mixed-precision" class="md-nav__link">
    Use mixed precision
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Trainer</h1>

<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>The <code>trainer</code> section of the configuration lets you specify parameters that
configure the training process, like the number of epochs or the learning rate.
By default, the ECD trainer is used.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:3"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><input id="__tabbed_1_3" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">ECD</label><label for="__tabbed_1_2">LLM</label><label for="__tabbed_1_3">GBM</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">early_stop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adam</span>
<span class="w">        </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">        </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">        </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">        </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">regularization_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">l2</span>
<span class="w">    </span><span class="nt">use_mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">compile</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">checkpoints_per_epoch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">effective_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">regularization_lambda</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">validation_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">validation_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">train_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">steps_per_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">max_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1099511627776</span>
<span class="w">    </span><span class="nt">eval_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">evaluate_training_set</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">should_shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau_patience</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2.0</span>
<span class="w">    </span><span class="nt">increase_batch_size_eval_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">loss</span>
<span class="w">    </span><span class="nt">increase_batch_size_eval_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">training</span>
<span class="w">    </span><span class="nt">gradient_clipping</span><span class="p">:</span>
<span class="w">        </span><span class="nt">clipglobalnorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">        </span><span class="nt">clipnorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">clipvalue</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">learning_rate_scaling</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">linear</span>
<span class="w">    </span><span class="nt">bucketing_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">skip_all_evaluation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">learning_rate_scheduler</span><span class="p">:</span>
<span class="w">        </span><span class="nt">decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">decay_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.96</span>
<span class="w">        </span><span class="nt">decay_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10000</span>
<span class="w">        </span><span class="nt">staircase</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">reduce_on_plateau</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">        </span><span class="nt">reduce_on_plateau_patience</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">        </span><span class="nt">reduce_on_plateau_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">        </span><span class="nt">warmup_evaluations</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">        </span><span class="nt">warmup_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">        </span><span class="nt">reduce_eval_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">loss</span>
<span class="w">        </span><span class="nt">reduce_eval_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">training</span>
<span class="w">        </span><span class="nt">t_0</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">t_mult</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">eta_min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">finetune</span>
<span class="w">    </span><span class="nt">early_stop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adam</span>
<span class="w">        </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">        </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">        </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">        </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">regularization_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">l2</span>
<span class="w">    </span><span class="nt">use_mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">compile</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">checkpoints_per_epoch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">effective_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
<span class="w">    </span><span class="nt">regularization_lambda</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">validation_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">validation_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">train_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">steps_per_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">max_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1099511627776</span>
<span class="w">    </span><span class="nt">eval_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">evaluate_training_set</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">should_shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau_patience</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">    </span><span class="nt">increase_batch_size_on_plateau_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2.0</span>
<span class="w">    </span><span class="nt">increase_batch_size_eval_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">loss</span>
<span class="w">    </span><span class="nt">increase_batch_size_eval_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">training</span>
<span class="w">    </span><span class="nt">gradient_clipping</span><span class="p">:</span>
<span class="w">        </span><span class="nt">clipglobalnorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">        </span><span class="nt">clipnorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">clipvalue</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">learning_rate_scaling</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">linear</span>
<span class="w">    </span><span class="nt">bucketing_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">skip_all_evaluation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">learning_rate_scheduler</span><span class="p">:</span>
<span class="w">        </span><span class="nt">decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">decay_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.96</span>
<span class="w">        </span><span class="nt">decay_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10000</span>
<span class="w">        </span><span class="nt">staircase</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">reduce_on_plateau</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">        </span><span class="nt">reduce_on_plateau_patience</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">        </span><span class="nt">reduce_on_plateau_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">        </span><span class="nt">warmup_evaluations</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">        </span><span class="nt">warmup_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">        </span><span class="nt">reduce_eval_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">loss</span>
<span class="w">        </span><span class="nt">reduce_eval_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">training</span>
<span class="w">        </span><span class="nt">t_0</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">t_mult</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">eta_min</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">base_learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">early_stop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.03</span>
<span class="w">    </span><span class="nt">max_depth</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">18</span>
<span class="w">    </span><span class="nt">boosting_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gbdt</span>
<span class="w">    </span><span class="nt">bagging_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.8</span>
<span class="w">    </span><span class="nt">feature_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.75</span>
<span class="w">    </span><span class="nt">extra_trees</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">lambda_l1</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.25</span>
<span class="w">    </span><span class="nt">lambda_l2</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">    </span><span class="nt">drop_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">    </span><span class="nt">tree_learner</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">serial</span>
<span class="w">    </span><span class="nt">boosting_rounds_per_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<span class="w">    </span><span class="nt">num_boost_round</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">    </span><span class="nt">num_leaves</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">82</span>
<span class="w">    </span><span class="nt">min_data_in_leaf</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<span class="w">    </span><span class="nt">pos_bagging_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">neg_bagging_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">bagging_freq</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">bagging_seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">    </span><span class="nt">feature_fraction_bynode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">feature_fraction_seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">    </span><span class="nt">extra_seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span>
<span class="w">    </span><span class="nt">linear_lambda</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">max_drop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<span class="w">    </span><span class="nt">skip_drop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">    </span><span class="nt">uniform_drop</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">drop_seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">validation_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">validation_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">eval_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1048576</span>
<span class="w">    </span><span class="nt">evaluate_training_set</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">min_sum_hessian_in_leaf</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">max_delta_step</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">min_gain_to_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.03</span>
<span class="w">    </span><span class="nt">xgboost_dart_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">top_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">    </span><span class="nt">other_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">    </span><span class="nt">min_data_per_group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">max_cat_threshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">    </span><span class="nt">cat_l2</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span>
<span class="w">    </span><span class="nt">cat_smooth</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span>
<span class="w">    </span><span class="nt">max_cat_to_onehot</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">cegb_tradeoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">cegb_penalty_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">path_smooth</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">verbose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="nt">max_bin</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">255</span>
<span class="w">    </span><span class="nt">feature_pre_filter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">skip_all_evaluation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
</div>
</div>
</div>
<h3 id="trainer-parameters">Trainer parameters<a class="headerlink" href="#trainer-parameters" title="Permanent link">&para;</a></h3>
<div class="tabbed-set tabbed-alternate" data-tabs="2:3"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><input id="__tabbed_2_3" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">ECD</label><label for="__tabbed_2_2">LLM</label><label for="__tabbed_2_3">GBM</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<ul>
<li><strong><code>early_stop</code></strong> (default: <code>5</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of consecutive rounds of evaluation without any improvement on the <code>validation_metric</code> that triggers training to stop. Can be set to -1, which disables early stopping entirely.</li>
<li><strong><code>learning_rate</code></strong> (default: <code>0.001</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Controls how much to change the model in response to the estimated error each time the model weights are updated. If 'auto', the optimal learning rate is estimated by choosing the learning rate that produces the smallest non-diverging gradient update.</li>
<li><strong><code>epochs</code></strong> (default: <code>100</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of epochs the algorithm is intended to be run over. Overridden if <code>train_steps</code> is set</li>
<li><strong><code>batch_size</code></strong> (default: <code>auto</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The number of training examples utilized in one training step of the model. If ’auto’, the batch size that maximized training throughput (samples / sec) will be used. For CPU training, the tuned batch size is capped at 128 as throughput benefits of large batch sizes are less noticeable without a GPU.</li>
<li><strong><code>optimizer</code></strong> (default: <code>{"type": "adam"}</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Optimizer type and its parameters. The optimizer is responsble for applying the gradients computed from the loss during backpropagation as updates to the model weights. See <a href="#optimizer-parameters">Optimizer parameters</a> for details.</li>
<li><strong><code>regularization_type</code></strong> (default: <code>l2</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Type of regularization. Options: <code>l1</code>, <code>l2</code>, <code>l1_l2</code>, <code>null</code>.</li>
<li><strong><code>use_mixed_precision</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Enable automatic mixed-precision (AMP) during training. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>compile</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Whether to compile the model before training. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>checkpoints_per_epoch</code></strong> (default: <code>0</code>): Number of checkpoints per epoch. For example, 2 -&gt; checkpoints are written every half of an epoch. Note that it is invalid to specify both non-zero <code>steps_per_checkpoint</code> and non-zero <code>checkpoints_per_epoch</code>.</li>
<li><strong><code>effective_batch_size</code></strong> (default: <code>auto</code>): The effective batch size is the total number of samples used to compute a single gradient update to the model weights. This differs from <code>batch_size</code> by taking <code>gradient_accumulation_steps</code> and number of training worker processes into account. In practice, <code>effective_batch_size = batch_size * gradient_accumulation_steps * num_workers</code>. If 'auto', the effective batch size is derivied implicitly from <code>batch_size</code>, but if set explicitly, then one of <code>batch_size</code> or <code>gradient_accumulation_steps</code> must be set to something other than 'auto', and consequently will be set following the formula given above.</li>
<li><strong><code>gradient_accumulation_steps</code></strong> (default: <code>auto</code>): Number of steps to accumulate gradients over before performing a weight update.</li>
<li><strong><code>regularization_lambda</code></strong> (default: <code>0.0</code>): Strength of the regularization.</li>
<li><strong><code>validation_field</code></strong> (default: <code>null</code>): The field for which the <code>validation_metric</code> is used for validation-related mechanics like early stopping, parameter change plateaus, as well as what hyperparameter optimization uses to determine the best trial. If unset (default), the first output feature is used. If explicitly specified, neither <code>validation_field</code> nor <code>validation_metric</code> are overwritten.</li>
<li><strong><code>validation_metric</code></strong> (default: <code>null</code>): Metric from <code>validation_field</code> that is used. If validation_field is not explicitly specified, this is overwritten to be the first output feature type's <code>default_validation_metric</code>, consistent with validation_field. If the validation_metric is specified, then we will use the first output feature that produces this metric as the <code>validation_field</code>.</li>
<li><strong><code>train_steps</code></strong> (default: <code>null</code>): Maximum number of training steps the algorithm is intended to be run over. Unset by default. If set, will override <code>epochs</code> and if left unset then <code>epochs</code> is used to determine training length.</li>
<li><strong><code>steps_per_checkpoint</code></strong> (default: <code>0</code>): How often the model is checkpointed. Also dictates maximum evaluation frequency. If 0 the model is checkpointed after every epoch.</li>
<li><strong><code>max_batch_size</code></strong> (default: <code>1099511627776</code>): Auto batch size tuning and increasing batch size on plateau will be capped at this value. The default value is 2^40.</li>
<li><strong><code>eval_batch_size</code></strong> (default: <code>null</code>): Size of batch to pass to the model for evaluation. If it is <code>0</code> or <code>None</code>, the same value of <code>batch_size</code> is used. This is useful to speedup evaluation with a much bigger batch size than training, if enough memory is available. If ’auto’, the biggest batch size (power of 2) that can fit in memory will be used.</li>
<li><strong><code>evaluate_training_set</code></strong> (default: <code>false</code>): Whether to evaluate on the entire training set during evaluation. By default, training metrics will be computed at the end of each training step, and accumulated up to the evaluation phase. In practice, computing training set metrics during training is up to 30% faster than running a separate evaluation pass over the training set, but results in more noisy training metrics, particularly during the earlier epochs. It's recommended to only set this to True if you need very exact training set metrics, and are willing to pay a significant performance penalty for them. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>should_shuffle</code></strong> (default: <code>true</code>): Whether to shuffle batches during training when true. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>increase_batch_size_on_plateau</code></strong> (default: <code>0</code>): The number of times to increase the batch size on a plateau.</li>
<li><strong><code>increase_batch_size_on_plateau_patience</code></strong> (default: <code>5</code>): How many epochs to wait for before increasing the batch size.</li>
<li><strong><code>increase_batch_size_on_plateau_rate</code></strong> (default: <code>2.0</code>): Rate at which the batch size increases.</li>
<li><strong><code>increase_batch_size_eval_metric</code></strong> (default: <code>loss</code>): Which metric to listen on for increasing the batch size.</li>
<li><strong><code>increase_batch_size_eval_split</code></strong> (default: <code>training</code>): Which dataset split to listen on for increasing the batch size.</li>
<li><strong><code>gradient_clipping</code></strong> : Parameter values for gradient clipping.</li>
<li><strong><code>gradient_clipping.clipglobalnorm</code></strong> (default: <code>0.5</code>): Maximum allowed norm of the gradients</li>
<li><strong><code>gradient_clipping.clipnorm</code></strong> (default: <code>null</code>): Maximum allowed norm of the gradients</li>
<li><strong><code>gradient_clipping.clipvalue</code></strong> (default: <code>null</code>): Maximum allowed value of the gradients</li>
<li><strong><code>learning_rate_scaling</code></strong> (default: <code>linear</code>): Scale by which to increase the learning rate as the number of distributed workers increases. Traditionally the learning rate is scaled linearly with the number of workers to reflect the proportion by which the effective batch size is increased. For very large batch sizes, a softer square-root scale can sometimes lead to better model performance. If the learning rate is hand-tuned for a given number of workers, setting this value to constant can be used to disable scale-up. Options: <code>constant</code>, <code>sqrt</code>, <code>linear</code>.</li>
<li><strong><code>bucketing_field</code></strong> (default: <code>null</code>): Feature to use for bucketing datapoints</li>
<li><strong><code>skip_all_evaluation</code></strong> (default: <code>false</code>): Whether to skip evaluation entirely. If you are training a model with a well-known configuration on a well-known dataset and are confident about the expected results, you might skip all evaluation. Moreover, evaluating a model, especially on large validation or test sets, can be time-consuming. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>learning_rate_scheduler</code></strong> : Parameter values for learning rate scheduler.</li>
<li><strong><code>learning_rate_scheduler.decay</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Turn on decay of the learning rate. Options: <code>linear</code>, <code>exponential</code>, <code>cosine</code>, <code>null</code>.</li>
<li><strong><code>learning_rate_scheduler.decay_rate</code></strong> (default: <code>0.96</code>): Decay per epoch (%): Factor to decrease the Learning rate.</li>
<li><strong><code>learning_rate_scheduler.decay_steps</code></strong> (default: <code>10000</code>): The number of steps to take in the exponential learning rate decay.</li>
<li><strong><code>learning_rate_scheduler.staircase</code></strong> (default: <code>false</code>): Decays the learning rate at discrete intervals. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau</code></strong> (default: <code>0</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: How many times to reduce the learning rate when the algorithm hits a plateau (i.e. the performance on the training set does not improve)</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau_patience</code></strong> (default: <code>10</code>): How many evaluation steps have to pass before the learning rate reduces when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau_rate</code></strong> (default: <code>0.1</code>): Rate at which we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.warmup_evaluations</code></strong> (default: <code>0</code>): Number of evaluation steps to warmup the learning rate for.</li>
<li><strong><code>learning_rate_scheduler.warmup_fraction</code></strong> (default: <code>0.0</code>): Fraction of total training steps to warmup the learning rate for.</li>
<li><strong><code>learning_rate_scheduler.reduce_eval_metric</code></strong> (default: <code>loss</code>): Metric plateau used to trigger when we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_eval_split</code></strong> (default: <code>training</code>): Which dataset split to listen on for reducing the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.t_0</code></strong> (default: <code>null</code>): Number of steps before the first restart for cosine annealing decay. If not specified, it will be set to <code>steps_per_checkpoint</code>.</li>
<li><strong><code>learning_rate_scheduler.t_mult</code></strong> (default: <code>1</code>): Period multiplier after each restart for cosine annealing decay. Defaults to 1, i.e., restart every <code>t_0</code> steps. If set to a larger value, the period between restarts increases by that multiplier. For e.g., if t_mult is 2, then the periods would be: t_0, 2<em>t_0, 2^2</em>t_0, 2^3*t_0, etc.</li>
<li><strong><code>learning_rate_scheduler.eta_min</code></strong> (default: <code>0</code>): Minimum learning rate allowed for cosine annealing decay. Default: 0.</li>
</ul>
</div>
<div class="tabbed-block">
<ul>
<li><strong><code>type</code></strong> (default: <code>finetune</code>):  Options: <code>finetune</code>.</li>
<li><strong><code>early_stop</code></strong> (default: <code>5</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of consecutive rounds of evaluation without any improvement on the <code>validation_metric</code> that triggers training to stop. Can be set to -1, which disables early stopping entirely.</li>
<li><strong><code>learning_rate</code></strong> (default: <code>0.001</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Controls how much to change the model in response to the estimated error each time the model weights are updated. If 'auto', the optimal learning rate is estimated by choosing the learning rate that produces the smallest non-diverging gradient update.</li>
<li><strong><code>epochs</code></strong> (default: <code>100</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of epochs the algorithm is intended to be run over. Overridden if <code>train_steps</code> is set</li>
<li><strong><code>batch_size</code></strong> (default: <code>auto</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The number of training examples utilized in one training step of the model. If ’auto’, the batch size that maximized training throughput (samples / sec) will be used. For CPU training, the tuned batch size is capped at 128 as throughput benefits of large batch sizes are less noticeable without a GPU.</li>
<li><strong><code>optimizer</code></strong> (default: <code>{"type": "adam"}</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Optimizer type and its parameters. The optimizer is responsble for applying the gradients computed from the loss during backpropagation as updates to the model weights. See <a href="#optimizer-parameters">Optimizer parameters</a> for details.</li>
<li><strong><code>regularization_type</code></strong> (default: <code>l2</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Type of regularization. Options: <code>l1</code>, <code>l2</code>, <code>l1_l2</code>, <code>null</code>.</li>
<li><strong><code>use_mixed_precision</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Enable automatic mixed-precision (AMP) during training. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>compile</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Whether to compile the model before training. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>checkpoints_per_epoch</code></strong> (default: <code>0</code>): Number of checkpoints per epoch. For example, 2 -&gt; checkpoints are written every half of an epoch. Note that it is invalid to specify both non-zero <code>steps_per_checkpoint</code> and non-zero <code>checkpoints_per_epoch</code>.</li>
<li><strong><code>effective_batch_size</code></strong> (default: <code>auto</code>): The effective batch size is the total number of samples used to compute a single gradient update to the model weights. This differs from <code>batch_size</code> by taking <code>gradient_accumulation_steps</code> and number of training worker processes into account. In practice, <code>effective_batch_size = batch_size * gradient_accumulation_steps * num_workers</code>. If 'auto', the effective batch size is derivied implicitly from <code>batch_size</code>, but if set explicitly, then one of <code>batch_size</code> or <code>gradient_accumulation_steps</code> must be set to something other than 'auto', and consequently will be set following the formula given above.</li>
<li><strong><code>gradient_accumulation_steps</code></strong> (default: <code>auto</code>): Number of steps to accumulate gradients over before performing a weight update.</li>
<li><strong><code>regularization_lambda</code></strong> (default: <code>0.0</code>): Strength of the regularization.</li>
<li><strong><code>validation_field</code></strong> (default: <code>null</code>): The field for which the <code>validation_metric</code> is used for validation-related mechanics like early stopping, parameter change plateaus, as well as what hyperparameter optimization uses to determine the best trial. If unset (default), the first output feature is used. If explicitly specified, neither <code>validation_field</code> nor <code>validation_metric</code> are overwritten.</li>
<li><strong><code>validation_metric</code></strong> (default: <code>null</code>): Metric from <code>validation_field</code> that is used. If validation_field is not explicitly specified, this is overwritten to be the first output feature type's <code>default_validation_metric</code>, consistent with validation_field. If the validation_metric is specified, then we will use the first output feature that produces this metric as the <code>validation_field</code>.</li>
<li><strong><code>train_steps</code></strong> (default: <code>null</code>): Maximum number of training steps the algorithm is intended to be run over. Unset by default. If set, will override <code>epochs</code> and if left unset then <code>epochs</code> is used to determine training length.</li>
<li><strong><code>steps_per_checkpoint</code></strong> (default: <code>0</code>): How often the model is checkpointed. Also dictates maximum evaluation frequency. If 0 the model is checkpointed after every epoch.</li>
<li><strong><code>max_batch_size</code></strong> (default: <code>1099511627776</code>): Auto batch size tuning and increasing batch size on plateau will be capped at this value. The default value is 2^40.</li>
<li><strong><code>eval_batch_size</code></strong> (default: <code>null</code>): Size of batch to pass to the model for evaluation. If it is <code>0</code> or <code>None</code>, the same value of <code>batch_size</code> is used. This is useful to speedup evaluation with a much bigger batch size than training, if enough memory is available. If ’auto’, the biggest batch size (power of 2) that can fit in memory will be used.</li>
<li><strong><code>evaluate_training_set</code></strong> (default: <code>false</code>): Whether to evaluate on the entire training set during evaluation. By default, training metrics will be computed at the end of each training step, and accumulated up to the evaluation phase. In practice, computing training set metrics during training is up to 30% faster than running a separate evaluation pass over the training set, but results in more noisy training metrics, particularly during the earlier epochs. It's recommended to only set this to True if you need very exact training set metrics, and are willing to pay a significant performance penalty for them. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>should_shuffle</code></strong> (default: <code>true</code>): Whether to shuffle batches during training when true. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>increase_batch_size_on_plateau</code></strong> (default: <code>0</code>): The number of times to increase the batch size on a plateau.</li>
<li><strong><code>increase_batch_size_on_plateau_patience</code></strong> (default: <code>5</code>): How many epochs to wait for before increasing the batch size.</li>
<li><strong><code>increase_batch_size_on_plateau_rate</code></strong> (default: <code>2.0</code>): Rate at which the batch size increases.</li>
<li><strong><code>increase_batch_size_eval_metric</code></strong> (default: <code>loss</code>): Which metric to listen on for increasing the batch size.</li>
<li><strong><code>increase_batch_size_eval_split</code></strong> (default: <code>training</code>): Which dataset split to listen on for increasing the batch size.</li>
<li><strong><code>gradient_clipping</code></strong> : Parameter values for gradient clipping.</li>
<li><strong><code>gradient_clipping.clipglobalnorm</code></strong> (default: <code>0.5</code>): Maximum allowed norm of the gradients</li>
<li><strong><code>gradient_clipping.clipnorm</code></strong> (default: <code>null</code>): Maximum allowed norm of the gradients</li>
<li><strong><code>gradient_clipping.clipvalue</code></strong> (default: <code>null</code>): Maximum allowed value of the gradients</li>
<li><strong><code>learning_rate_scaling</code></strong> (default: <code>linear</code>): Scale by which to increase the learning rate as the number of distributed workers increases. Traditionally the learning rate is scaled linearly with the number of workers to reflect the proportion by which the effective batch size is increased. For very large batch sizes, a softer square-root scale can sometimes lead to better model performance. If the learning rate is hand-tuned for a given number of workers, setting this value to constant can be used to disable scale-up. Options: <code>constant</code>, <code>sqrt</code>, <code>linear</code>.</li>
<li><strong><code>bucketing_field</code></strong> (default: <code>null</code>): Feature to use for bucketing datapoints</li>
<li><strong><code>skip_all_evaluation</code></strong> (default: <code>false</code>): Whether to skip evaluation entirely. If you are training a model with a well-known configuration on a well-known dataset and are confident about the expected results, you might skip all evaluation. Moreover, evaluating a model, especially on large validation or test sets, can be time-consuming. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>learning_rate_scheduler</code></strong> : Parameter values for learning rate scheduler.</li>
<li><strong><code>learning_rate_scheduler.decay</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Turn on decay of the learning rate. Options: <code>linear</code>, <code>exponential</code>, <code>cosine</code>, <code>null</code>.</li>
<li><strong><code>learning_rate_scheduler.decay_rate</code></strong> (default: <code>0.96</code>): Decay per epoch (%): Factor to decrease the Learning rate.</li>
<li><strong><code>learning_rate_scheduler.decay_steps</code></strong> (default: <code>10000</code>): The number of steps to take in the exponential learning rate decay.</li>
<li><strong><code>learning_rate_scheduler.staircase</code></strong> (default: <code>false</code>): Decays the learning rate at discrete intervals. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau</code></strong> (default: <code>0</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: How many times to reduce the learning rate when the algorithm hits a plateau (i.e. the performance on the training set does not improve)</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau_patience</code></strong> (default: <code>10</code>): How many evaluation steps have to pass before the learning rate reduces when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_on_plateau_rate</code></strong> (default: <code>0.1</code>): Rate at which we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.warmup_evaluations</code></strong> (default: <code>0</code>): Number of evaluation steps to warmup the learning rate for.</li>
<li><strong><code>learning_rate_scheduler.warmup_fraction</code></strong> (default: <code>0.0</code>): Fraction of total training steps to warmup the learning rate for.</li>
<li><strong><code>learning_rate_scheduler.reduce_eval_metric</code></strong> (default: <code>loss</code>): Metric plateau used to trigger when we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.reduce_eval_split</code></strong> (default: <code>training</code>): Which dataset split to listen on for reducing the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li>
<li><strong><code>learning_rate_scheduler.t_0</code></strong> (default: <code>null</code>): Number of steps before the first restart for cosine annealing decay. If not specified, it will be set to <code>steps_per_checkpoint</code>.</li>
<li><strong><code>learning_rate_scheduler.t_mult</code></strong> (default: <code>1</code>): Period multiplier after each restart for cosine annealing decay. Defaults to 1, i.e., restart every <code>t_0</code> steps. If set to a larger value, the period between restarts increases by that multiplier. For e.g., if t_mult is 2, then the periods would be: t_0, 2<em>t_0, 2^2</em>t_0, 2^3*t_0, etc.</li>
<li><strong><code>learning_rate_scheduler.eta_min</code></strong> (default: <code>0</code>): Minimum learning rate allowed for cosine annealing decay. Default: 0.</li>
<li><strong><code>base_learning_rate</code></strong> (default: <code>0.0</code>): Base learning rate used for training in the LLM trainer.</li>
</ul>
</div>
<div class="tabbed-block">
<p>See the <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">LightGBM documentation</a> for more details about the available parameters.</p>
<ul>
<li><strong><code>early_stop</code></strong> (default: <code>5</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of consecutive rounds of evaluation without any improvement on the <code>validation_metric</code> that triggers training to stop. Can be set to -1, which disables early stopping entirely.</li>
<li><strong><code>learning_rate</code></strong> (default: <code>0.03</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Controls how much to change the model in response to the estimated error each time the model weights are updated.</li>
<li><strong><code>max_depth</code></strong> (default: <code>18</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Maximum depth of a tree in the GBM trainer. A negative value means no limit.</li>
<li><strong><code>boosting_type</code></strong> (default: <code>gbdt</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Type of boosting algorithm to use with GBM trainer. Options: <code>gbdt</code>, <code>dart</code>.</li>
<li><strong><code>bagging_fraction</code></strong> (default: <code>0.8</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Fraction of data to use for bagging with GBM trainer.</li>
<li><strong><code>feature_fraction</code></strong> (default: <code>0.75</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Fraction of features to use in the GBM trainer.</li>
<li><strong><code>extra_trees</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Whether to use extremely randomized trees in the GBM trainer. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>lambda_l1</code></strong> (default: <code>0.25</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: L1 regularization factor for the GBM trainer.</li>
<li><strong><code>lambda_l2</code></strong> (default: <code>0.2</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: L2 regularization factor for the GBM trainer.</li>
<li><strong><code>drop_rate</code></strong> (default: <code>0.1</code>): Dropout rate for the GBM trainer. Used only with boosting_type 'dart'.</li>
<li><strong><code>tree_learner</code></strong> (default: <code>serial</code>): Type of tree learner to use with GBM trainer. Options: <code>serial</code>, <code>feature</code>, <code>data</code>, <code>voting</code>.</li>
<li><strong><code>boosting_rounds_per_checkpoint</code></strong> (default: <code>50</code>): Number of boosting rounds per checkpoint / evaluation round.</li>
<li><strong><code>num_boost_round</code></strong> (default: <code>1000</code>): Number of boosting rounds to perform with GBM trainer.</li>
<li><strong><code>num_leaves</code></strong> (default: <code>82</code>): Number of leaves to use in the tree with GBM trainer.</li>
<li><strong><code>min_data_in_leaf</code></strong> (default: <code>20</code>): Minimum number of data points in a leaf with GBM trainer.</li>
<li><strong><code>pos_bagging_fraction</code></strong> (default: <code>1.0</code>): Fraction of positive data to use for bagging with GBM trainer.</li>
<li><strong><code>neg_bagging_fraction</code></strong> (default: <code>1.0</code>): Fraction of negative data to use for bagging with GBM trainer.</li>
<li><strong><code>bagging_freq</code></strong> (default: <code>1</code>): Frequency of bagging with GBM trainer.</li>
<li><strong><code>bagging_seed</code></strong> (default: <code>3</code>): Random seed for bagging with GBM trainer.</li>
<li><strong><code>feature_fraction_bynode</code></strong> (default: <code>1.0</code>): Fraction of features to use for each tree node with GBM trainer.</li>
<li><strong><code>feature_fraction_seed</code></strong> (default: <code>2</code>): Random seed for feature fraction with GBM trainer.</li>
<li><strong><code>extra_seed</code></strong> (default: <code>6</code>): Random seed for extremely randomized trees in the GBM trainer.</li>
<li><strong><code>linear_lambda</code></strong> (default: <code>0.0</code>): Linear tree regularization in the GBM trainer.</li>
<li><strong><code>max_drop</code></strong> (default: <code>50</code>): Maximum number of dropped trees during one boosting iteration. Used only with boosting_type 'dart'. A negative value means no limit.</li>
<li><strong><code>skip_drop</code></strong> (default: <code>0.5</code>): Probability of skipping the dropout during one boosting iteration. Used only with boosting_type 'dart'.</li>
<li><strong><code>uniform_drop</code></strong> (default: <code>false</code>): Whether to use uniform dropout in the GBM trainer. Used only with boosting_type 'dart'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>drop_seed</code></strong> (default: <code>4</code>): Random seed to choose dropping models in the GBM trainer. Used only with boosting_type 'dart'.</li>
<li><strong><code>validation_field</code></strong> (default: <code>null</code>): First output feature, by default it is set as the same field of the first output feature.</li>
<li><strong><code>validation_metric</code></strong> (default: <code>null</code>): Metric used on <code>validation_field</code>, set by default to the output feature type's <code>default_validation_metric</code>.</li>
<li><strong><code>eval_batch_size</code></strong> (default: <code>1048576</code>): Size of batch to pass to the model for evaluation.</li>
<li><strong><code>evaluate_training_set</code></strong> (default: <code>false</code>): Whether to evaluate on the entire training set during evaluation. By default, training metrics will be computed at the end of each training step, and accumulated up to the evaluation phase. In practice, computing training set metrics during training is up to 30% faster than running a separate evaluation pass over the training set, but results in more noisy training metrics, particularly during the earlier epochs. It's recommended to only set this to True if you need very exact training set metrics, and are willing to pay a significant performance penalty for them. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>min_sum_hessian_in_leaf</code></strong> (default: <code>0.001</code>): Minimum sum of hessians in a leaf with GBM trainer.</li>
<li><strong><code>max_delta_step</code></strong> (default: <code>0.0</code>): Used to limit the max output of tree leaves in the GBM trainer. A negative value means no constraint.</li>
<li><strong><code>min_gain_to_split</code></strong> (default: <code>0.03</code>): Minimum gain to split a leaf in the GBM trainer.</li>
<li><strong><code>xgboost_dart_mode</code></strong> (default: <code>false</code>): Whether to use xgboost dart mode in the GBM trainer. Used only with boosting_type 'dart'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>top_rate</code></strong> (default: <code>0.2</code>): The retain ratio of large gradient data in the GBM trainer. Used only with boosting_type 'goss'.</li>
<li><strong><code>other_rate</code></strong> (default: <code>0.1</code>): The retain ratio of small gradient data in the GBM trainer. Used only with boosting_type 'goss'.</li>
<li><strong><code>min_data_per_group</code></strong> (default: <code>100</code>): Minimum number of data points per categorical group for the GBM trainer.</li>
<li><strong><code>max_cat_threshold</code></strong> (default: <code>32</code>): Number of split points considered for categorical features for the GBM trainer.</li>
<li><strong><code>cat_l2</code></strong> (default: <code>10.0</code>): L2 regularization factor for categorical split in the GBM trainer.</li>
<li><strong><code>cat_smooth</code></strong> (default: <code>10.0</code>): Smoothing factor for categorical split in the GBM trainer.</li>
<li><strong><code>max_cat_to_onehot</code></strong> (default: <code>4</code>): Maximum categorical cardinality required before one-hot encoding in the GBM trainer.</li>
<li><strong><code>cegb_tradeoff</code></strong> (default: <code>1.0</code>): Cost-effective gradient boosting multiplier for all penalties in the GBM trainer.</li>
<li><strong><code>cegb_penalty_split</code></strong> (default: <code>0.0</code>): Cost-effective gradient boosting penalty for splitting a node in the GBM trainer.</li>
<li><strong><code>path_smooth</code></strong> (default: <code>0.0</code>): Smoothing factor applied to tree nodes in the GBM trainer.</li>
<li><strong><code>verbose</code></strong> (default: <code>-1</code>): Verbosity level for GBM trainer. Options: <code>-1</code>, <code>0</code>, <code>1</code>, <code>2</code>.</li>
<li><strong><code>max_bin</code></strong> (default: <code>255</code>): Maximum number of bins to use for discretizing features with GBM trainer.</li>
<li><strong><code>feature_pre_filter</code></strong> (default: <code>true</code>): Whether to ignore features that are unsplittable based on min_data_in_leaf in the GBM trainer. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>skip_all_evaluation</code></strong> (default: <code>false</code>): Whether to skip evaluation entirely. If you are training a model with a well-known configuration on a well-known dataset and are confident about the expected results, you might skip all evaluation. Moreover, evaluating a model, especially on large validation or test sets, can be time-consuming. Options: <code>true</code>, <code>false</code>.</li>
</ul>
</div>
</div>
</div>
<h3 id="optimizer-parameters">Optimizer parameters<a class="headerlink" href="#optimizer-parameters" title="Permanent link">&para;</a></h3>
<div class="tabbed-set tabbed-alternate" data-tabs="3:2"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><input id="__tabbed_3_2" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">ECD / LLM</label><label for="__tabbed_3_2">GBM</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>The available optimizers wrap the ones available in PyTorch.
For details about the parameters that can be used to configure different optimizers, please refer to the <a href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a>.</p>
<p>The <code>learning_rate</code> parameter used by the optimizer comes from the <code>trainer</code> section.
Other optimizer specific parameters, shown with their Ludwig default settings, follow:</p>
<h4 id="sgd">sgd<a class="headerlink" href="#sgd" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sgd</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">dampening</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">nesterov</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.0</code>): Momentum factor.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>dampening</code></strong> (default: <code>0.0</code>): Dampening for momentum.</li>
<li><strong><code>nesterov</code></strong> (default: <code>false</code>): Enables Nesterov momentum. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="sgd_8bit">sgd_8bit<a class="headerlink" href="#sgd_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sgd_8bit</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">dampening</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">nesterov</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.0</code>): Momentum factor.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>dampening</code></strong> (default: <code>0.0</code>): Dampening for momentum.</li>
<li><strong><code>nesterov</code></strong> (default: <code>false</code>): Enables Nesterov momentum. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>false</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="lbfgs">lbfgs<a class="headerlink" href="#lbfgs" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lbfgs</span>
<span class="w">    </span><span class="nt">max_iter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span>
<span class="w">    </span><span class="nt">max_eval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">tolerance_grad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-07</span>
<span class="w">    </span><span class="nt">tolerance_change</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-09</span>
<span class="w">    </span><span class="nt">history_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">line_search_fn</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</code></pre></div>
<ul>
<li><strong><code>max_iter</code></strong> (default: <code>20</code>): Maximum number of iterations per optimization step.</li>
<li><strong><code>max_eval</code></strong> (default: <code>null</code>): Maximum number of function evaluations per optimization step. Default: <code>max_iter</code> * 1.25.</li>
<li><strong><code>tolerance_grad</code></strong> (default: <code>1e-07</code>): Termination tolerance on first order optimality.</li>
<li><strong><code>tolerance_change</code></strong> (default: <code>1e-09</code>): Termination tolerance on function value/parameter changes.</li>
<li><strong><code>history_size</code></strong> (default: <code>100</code>): Update history size.</li>
<li><strong><code>line_search_fn</code></strong> (default: <code>null</code>): Line search function to use. Options: <code>strong_wolfe</code>, <code>null</code>.</li>
</ul>
<h4 id="adam">adam<a class="headerlink" href="#adam" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adam</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="adam_8bit">adam_8bit<a class="headerlink" href="#adam_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adam_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="paged_adam">paged_adam<a class="headerlink" href="#paged_adam" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_adam</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="paged_adam_8bit">paged_adam_8bit<a class="headerlink" href="#paged_adam_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_adam_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="adamw">adamw<a class="headerlink" href="#adamw" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adamw</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'.  Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="adamw_8bit">adamw_8bit<a class="headerlink" href="#adamw_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adamw_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'.  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="paged_adamw">paged_adamw<a class="headerlink" href="#paged_adamw" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_adamw</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'.  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="paged_adamw_8bit">paged_adamw_8bit<a class="headerlink" href="#paged_adamw_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_adamw_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'.  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="adadelta">adadelta<a class="headerlink" href="#adadelta" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adadelta</span>
<span class="w">    </span><span class="nt">rho</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-06</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
</code></pre></div>
<ul>
<li><strong><code>rho</code></strong> (default: <code>0.9</code>): Coefficient used for computing a running average of squared gradients.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-06</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
</ul>
<h4 id="adagrad">adagrad<a class="headerlink" href="#adagrad" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adagrad</span>
<span class="w">    </span><span class="nt">initial_accumulator_value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">lr_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-10</span>
</code></pre></div>
<ul>
<li><strong><code>initial_accumulator_value</code></strong> (default: <code>0</code>): </li>
<li><strong><code>lr_decay</code></strong> (default: <code>0</code>): Learning rate decay.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>eps</code></strong> (default: <code>1e-10</code>): Term added to the denominator to improve numerical stability.</li>
</ul>
<h4 id="adagrad_8bit">adagrad_8bit<a class="headerlink" href="#adagrad_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adagrad_8bit</span>
<span class="w">    </span><span class="nt">initial_accumulator_value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">lr_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-10</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>initial_accumulator_value</code></strong> (default: <code>0</code>): </li>
<li><strong><code>lr_decay</code></strong> (default: <code>0</code>): Learning rate decay.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>eps</code></strong> (default: <code>1e-10</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="adamax">adamax<a class="headerlink" href="#adamax" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adamax</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
</ul>
<h4 id="nadam">nadam<a class="headerlink" href="#nadam" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nadam</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">momentum_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.004</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>momentum_decay</code></strong> (default: <code>0.004</code>): Momentum decay.</li>
</ul>
<h4 id="rmsprop">rmsprop<a class="headerlink" href="#rmsprop" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rmsprop</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.99</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">centered</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.0</code>): Momentum factor.</li>
<li><strong><code>alpha</code></strong> (default: <code>0.99</code>): Smoothing constant.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>centered</code></strong> (default: <code>false</code>): If True, computes the centered RMSProp, and the gradient is normalized by an estimation of its variance. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
</ul>
<h4 id="rmsprop_8bit">rmsprop_8bit<a class="headerlink" href="#rmsprop_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rmsprop_8bit</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.99</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">centered</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.0</code>): Momentum factor.</li>
<li><strong><code>alpha</code></strong> (default: <code>0.99</code>): Smoothing constant.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>centered</code></strong> (default: <code>false</code>): If True, computes the centered RMSProp, and the gradient is normalized by an estimation of its variance. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
</ul>
<h4 id="lamb">lamb<a class="headerlink" href="#lamb" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lamb</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">bias_correction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">adam_w_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">max_unorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bias_correction</code></strong> (default: <code>true</code>):  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>adam_w_mode</code></strong> (default: <code>true</code>): Whether to use the AdamW mode of this algorithm from the paper 'Decoupled Weight Decay Regularization'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>false</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>max_unorm</code></strong> (default: <code>1.0</code>): </li>
</ul>
<h4 id="lamb_8bit">lamb_8bit<a class="headerlink" href="#lamb_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lamb_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-08</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">amsgrad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">bias_correction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">adam_w_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">max_unorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>eps</code></strong> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>amsgrad</code></strong> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bias_correction</code></strong> (default: <code>true</code>):  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>adam_w_mode</code></strong> (default: <code>true</code>): Whether to use the AdamW mode of this algorithm from the paper 'Decoupled Weight Decay Regularization'. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>false</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>max_unorm</code></strong> (default: <code>1.0</code>): </li>
</ul>
<h4 id="lars">lars<a class="headerlink" href="#lars" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lars</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="nt">dampening</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">nesterov</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">max_unorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.9</code>): Momentum factor.</li>
<li><strong><code>dampening</code></strong> (default: <code>0.0</code>): Dampening for momentum.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>nesterov</code></strong> (default: <code>false</code>): Enables Nesterov momentum. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>max_unorm</code></strong> (default: <code>1.0</code>): </li>
</ul>
<h4 id="lars_8bit">lars_8bit<a class="headerlink" href="#lars_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lars_8bit</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="nt">dampening</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">nesterov</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">max_unorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</code></pre></div>
<ul>
<li><strong><code>momentum</code></strong> (default: <code>0.9</code>): Momentum factor.</li>
<li><strong><code>dampening</code></strong> (default: <code>0.0</code>): Dampening for momentum.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>nesterov</code></strong> (default: <code>false</code>): Enables Nesterov momentum. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>max_unorm</code></strong> (default: <code>1.0</code>): </li>
</ul>
<h4 id="lion">lion<a class="headerlink" href="#lion" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lion</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="lion_8bit">lion_8bit<a class="headerlink" href="#lion_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lion_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="paged_lion">paged_lion<a class="headerlink" href="#paged_lion" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_lion</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="paged_lion_8bit">paged_lion_8bit<a class="headerlink" href="#paged_lion_8bit" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nt">optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">paged_lion_8bit</span>
<span class="w">    </span><span class="nt">betas</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.999</span>
<span class="w">    </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">percentile_clipping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">    </span><span class="nt">block_wise</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong><code>betas</code></strong> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li>
<li><strong><code>weight_decay</code></strong> (default: <code>0.0</code>): Weight decay (L2 penalty).</li>
<li><strong><code>percentile_clipping</code></strong> (default: <code>100</code>): Percentile clipping.</li>
<li><strong><code>block_wise</code></strong> (default: <code>true</code>): Whether to use block wise update. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Gradient clipping is also configurable, through optimizers, with the following parameters:</p>
<div class="highlight"><pre><span></span><code><span class="nt">clip_global_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="nt">clipnorm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">clip_value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</code></pre></div>
</div>
</div>
<div class="tabbed-block">
<p>No optimizer parameters are available for the LightGBM trainer.</p>
</div>
</div>
</div>
<h2 id="training-length">Training length<a class="headerlink" href="#training-length" title="Permanent link">&para;</a></h2>
<p>The length of the training process is configured by:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="4:2"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio" /><input id="__tabbed_4_2" name="__tabbed_4" type="radio" /><div class="tabbed-labels"><label for="__tabbed_4_1">ECD / LLM</label><label for="__tabbed_4_2">GBM</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<ul>
<li><code>epochs</code> (default: 100): One epoch is one pass through the entire dataset. By
    default, <code>epochs</code> is 100 which means that the training process will run for
    a maximum of 100 epochs before terminating.</li>
<li><code>train_steps</code> (default: <code>None</code>): The maximum number of steps to train for,
    using one mini-batch per step. By default this is unset, and <code>epochs</code> will
    be used to determine training length.</li>
</ul>
</div>
<div class="tabbed-block">
<ul>
<li><code>num_boost_round</code> (default: 100): The number of boosting iterations. By default,
    <code>num_boost_round</code> is 100 which means that the training process will run for
    a maximum of 100 boosting iterations before terminating.</li>
</ul>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In general, it's a good idea to set up a long training runway, relying on
early stopping criteria (<code>early_stop</code>) to stop training when there
hasn't been any improvement for a long time.</p>
</div>
<h2 id="early-stopping">Early stopping<a class="headerlink" href="#early-stopping" title="Permanent link">&para;</a></h2>
<p>Machine learning models, when trained for too long, are often prone to
overfitting. It's generally a good policy to set up some early stopping criteria
as it's not useful to have a model train after it's maximized what it can learn,
as to retain it's ability to generalize to new data.</p>
<h3 id="how-early-stopping-works-in-ludwig">How early stopping works in Ludwig<a class="headerlink" href="#how-early-stopping-works-in-ludwig" title="Permanent link">&para;</a></h3>
<p>By default, Ludwig sets <code>trainer.early_stop=5</code>, which means that if there have
been <code>5</code> consecutive rounds of evaluation where there hasn't been any
improvement on the <strong>validation</strong> subset, then training will terminate.</p>
<p>Ludwig runs evaluation once per checkpoint, which by default is once per epoch.
Checkpoint frequency can be configured using <code>checkpoints_per_epoch</code> (default:
<code>1</code>) or <code>steps_per_checkpoint</code> (default: <code>0</code>, disabled). See
<a href="#checkpoint-evaluation-frequency">this section</a> for more details.</p>
<h3 id="changing-the-metric-early-stopping-metrics">Changing the metric early stopping metrics<a class="headerlink" href="#changing-the-metric-early-stopping-metrics" title="Permanent link">&para;</a></h3>
<p>The metric that dictates early stopping is
<code>trainer.validation_field</code> and <code>trainer.validation_metric</code>. By default, early
stopping uses the combined loss on the validation subset.</p>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">validation_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">combined</span>
<span class="w">    </span><span class="nt">validation_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">loss</span>
</code></pre></div>
<p>However, this can be configured to use other metrics. For example, if we had an
output feature called <code>recommended</code>, then we can configure early stopping on the
output feature accuracy like so:</p>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">validation_field</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">recommended</span>
<span class="w">    </span><span class="nt">validation_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">accuracy</span>
</code></pre></div>
<h3 id="disabling-early-stopping">Disabling early stopping<a class="headerlink" href="#disabling-early-stopping" title="Permanent link">&para;</a></h3>
<p><code>trainer.early_stop</code> can be set to <code>-1</code>, which disables early stopping entirely.</p>
<h2 id="checkpoint-evaluation-frequency">Checkpoint-evaluation frequency<a class="headerlink" href="#checkpoint-evaluation-frequency" title="Permanent link">&para;</a></h2>
<div class="tabbed-set tabbed-alternate" data-tabs="5:1"><input checked="checked" id="__tabbed_5_1" name="__tabbed_5" type="radio" /><div class="tabbed-labels"><label for="__tabbed_5_1">ECD / LLM</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<p>Evaluation is run every time the model is checkpointed.</p>
<p>By default, checkpoint-evaluation will occur once every epoch.</p>
<p>The frequency of checkpoint-evaluation can be configured using:</p>
<ul>
<li><code>steps_per_checkpoint</code> (default: 0): every <code>n</code> training steps</li>
<li><code>checkpoints_per_epoch</code> (default: 0): <code>n</code> times per epoch</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is invalid to specify both non-zero <code>steps_per_checkpoint</code> and non-zero
<code>checkpoints_per_epoch</code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Running evaluation once per epoch is an appropriate fit for small datasets 
that fit in memory and train quickly. However, this can be a poor fit for
unstructured datasets, which tend to be much larger, and train more slowly
due to larger models.</p>
<p>Running evaluation too frequently can be wasteful while running evaluation
not frequently enough can be uninformative. In large-scale training runs,
it's common for evaluation to be configured to run on a sub-epoch time
scale, or every few thousand steps.</p>
<p>We recommend configuring evaluation such that new evaluation results are
available at least several times an hour. In general, it is not necessary
for models to train over the entirety of a dataset, nor evaluate over the
entirety of a test set, to produce useful monitoring metrics and signals to
indicate model performance.</p>
</div>
<h2 id="increasing-throughput-on-gpus">Increasing throughput on GPUs<a class="headerlink" href="#increasing-throughput-on-gpus" title="Permanent link">&para;</a></h2>
<h3 id="increase-batch-size">Increase batch size<a class="headerlink" href="#increase-batch-size" title="Permanent link">&para;</a></h3>
<div class="tabbed-set tabbed-alternate" data-tabs="6:1"><input checked="checked" id="__tabbed_6_1" name="__tabbed_6" type="radio" /><div class="tabbed-labels"><label for="__tabbed_6_1">ECD / LLM</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
</code></pre></div>
<p>Users training on GPUs can often increase training throughput by increasing
the <code>batch_size</code> so that more examples are computed every training step. Set
<code>batch_size</code> to <code>auto</code> to use the largest batch size that can fit in memory.</p>
<h3 id="use-mixed-precision">Use mixed precision<a class="headerlink" href="#use-mixed-precision" title="Permanent link">&para;</a></h3>
<div class="tabbed-set tabbed-alternate" data-tabs="7:1"><input checked="checked" id="__tabbed_7_1" name="__tabbed_7" type="radio" /><div class="tabbed-labels"><label for="__tabbed_7_1">ECD / LLM</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">use_mixed_precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<p>Speeds up training by using float16 parameters where it makes sense. Mixed precision training on GPU can dramatically
speedup training, with some risks to model convergence. In practice, it works particularly well when fine-tuning
a pretrained model like a HuggingFace transformer. See blog <a href="https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/">here</a> for more details.</p>


  




                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <!--
  Copyright (c) 2016-2022 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Footer -->
<footer class="md-footer">

    <!-- Link to previous and/or next page -->
    
    <nav class="md-footer__inner md-grid" aria-label="">

        <!-- Link to previous page -->
        
        
        <a href="../combiner/" class="md-footer__link md-footer__link--prev"
            aria-label="Previous: Combiner" rel="prev">
            <div class="md-footer__button md-icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
                <div class="md-ellipsis">
                    <span class="md-footer__direction">
                        Previous
                    </span>
                    Combiner
                </div>
            </div>
        </a>
        

        <!-- Link to next page -->
        
        
        <a href="../hyperparameter_optimization/" class="md-footer__link md-footer__link--next"
            aria-label="Next: Hyperopt" rel="next">
            <div class="md-footer__title">
                <div class="md-ellipsis">
                    <span class="md-footer__direction">
                        Next
                    </span>
                    Hyperopt
                </div>
            </div>
            <div class="md-footer__button md-icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
        </a>
        
    </nav>
    

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">
            <!--
  Copyright (c) 2016-2021 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Copyright and theme information -->
<div class="md-copyright">
    
    <div class="md-copyright__highlight">
        Copyright &copy; 2018 - 2020 Uber Technologies Inc., 2021 - 2022 Linux Foundation Data & AI
    </div>
    
    

    Website by <a href="http://w4nderlu.st">w4nderlust</a> powered by
    <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>,
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">Material for MkDocs</a>,
    <a href="http://www.styleshout.com/" target="_blank" rel="noopener">styleshout</a> and
    <a href="http://cables.gl/" target="_blank" rel="noopener">cables</a>.
    
</div>

            <!-- Social links -->
            
        </div>
    </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes", "navigation.tabs.sticky"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.2a6f1dda.min.js"></script>
      
    
  </body>
</html>