
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
  
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Declarative machine learning: End-to-end machine learning pipelines using data-driven configurations.">
      
      
        <meta name="author" content="Piero Molino">
      
      
        <link rel="canonical" href="https://ludwig.ai/latest/configuration/large_language_model/">
      
      
        <link rel="prev" href="../model_type/">
      
      
        <link rel="next" href="../preprocessing/">
      
      
      <link rel="icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.6">
    
  <meta content="http://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/og-image.jpg"
    property="og:image">
  <meta content="https://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/og-image.jpg"
    property="og:image:secure_url">

    
      
        <title>Large Language Models - Ludwig</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.35e1ed30.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/monokai.css">
    
      <link rel="stylesheet" href="../../stylesheets/colorful.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-H8VVJF9L6G"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-H8VVJF9L6G",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-H8VVJF9L6G",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#base-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Ludwig" class="md-header__button md-logo" aria-label="Ludwig" data-md-component="logo">
      
<img alt="logo" src="../../images/ludwig_logo.svg"
     style="height:1rem;width:4rem;">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ludwig
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Large Language Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="grey" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ludwig-ai/ludwig" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
      </div>
    
  </nav>
  
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Ludwig" class="md-nav__button md-logo" aria-label="Ludwig" data-md-component="logo">
      
<img alt="logo" src="../../images/ludwig_logo.svg"
     style="height:1rem;width:4rem;">

    </a>
    Ludwig
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ludwig-ai/ludwig" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ludwig
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../getting_started/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    ðŸš€ Getting Started
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            ðŸš€ Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/prepare_data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset preparation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/evaluate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prediction and Evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/hyperopt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperopt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serving
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/ray/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed training on Ray
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/llm_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Fine-tuning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ludwig with Docker
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../user_guide/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    ðŸ“– User Guide
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            ðŸ“– User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/what_is_ludwig/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is Ludwig?
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/how_ludwig_works/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How Ludwig Works
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/command_line_interface/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Command Line Interface
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Python API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/api/LudwigModel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LudwigModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/api/visualization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualization
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Datasets
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Datasets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/supported_formats/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Formats
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/data_preprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/data_postprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Postprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/datasets/dataset_zoo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset Zoo
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../user_guide/llms/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Large Language Models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Large Language Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/llms/finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-Tuning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/llms/in_context_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    In-Context Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/llms/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text Classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/gpus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPUs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../user_guide/distributed_training/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Distributed Training
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            Distributed Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/distributed_training/finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-Tuning Pretrained Models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/hyperopt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperparameter Optimization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/cloud_storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cloud Storage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/automl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AutoML
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/visualizations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualizations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/model_export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/serving/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serving
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide/integrations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Third-Party Integrations
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    ðŸ“š Configuration
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            ðŸ“š Configuration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../model_type/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Large Language Models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Large Language Models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#base-model" class="md-nav__link">
    Base Model
  </a>
  
    <nav class="md-nav" aria-label="Base Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#huggingface-access-token" class="md-nav__link">
    HuggingFace Access Token
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    Features
  </a>
  
    <nav class="md-nav" aria-label="Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-features" class="md-nav__link">
    Input Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#output-features" class="md-nav__link">
    Output Features
  </a>
  
    <nav class="md-nav" aria-label="Output Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-text-output-feature" class="md-nav__link">
    LLM Text Output Feature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-category-output-feature" class="md-nav__link">
    LLM Category Output Feature
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt" class="md-nav__link">
    Prompt
  </a>
  
    <nav class="md-nav" aria-label="Prompt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#retrieval" class="md-nav__link">
    Retrieval
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#max-sequence-lengths" class="md-nav__link">
    Max Sequence Lengths
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adapter" class="md-nav__link">
    Adapter
  </a>
  
    <nav class="md-nav" aria-label="Adapter">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lora" class="md-nav__link">
    LoRA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adalora" class="md-nav__link">
    AdaLoRA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaption-prompt" class="md-nav__link">
    Adaption Prompt
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ia3" class="md-nav__link">
    IA3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quantization" class="md-nav__link">
    Quantization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-parameters" class="md-nav__link">
    Model Parameters
  </a>
  
    <nav class="md-nav" aria-label="Model Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rope-scaling" class="md-nav__link">
    RoPE Scaling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neftune-noise-alpha" class="md-nav__link">
    Neftune Noise Alpha
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainer" class="md-nav__link">
    Trainer
  </a>
  
    <nav class="md-nav" aria-label="Trainer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning" class="md-nav__link">
    Fine-Tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-context-learning" class="md-nav__link">
    In-Context Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    Generation
  </a>
  
    <nav class="md-nav" aria-label="Generation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generation-strategies" class="md-nav__link">
    Generation Strategies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#post-fine-tuning" class="md-nav__link">
    Post Fine-Tuning
  </a>
  
    <nav class="md-nav" aria-label="Post Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#uploading-fine-tuned-llm-weights-to-huggingface-hub" class="md-nav__link">
    Uploading Fine-Tuned LLM weights to HuggingFace Hub
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Features
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Features
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/supported_data_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supported Data Types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/input_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Input Features (â†‘)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/output_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Output Features (â†“)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/binary_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Binary Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/number_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Number Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/category_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Category Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/bag_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Bag Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/set_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Set Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/sequence_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Sequence Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/text_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Text Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/vector_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â‡… Vector Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/audio_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â†‘ Audio Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/date_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â†‘ Date Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/h3_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â†‘ H3 Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/image_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â†‘ Image Features
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../features/time_series_features/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â†‘ Time Series Features
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../defaults/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Defaults
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../combiner/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Combiner
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../hyperparameter_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperopt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../backend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Backend
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../examples/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    ðŸ’¡ Examples
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            ðŸ’¡ Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    LLMs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            LLMs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-tuning for classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Instruction-tuning llama-2-7b
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_finetuning_deepspeed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adapter-based encoder fine-tuning for text classification with deepspeed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adapter-based fine-tuning for text generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_zero_shot_text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero-shot batch inference for text generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_zero_shot_batch_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero-shot batch inference for text classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_few_shot_batch_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Few-shot batch inference for text classification (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_tabular_zero_shot_batch_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zero-shot batch inference for tabular classification (TabLLM)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llms/llm_tabular_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-tuning for tabular classification (TabLLM)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Supervised ML
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Supervised ML
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/adult_census_income/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tabular Data Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/mnist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multimodal_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodal Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/hyperopt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperparameter Optimization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/gbm_fraud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fraud with GBMs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/sentiment_analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentiment Analysis
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Use Cases
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Use Cases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/ner_tagging/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Named Entity Recognition Tagging
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/nlu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Natural Language Understanding
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/machine_translation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Machine Translation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chit-Chat Dialogue Modeling through Sequence2Sequence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/sentiment_analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentiment Analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/oneshot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    One-shot Learning with Siamese Networks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/visual_qa/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visual Question Answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spoken Digit Speech Recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/speaker_verification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speaker Verification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/titanic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Binary Classification (Titanic)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Timeseries forecasting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/weather/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Timeseries forecasting (Weather)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/movie_ratings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Movie rating prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multi_label/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-label classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/multi_task/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Task Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/fuel_efficiency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Regression - Fuel Efficiency Prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/fraud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fraud Detection
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../developer_guide/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    ðŸ› ï¸ Developer Guide
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            ðŸ› ï¸ Developer Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to Contribute
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/codebase_structure/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Codebase Structure
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/api_annotations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ludwig API Guarantees
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_an_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add an Encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_combiner/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Combiner
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_feature_type/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Feature Type
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_loss_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Loss Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_tokenizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Tokenizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_hyperopt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Hyperopt Algorithm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_pretrained_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Pretrained Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_an_integration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add an Integration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/add_a_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Add a Dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/style_guidelines_and_tests/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Style Guidelines and Tests
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/unit_test_design_guidelines/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Unit Test Design Guidelines
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/run_tests_on_gpu_using_ray/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Run Tests on GPU Using Ray
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../developer_guide/release_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Release Process
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../community/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ðŸ‘‹ Community
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    â“ FAQ
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#base-model" class="md-nav__link">
    Base Model
  </a>
  
    <nav class="md-nav" aria-label="Base Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#huggingface-access-token" class="md-nav__link">
    HuggingFace Access Token
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    Features
  </a>
  
    <nav class="md-nav" aria-label="Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#input-features" class="md-nav__link">
    Input Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#output-features" class="md-nav__link">
    Output Features
  </a>
  
    <nav class="md-nav" aria-label="Output Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llm-text-output-feature" class="md-nav__link">
    LLM Text Output Feature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-category-output-feature" class="md-nav__link">
    LLM Category Output Feature
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt" class="md-nav__link">
    Prompt
  </a>
  
    <nav class="md-nav" aria-label="Prompt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#retrieval" class="md-nav__link">
    Retrieval
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#max-sequence-lengths" class="md-nav__link">
    Max Sequence Lengths
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adapter" class="md-nav__link">
    Adapter
  </a>
  
    <nav class="md-nav" aria-label="Adapter">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lora" class="md-nav__link">
    LoRA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adalora" class="md-nav__link">
    AdaLoRA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaption-prompt" class="md-nav__link">
    Adaption Prompt
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ia3" class="md-nav__link">
    IA3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quantization" class="md-nav__link">
    Quantization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-parameters" class="md-nav__link">
    Model Parameters
  </a>
  
    <nav class="md-nav" aria-label="Model Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rope-scaling" class="md-nav__link">
    RoPE Scaling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neftune-noise-alpha" class="md-nav__link">
    Neftune Noise Alpha
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainer" class="md-nav__link">
    Trainer
  </a>
  
    <nav class="md-nav" aria-label="Trainer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning" class="md-nav__link">
    Fine-Tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-context-learning" class="md-nav__link">
    In-Context Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    Generation
  </a>
  
    <nav class="md-nav" aria-label="Generation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generation-strategies" class="md-nav__link">
    Generation Strategies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#post-fine-tuning" class="md-nav__link">
    Post Fine-Tuning
  </a>
  
    <nav class="md-nav" aria-label="Post Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#uploading-fine-tuned-llm-weights-to-huggingface-hub" class="md-nav__link">
    Uploading Fine-Tuned LLM weights to HuggingFace Hub
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Large Language Models</h1>

<p>Large Language Models (LLMs) are a kind of neural network used for text generation
tasks like chatbots, coding assistants, etc. Unlike ECD models, which are primarily
designed for <em>predictive</em> tasks, LLMs are a fundamentally <em>generative</em> model type.</p>
<p>The <em>backbone</em> of an LLM (without the language model head used for next token
generation) can be used as a <strong>text encoder</strong> in ECD models when using the
<a href="../features/text_features/#huggingface-encoders">auto_transformer</a> encoder. If you wish
to use LLMs for predictive tasks like classification and regression, try ECD. For generative
tasks, read on!</p>
<p>Example config for fine-tuning <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf">LLaMA-2-7b</a>:</p>
<div class="highlight"><pre><span></span><code><span class="nt">model_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm</span>
<span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Llama-2-7b-hf</span>
<span class="nt">input_features</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text</span>
<span class="nt">output_features</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">response</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text</span>
<span class="nt">prompt</span><span class="p">:</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">    </span><span class="no">[INST] &lt;&lt;SYS&gt;&gt;</span>
<span class="w">    </span><span class="no">You are a helpful, detailed, and polite artificial </span>
<span class="w">    </span><span class="no">intelligence assistant. Your answers are clear and </span>
<span class="w">    </span><span class="no">suitable for a professional environment.</span>
<span class="w">    </span><span class="no">If context is provided, answer using only the provided </span>
<span class="w">    </span><span class="no">contextual information.</span>
<span class="w">    </span><span class="no">&lt;&lt;/SYS&gt;&gt;</span>
<span class="w">    </span><span class="no">{user_message_1} [/INST]</span>
<span class="nt">adapter</span><span class="p">:</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lora</span>
<span class="nt">quantization</span><span class="p">:</span>
<span class="w">  </span><span class="nt">bits</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">finetune</span>
<span class="w">  </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0001</span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
</code></pre></div>
<h2 id="base-model">Base Model<a class="headerlink" href="#base-model" title="Permanent link">&para;</a></h2>
<p>The <code>base_model</code> parameter specifies the pretrained large language model to serve
as the foundation of your custom LLM.</p>
<p>Currently, any pretrained HuggingFace Causal LM model from the <a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads">HuggingFace Hub</a> is supported as a <code>base_model</code>.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Llama-2-7b-hf</span>
</code></pre></div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Some models on the HuggingFace Hub require executing untrusted code. For security reasons,
these models are currently unsupported. If you have interest in using one of these models,
please file a GitHub issue with your use case.</p>
</div>
<p>You can also pass in a path to a locally saved Hugging Face model instead of loading from Hugging Face directly.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">path/to/local/model/weights</span>
</code></pre></div>
<h3 id="huggingface-access-token">HuggingFace Access Token<a class="headerlink" href="#huggingface-access-token" title="Permanent link">&para;</a></h3>
<p>Some base models like Llama-2 require authorization from HuggingFace to download,
which in turn requires obtaining a HuggingFace <a href="https://huggingface.co/docs/hub/security-tokens">User Access Token</a>.</p>
<p>Once you have obtained permission to download your preferred base model and have a user access token,
you only need to ensure that your token is exposes as an environment variable in order for Ludwig to be
able to use it:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">HUGGING_FACE_HUB_TOKEN</span><span class="o">=</span><span class="s2">&quot;&lt;api_token&gt;&quot;</span>
ludwig<span class="w"> </span>train<span class="w"> </span>...
</code></pre></div>
<h2 id="features">Features<a class="headerlink" href="#features" title="Permanent link">&para;</a></h2>
<h3 id="input-features">Input Features<a class="headerlink" href="#input-features" title="Permanent link">&para;</a></h3>
<p>Currently, the LLM model type only supports a single input feature of type <code>text</code>.</p>
<p>If no <code>prompt</code> template is provided, this feature must correspond to a column
in the input dataset. If a prompt template is provided, the rendered prompt
will be used as the input feature value during training and inference.</p>
<div class="highlight"><pre><span></span><code><span class="nt">input_features</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text</span>
</code></pre></div>
<p>See <a href="../features/text_features/">Text Features</a> for
configuration options.</p>
<h3 id="output-features">Output Features<a class="headerlink" href="#output-features" title="Permanent link">&para;</a></h3>
<p>Currently, the LLM model type only supports a single output feature.</p>
<h4 id="llm-text-output-feature">LLM Text Output Feature<a class="headerlink" href="#llm-text-output-feature" title="Permanent link">&para;</a></h4>
<p>When fine-tuning (<code>trainer.type: finetune</code>), the output feature type must be
<code>text</code>. Even if you are fine-tuning your LLM for a binary or multi-class classification
problem, set the output feature type of that column to <code>text</code>.</p>
<p>For in-context learning or zero shot learning (<code>trainer.type: none</code>), the output
feature type can be one of <code>text</code> or <code>category</code>.</p>
<div class="highlight"><pre><span></span><code><span class="nt">output_features</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">response</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text</span>
</code></pre></div>
<p>See <a href="../features/text_features/#output-features">Text Output Features</a> for
configuration options.</p>
<h4 id="llm-category-output-feature">LLM Category Output Feature<a class="headerlink" href="#llm-category-output-feature" title="Permanent link">&para;</a></h4>
<p>In order to use the <code>category</code> output feature type, you must provide two additional specifications. The first additional specification is a set of <code>match</code> values as part of the decoder configuration. These match values are used to determine which category label to assign to the generated response. This is particularly helpful to mitigate against cases where LLM text generation deviates from the desired response format.</p>
<p>The second additional specification is a fallback label in <code>preprocessing.fallback_label</code>. This label is used both for filling in missing values in the output feature column in your dataset, but also for providing a pre-determined value when the LLM is unable to generate a response that matches any of the categories provided.</p>
<div class="highlight"><pre><span></span><code><span class="nt">output_features</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">label</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">category</span>
<span class="w">    </span><span class="nt">preprocessing</span><span class="p">:</span>
<span class="w">      </span><span class="nt">fallback_label</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;neutral&quot;</span>
<span class="w">    </span><span class="nt">decoder</span><span class="p">:</span>
<span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">category_extractor</span>
<span class="w">      </span><span class="nt">match</span><span class="p">:</span>
<span class="w">        </span><span class="s">&quot;negative&quot;</span><span class="p p-Indicator">:</span>
<span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contains</span>
<span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;negative&quot;</span>
<span class="w">        </span><span class="s">&quot;neutral&quot;</span><span class="p p-Indicator">:</span>
<span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contains</span>
<span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;neutral&quot;</span>
<span class="w">        </span><span class="s">&quot;positive&quot;</span><span class="p p-Indicator">:</span>
<span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">contains</span>
<span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;positive&quot;</span>
</code></pre></div>
<h2 id="prompt">Prompt<a class="headerlink" href="#prompt" title="Permanent link">&para;</a></h2>
<p>One of the unique properties of large language models as compared to more conventional deep learning models is their ability to incorporate context inserted into the â€œpromptâ€ to generate more specific and accurate responses.</p>
<p>The <code>prompt</code> parameter can be used to:</p>
<ul>
<li>Provide necessary boilerplate needed to make the LLM respond in the correct way (for example, with a response to a question rather than a continuation of the input sequence).</li>
<li>Combine multiple columns from a dataset into a single text input feature (see <a href="https://arxiv.org/abs/2210.10723">TabLLM</a>).</li>
<li>Provide additional context to the model that can help it understand the task, or provide restrictions to prevent hallucinations.</li>
</ul>
<p>To make use of prompting, one of <code>prompt.template</code> or <code>prompt.task</code> must be provided. Otherwise the input feature value is passed into
the LLM as-is. Use <code>template</code> for fine-grained control over every aspect of the prompt, and use <code>task</code> to specify the nature of the
task the LLM is to perform while delegating the exact prompt template to Ludwig's defaults.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Some models that have already been instruction tuned will have been trained
to expect a specific prompt template structure. Unfortunately, this isn't
provided in any model metadata, and as such, you may need to dig around or
experiment with different prompt templates to find what works best when
performing <a href="#in-context-learning">in-context learning</a>.</p>
</div>
<div class="highlight"><pre><span></span><code><span class="nt">prompt</span><span class="p">:</span>
<span class="w">    </span><span class="nt">template</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">task</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">retrieval</span><span class="p">:</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">index_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">        </span><span class="nt">k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</code></pre></div>
<ul>
<li><strong><code>template</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The template to use for the prompt. Must contain at least one of the columns from the input dataset or <code>__sample__</code> as a variable surrounded in curly brackets {} to indicate where to insert the current feature. Multiple columns can be inserted, e.g.: <code>The {color} {animal} jumped over the {size} {object}</code>, where every term in curly brackets is a column in the dataset. If a <code>task</code> is specified, then the template must also contain the <code>__task__</code> variable. If <code>retrieval</code> is specified, then the template must also contain the <code>__context__</code> variable. If no template is provided, then a default will be used based on the retrieval settings, and a task must be set in the config.</li>
<li><strong><code>task</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The task to use for the prompt. Required if <code>template</code> is not set.</li>
<li><strong><code>retrieval</code></strong> (default: <code>{"type": null}</code>): </li>
</ul>
<h3 id="retrieval">Retrieval<a class="headerlink" href="#retrieval" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nt">retrieval</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">index_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</code></pre></div>
<ul>
<li><strong><code>type</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The type of retrieval to use for the prompt. If <code>None</code>, then no retrieval is used, and the task is framed as a zero-shot learning problem. If not <code>None</code> (e.g. either 'random' or 'semantic'), then samples are retrieved from an index of the training set and used to augment the input to the model in a few-shot learning setting.</li>
<li><strong><code>index_name</code></strong> (default: <code>null</code>): The name of the index to use for the prompt. Indices are stored in the ludwig cache by default.</li>
<li><strong><code>model_name</code></strong> (default: <code>null</code>): The model used to generate the embeddings used to retrieve samples to inject in the prompt.</li>
<li><strong><code>k</code></strong> (default: <code>0</code>): The number of samples to retrieve.</li>
</ul>
<h2 id="max-sequence-lengths">Max Sequence Lengths<a class="headerlink" href="#max-sequence-lengths" title="Permanent link">&para;</a></h2>
<p>There are <a href="https://www.youtube.com/watch?v=g68qlo9Izf0&amp;t=2685s">many factors at play</a>
when it comes to fine-tuning LLMs efficiently on a single GPU.</p>
<p>One of the most important parameters in your control to keep GPU memory usage in
check is the choice of the maximum sequence length.</p>
<p>Ludwig provides 3 primary knobs to control max sequence lengths:</p>
<ol>
<li><code>input_feature.preprocessing.max_sequence_length</code> on the input example, which includes your prompt.</li>
<li><code>output_feature.preprocessing.max_sequence_length</code> on the output example, which does not include your prompt.</li>
<li><code>preprocessing.global_max_sequence_length</code>, which is the maximum length sequence (merged input and output) fed to the LLM's forward pass during training.</li>
</ol>
<p><img alt="img" src="../../images/max_sequence_lengths.png" /></p>
<p>If you are running into GPU OOM issues, consider profiling your dataset to
understand the distribution of sequence lengths. For input/output columns with a
long tail distribution, it may be worth considering choosing a smaller max
sequence length as to truncate a small portion of your data while still training
with smaller GPUs.</p>
<h2 id="adapter">Adapter<a class="headerlink" href="#adapter" title="Permanent link">&para;</a></h2>
<p>One of the biggest barriers to cost effective fine-tuning for LLMs is the need to update billions of parameters each training step. Parameter efficient fine-tuning (PEFT) adatpers are a collection of techniques that reduce the number of trainable parameters during fine-tuning to speed up training, and decrease the memory and disk space required to train large language models.</p>
<p><a href="https://github.com/huggingface/peft">PEFT</a> is a popular library from HuggingFace that implements a number of popular parameter efficient fine-tuning strategies. Ludwig provides native integration with PEFT, allowing you to leverage any number of techniques to more efficiently fine-tune LLMs through
the <code>adapter</code> config parameter.</p>
<h4 id="lora">LoRA<a class="headerlink" href="#lora" title="Permanent link">&para;</a></h4>
<p>LoRA is a simple, yet effective, method for parameter-efficient fine-tuning of pretrained language models.
It works by adding a small number of trainable parameters to the model, which are used to adapt the
pretrained parameters to the downstream task. This allows the model to be fine-tuned with a much smaller
number of training examples, and can even be used to fine-tune models on tasks that have no training data
available at all.</p>
<div class="highlight"><pre><span></span><code><span class="nt">adapter</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lora</span>
<span class="w">    </span><span class="nt">r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
<span class="w">    </span><span class="nt">target_modules</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="w">    </span><span class="nt">pretrained_adapter_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">postprocessor</span><span class="p">:</span>
<span class="w">        </span><span class="nt">merge_adapter_into_base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">progressbar</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">bias_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">none</span>
</code></pre></div>
<ul>
<li><strong><code>r</code></strong> (default: <code>8</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Lora attention dimension.</li>
<li><strong><code>dropout</code></strong> (default: <code>0.05</code>): The dropout probability for Lora layers.</li>
<li><strong><code>target_modules</code></strong> (default: <code>null</code>): List of module names or regex expression of the module names to replace with LoRA. For example, ['q', 'v'] or '.<em>decoder.</em>(SelfAttention|EncDecAttention).*(q|v)$'. Defaults to targeting the query and value matrices of all self-attention and encoder-decoder attention layers.</li>
<li><strong><code>alpha</code></strong> (default: <code>null</code>): The alpha parameter for Lora scaling. Defaults to <code>2 * r</code>.</li>
<li><strong><code>pretrained_adapter_weights</code></strong> (default: <code>null</code>): Path to pretrained weights.</li>
<li><strong><code>postprocessor</code></strong> : </li>
<li><strong><code>postprocessor.merge_adapter_into_base_model</code></strong> (default: <code>false</code>): Instructs whether or not the fine-tuned LoRA weights are to be merged into the base LLM model so
that the complete fine-tuned model is available to be used and/or persisted, and then reused upon loading as a single
model (rather than having to load base and fine-tuned models separately). Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>postprocessor.progressbar</code></strong> (default: <code>false</code>): Instructs whether or not to show a progress bar indicating the unload and merge process. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bias_type</code></strong> (default: <code>none</code>): Bias type for Lora. Options: <code>none</code>, <code>all</code>, <code>lora_only</code>.</li>
</ul>
<h4 id="adalora">AdaLoRA<a class="headerlink" href="#adalora" title="Permanent link">&para;</a></h4>
<p>AdaLoRA is an extension of LoRA that allows the model to adapt the pretrained parameters to the downstream
task in a task-specific manner. This is done by adding a small number of trainable parameters to the model,
which are used to adapt the pretrained parameters to the downstream task. This allows the model to be
fine-tuned with a much smaller number of training examples, and can even be used to fine-tune models on tasks
that have no training data available at all.</p>
<div class="highlight"><pre><span></span><code><span class="nt">adapter</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adalora</span>
<span class="w">    </span><span class="nt">r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
<span class="w">    </span><span class="nt">target_modules</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="w">    </span><span class="nt">pretrained_adapter_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">postprocessor</span><span class="p">:</span>
<span class="w">        </span><span class="nt">merge_adapter_into_base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">progressbar</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">bias_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">none</span>
<span class="w">    </span><span class="nt">target_r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">init_r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">12</span>
<span class="w">    </span><span class="nt">tinit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">tfinal</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">delta_t</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">beta1</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.85</span>
<span class="w">    </span><span class="nt">beta2</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.85</span>
<span class="w">    </span><span class="nt">orth_reg_weight</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>
<span class="w">    </span><span class="nt">total_step</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">rank_pattern</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</code></pre></div>
<ul>
<li><strong><code>r</code></strong> (default: <code>8</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Lora attention dimension.</li>
<li><strong><code>dropout</code></strong> (default: <code>0.05</code>): The dropout probability for Lora layers.</li>
<li><strong><code>target_modules</code></strong> (default: <code>null</code>): List of module names or regex expression of the module names to replace with LoRA. For example, ['q', 'v'] or '.<em>decoder.</em>(SelfAttention|EncDecAttention).*(q|v)$'. Defaults to targeting the query and value matrices of all self-attention and encoder-decoder attention layers.</li>
<li><strong><code>alpha</code></strong> (default: <code>null</code>): The alpha parameter for Lora scaling. Defaults to <code>2 * r</code>.</li>
<li><strong><code>pretrained_adapter_weights</code></strong> (default: <code>null</code>): Path to pretrained weights.</li>
<li><strong><code>postprocessor</code></strong> : </li>
<li><strong><code>postprocessor.merge_adapter_into_base_model</code></strong> (default: <code>false</code>): Instructs whether or not the fine-tuned LoRA weights are to be merged into the base LLM model so
that the complete fine-tuned model is available to be used and/or persisted, and then reused upon loading as a single
model (rather than having to load base and fine-tuned models separately). Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>postprocessor.progressbar</code></strong> (default: <code>false</code>): Instructs whether or not to show a progress bar indicating the unload and merge process. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bias_type</code></strong> (default: <code>none</code>): Bias type for Lora. Options: <code>none</code>, <code>all</code>, <code>lora_only</code>.</li>
<li><strong><code>target_r</code></strong> (default: <code>8</code>): Target Lora Matrix Dimension. The target average rank of incremental matrix.</li>
<li><strong><code>init_r</code></strong> (default: <code>12</code>): Initial Lora Matrix Dimension. The initial rank for each incremental matrix.</li>
<li><strong><code>tinit</code></strong> (default: <code>0</code>): The steps of initial fine-tuning warmup.</li>
<li><strong><code>tfinal</code></strong> (default: <code>0</code>): The steps of final fine-tuning warmup.</li>
<li><strong><code>delta_t</code></strong> (default: <code>1</code>): The time internval between two budget allocations. The step interval of rank allocation.</li>
<li><strong><code>beta1</code></strong> (default: <code>0.85</code>): The hyperparameter of EMA for sensitivity smoothing.</li>
<li><strong><code>beta2</code></strong> (default: <code>0.85</code>):  The hyperparameter of EMA for undertainty quantification.</li>
<li><strong><code>orth_reg_weight</code></strong> (default: <code>0.5</code>): The coefficient of orthogonality regularization.</li>
<li><strong><code>total_step</code></strong> (default: <code>null</code>): The total training steps that should be specified before training.</li>
<li><strong><code>rank_pattern</code></strong> (default: <code>null</code>): The allocated rank for each weight matrix by RankAllocator.</li>
</ul>
<h4 id="adaption-prompt">Adaption Prompt<a class="headerlink" href="#adaption-prompt" title="Permanent link">&para;</a></h4>
<p>Adaption Prompt is taken from the paper
<a href="https://arxiv.org/pdf/2303.16199.pdf">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a>.
It adds a set of learnable adaption prompts and prepends them to the word tokens at higher transformer layers.
Then, a zero-initialized attention mechanism with zero gating is introduced, which adaptively injects
new instructional cues into LLaMA, while effectively preserving its pre-trained knowledge. According to
the paper, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned
7B parameters.</p>
<div class="highlight"><pre><span></span><code><span class="nt">adapter</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adaption_prompt</span>
<span class="w">    </span><span class="nt">adapter_len</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">adapter_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">pretrained_adapter_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">postprocessor</span><span class="p">:</span>
<span class="w">        </span><span class="nt">merge_adapter_into_base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">progressbar</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong><code>adapter_len</code></strong> (default: <code>4</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of adapter tokens to insert.</li>
<li><strong><code>adapter_layers</code></strong> (default: <code>1</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Number of adapter layers to insert (from the top).</li>
<li><strong><code>pretrained_adapter_weights</code></strong> (default: <code>null</code>): Path to pretrained weights.</li>
<li><strong><code>postprocessor</code></strong> : </li>
<li><strong><code>postprocessor.merge_adapter_into_base_model</code></strong> (default: <code>false</code>): Instructs whether or not the fine-tuned LoRA weights are to be merged into the base LLM model so
that the complete fine-tuned model is available to be used and/or persisted, and then reused upon loading as a single
model (rather than having to load base and fine-tuned models separately). Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>postprocessor.progressbar</code></strong> (default: <code>false</code>): Instructs whether or not to show a progress bar indicating the unload and merge process. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h4 id="ia3">IA3<a class="headerlink" href="#ia3" title="Permanent link">&para;</a></h4>
<p><a href="https://arxiv.org/pdf/2205.05638.pdf">Infused Adapter by Inhibiting and Amplifying Inner Activations</a>, or IA3,
is a method that adds three learned vectors <code>l_k``,</code>l_v<code>`, and</code>l_ff`, to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network respectively. These learned vectors are the only trainable parameters during fine-tuning, and thus the original weights remain frozen. Dealing with learned vectors (as opposed to learned low-rank updates to a weight matrix like LoRA) keeps the number of trainable parameters much smaller.</p>
<div class="highlight"><pre><span></span><code><span class="nt">adapter</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ia3</span>
<span class="w">    </span><span class="nt">target_modules</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">feedforward_modules</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">fan_in_fan_out</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">modules_to_save</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">init_ia3_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">pretrained_adapter_weights</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">postprocessor</span><span class="p">:</span>
<span class="w">        </span><span class="nt">merge_adapter_into_base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">        </span><span class="nt">progressbar</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong><code>target_modules</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The names of the modules to apply (IA)^3 to.</li>
<li><strong><code>feedforward_modules</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The names of the modules to be treated as feedforward modules, as in the original paper. These modules will have (IA)^3 vectors multiplied to the input, instead of the output. feedforward_modules must be a name or a subset of names present in target_modules.</li>
<li><strong><code>fan_in_fan_out</code></strong> (default: <code>false</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses Conv1D which stores weights like (fan_in, fan_out) and hence this should be set to True.  Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>modules_to_save</code></strong> (default: <code>null</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: List of modules apart from (IA)^3 layers to be set as trainable and saved in the final checkpoint.</li>
<li><strong><code>init_ia3_weights</code></strong> (default: <code>true</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Whether to initialize the vectors in the (IA)^3 layers, defaults to True. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>pretrained_adapter_weights</code></strong> (default: <code>null</code>): Path to pretrained weights.</li>
<li><strong><code>postprocessor</code></strong> : </li>
<li><strong><code>postprocessor.merge_adapter_into_base_model</code></strong> (default: <code>false</code>): Instructs whether or not the fine-tuned LoRA weights are to be merged into the base LLM model so
that the complete fine-tuned model is available to be used and/or persisted, and then reused upon loading as a single
model (rather than having to load base and fine-tuned models separately). Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>postprocessor.progressbar</code></strong> (default: <code>false</code>): Instructs whether or not to show a progress bar indicating the unload and merge process. Options: <code>true</code>, <code>false</code>.</li>
</ul>
<h2 id="quantization">Quantization<a class="headerlink" href="#quantization" title="Permanent link">&para;</a></h2>
<p>Quantization allows you to load model parameters, which are typically stored
as 16 or 32 bit floating-points, as 4 bit or 8 bit integers. This allows
you to reduce the GPU memory overhead by a factor of up to 8x.</p>
<p>When combined with the LoRA <a href="#adapter">adapter</a>, you can perform quantized
fine-tuning as described in the paper <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>. For
context, this enables training large language models as big as 7 billion parameters
on a single commodity GPU with minimal performance penalties.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Quantized fine-tuning currently requires using <code>adapter: lora</code>. In-context
learning does not have this restriction.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Quantization is currently only supported with <code>backend: local</code>.</p>
</div>
<div class="highlight"><pre><span></span><code><span class="nt">quantization</span><span class="p">:</span>
<span class="w">    </span><span class="nt">bits</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">llm_int8_threshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6.0</span>
<span class="w">    </span><span class="nt">llm_int8_has_fp16_weight</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">bnb_4bit_compute_dtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">float16</span>
<span class="w">    </span><span class="nt">bnb_4bit_use_double_quant</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">bnb_4bit_quant_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nf4</span>
</code></pre></div>
<ul>
<li><strong><code>bits</code></strong> (default: <code>4</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The quantization level to apply to weights on load. Options: <code>4</code>, <code>8</code>.</li>
<li><strong><code>llm_int8_threshold</code></strong> (default: <code>6.0</code>): This corresponds to the outlier threshold for outlier detection as described in <code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code> paper: https://arxiv.org/abs/2208.07339. Any hidden states value that is above this threshold will be considered an outlier and the operation on those values will be done in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but there are some exceptional systematic outliers that are very differently distributed for large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6, but a lower threshold might be needed for more unstable models (small models, fine-tuning).</li>
<li><strong><code>llm_int8_has_fp16_weight</code></strong> (default: <code>false</code>): This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not have to be converted back and forth for the backward pass. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bnb_4bit_compute_dtype</code></strong> (default: <code>float16</code>): This sets the computational type which might be different than the input type. For example, inputs might be fp32, but computation can be set to bf16 for speedups. Options: <code>float32</code>, <code>float16</code>, <code>bfloat16</code>.</li>
<li><strong><code>bnb_4bit_use_double_quant</code></strong> (default: <code>true</code>): This flag is used for nested quantization where the quantization constants from the first quantization are quantized again. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>bnb_4bit_quant_type</code></strong> (default: <code>nf4</code>): This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options: <code>fp4</code>, <code>nf4</code>.</li>
</ul>
<h2 id="model-parameters">Model Parameters<a class="headerlink" href="#model-parameters" title="Permanent link">&para;</a></h2>
<p>The model parameters section is used to customized LLM model parameters during model initialization.
Currently, the only supported initialization parameter is <code>rope_scaling</code>.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Defaults</span>
<span class="nt">model_parameters</span><span class="p">:</span>
<span class="w">  </span><span class="nt">rope_scaling</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="w">  </span><span class="nt">neftune_noise_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</code></pre></div>
<h3 id="rope-scaling">RoPE Scaling<a class="headerlink" href="#rope-scaling" title="Permanent link">&para;</a></h3>
<p>Large language models like LLaMA-2 face a limitation in the length of context they can consider, which impacts their
capacity to comprehend intricate queries or chat-style discussions spanning multiple paragraphs. For instance,
LlaMA-2's context is capped at 4096 tokens, or roughly 3000 English words. This renders the model ineffective for
tasks involving lengthy documents that surpass this context length.</p>
<p><strong>RoPE Scaling</strong> presents a way to increase the context length of your model at the cost of a slight performance
penalty using a method called Position Interpolation. You can read more about it in the original paper
<a href="https://arxiv.org/pdf/2306.15595.pdf">here</a>.</p>
<p>There are two parameters to consider for RoPE scaling: <code>type</code> and <code>factor</code>. The typical rule of thumb is that
your new context length will be <code>context_length</code> * <code>factor</code>. So, if you want to extend LLaMA-2 to have a context
length of ~ 16K tokens, you would set the <code>factor</code> to 4.0. The <code>type</code> attribute supports <code>linear</code> interpolation
and <code>dynamic</code> interpolation. Typically, <code>dynamic</code> interpolation has the best performance over larger context lengths
while maintaining low perplexity.</p>
<p><img alt="" src="../../images/rope_scaling.webp" />
<em>Credit to /u/emozilla and /u/kaiokendev on Reddit for their work and this graphic.</em></p>
<p>You can enable RoPE Scaling in Ludwig using the following config:</p>
<div class="highlight"><pre><span></span><code><span class="nt">rope_scaling</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dynamic</span>
<span class="w">    </span><span class="nt">factor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2.0</span>
</code></pre></div>
<ul>
<li><strong><code>type</code></strong> (default: <code>null</code>): Currently supports two strategies: linear and dynamic scaling. Options: <code>linear</code>, <code>dynamic</code>, <code>null</code>.</li>
<li><strong><code>factor</code></strong> (default: <code>null</code>): The scaling factor for RoPE embeddings.</li>
</ul>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Typically, you need to fine-tune your LLM for about 1000 steps with RoPE scaling enabled
to ensure that the performance drop with RoPE scaling is minimal and the model adapts your data
to the new RoPE embeddings.</p>
</div>
<h3 id="neftune-noise-alpha">Neftune Noise Alpha<a class="headerlink" href="#neftune-noise-alpha" title="Permanent link">&para;</a></h3>
<p>NEFTune is a technique to boost the performance of models during fine-tuning. NEFTune adds noise to the embedding
vectors during training. The alpha parameter serves as a control mechanism, allowing users to regulate the intensity of noise introduced to embeddings. A higher alpha value corresponds to a greater amount of noise, impacting the embedding vectors during
the fine-tuning phase.</p>
<p>Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to
64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. You can find more information <a href="available at https://arxiv.org/pdf/2310.05914.pdf">in the paper</a> titled "NEFTune: Noisy Embeddings Improve Instruction Finetuning".</p>
<p><img alt="" src="../../images/neftune_performance.png" /></p>
<p>You can enable NEFTune in Ludwig using the following config:</p>
<div class="highlight"><pre><span></span><code><span class="nt">model_parameters</span><span class="p">:</span>
<span class="w">  </span><span class="nt">neftune_noise_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</code></pre></div>
<h2 id="trainer">Trainer<a class="headerlink" href="#trainer" title="Permanent link">&para;</a></h2>
<p>LLMs support multiple different training objectives:</p>
<ul>
<li><strong>Fine-Tuning</strong> (<code>type: finetune</code>): update the weights of a pretrained LLM with supervised learning.</li>
<li><strong>In-Context Learning</strong> (<code>type: none</code>): evaluate model performance and predict using only context provided in the prompt.</li>
</ul>
<h3 id="fine-tuning">Fine-Tuning<a class="headerlink" href="#fine-tuning" title="Permanent link">&para;</a></h3>
<p>For fine-tuning, see the <a href="../trainer/">Trainer</a> section for configuration
options.</p>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">finetune</span>
</code></pre></div>
<h3 id="in-context-learning">In-Context Learning<a class="headerlink" href="#in-context-learning" title="Permanent link">&para;</a></h3>
<p>For in-context learning, the <code>none</code> trainer is specified to denote that no
model parameters will be updated and the "training" step will essentially be
a no-op, except for the purpose of computing metrics on the test set.</p>
<div class="highlight"><pre><span></span><code><span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">none</span>
</code></pre></div>
<h2 id="generation">Generation<a class="headerlink" href="#generation" title="Permanent link">&para;</a></h2>
<p>When generating text during inference using a pretrained or fine-tuned LLM, you may
often want to control the generation process, such as what token decoding strategy to use,
how many new tokens to produce, which tokens to exclude, or how diverse you want the generated
text to be. All of these can be controlled through the <code>generation</code> config in Ludwig.</p>
<p>While Ludwig sets predefined default values for most of these parameters, some of the most useful parameters
to control the generation process are:</p>
<ul>
<li><code>max_new_tokens</code></li>
<li><code>temperature</code></li>
<li><code>do_sample</code></li>
<li><code>num_beams</code></li>
<li><code>top_k</code></li>
<li><code>top_p</code></li>
</ul>
<p>Check out the description for these parameters below!</p>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">    </span><span class="nt">max_new_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">    </span><span class="nt">temperature</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">    </span><span class="nt">min_new_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">max_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">    </span><span class="nt">min_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">use_cache</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">top_k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span>
<span class="w">    </span><span class="nt">top_p</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">early_stopping</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">max_time</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">num_beam_groups</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">penalty_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">typical_p</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">epsilon_cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">eta_cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">diversity_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">    </span><span class="nt">repetition_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">encoder_repetition_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">length_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">no_repeat_ngram_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">bad_words_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">force_words_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">renormalize_logits</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">forced_bos_token_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">forced_eos_token_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">remove_invalid_values</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">exponential_decay_length_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">suppress_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">begin_suppress_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">forced_decoder_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">sequence_bias</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">guidance_scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">pad_token_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">bos_token_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">eos_token_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</code></pre></div>
<ul>
<li><strong><code>max_new_tokens</code></strong> (default: <code>32</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: The maximum number of new tokens to generate, ignoring the number of tokens in the input prompt. If not set, this is dynamically determined by Ludwig based on either the <code>max_sequence_length</code> of the ouput feature, the global_max_sequence_length specified in preprocessing (if specified), or the maximum context length supported by the model (in the order specified).</li>
<li><strong><code>temperature</code></strong> (default: <code>0.1</code>) <span class="twemoji" title="High impact parameter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.69 2h10.56c.966 0 1.75.784 1.75 1.75v17.5a.75.75 0 0 1-1.218.585L12 17.21l-5.781 4.626A.75.75 0 0 1 5 21.253L4.94 3.756A1.748 1.748 0 0 1 6.69 2Z"/></svg></span>: Temperature is used to control the randomness of predictions. A high temperature value (closer to 1) makes the output more diverse and random, while a lower temperature (closer to 0) makes the model's responses more deterministic and focused on the most likely outcome. In other words, temperature adjusts the probability distribution from which the model picks the next token.</li>
<li><strong><code>min_new_tokens</code></strong> (default: <code>null</code>): The minimum number of new tokens to generate, ignoring the number of tokens in the input prompt.</li>
<li><strong><code>max_length</code></strong> (default: <code>32</code>): The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens. Its effect is overridden by max_new_tokens, if also set.</li>
<li><strong><code>min_length</code></strong> (default: <code>0</code>): The minimum length of the sequence to be generated. Corresponds to the length of the input prompt + min_new_tokens. Its effect is overridden by min_new_tokens, if also set.</li>
<li><strong><code>do_sample</code></strong> (default: <code>true</code>): Whether or not to use sampling ; use greedy decoding otherwise. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>num_beams</code></strong> (default: <code>1</code>): Number of beams for beam search. 1 means no beam search and is the default value. The beam search strategy generates the translation word by word from left-to-right while keeping a fixed number (beam) of active candidates at each time step during token generation. By increasing the beam size, the translation performance can increase at the expense of significantly reducing the decoder speed.</li>
<li><strong><code>use_cache</code></strong> (default: <code>true</code>): Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>top_k</code></strong> (default: <code>50</code>): The number of highest probability vocabulary tokens to keep for top-k-filtering.</li>
<li><strong><code>top_p</code></strong> (default: <code>1.0</code>): If set to float &lt; 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.</li>
<li><strong><code>early_stopping</code></strong> (default: <code>false</code>): Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values: True, where the generation stops as soon as there are num_beams complete candidates; False, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates; <code>never</code>, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm) Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>max_time</code></strong> (default: <code>null</code>): The maximum amount of time you allow the computation to run for in seconds. generation will still finish the current pass after allocated time has been passed. </li>
<li><strong><code>num_beam_groups</code></strong> (default: <code>1</code>): Number of groups to divide num_beams into in order to ensure diversity among different groups of beams. 1 means no group beam search.</li>
<li><strong><code>penalty_alpha</code></strong> (default: <code>null</code>): The values balance the model confidence and the degeneration penalty in contrastive  search decoding.</li>
<li><strong><code>typical_p</code></strong> (default: <code>1.0</code>): Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated. If set to float &lt; 1, the smallest set of the most locally typical tokens with probabilities that add up to typical_p or higher are kept for generation.</li>
<li><strong><code>epsilon_cutoff</code></strong> (default: <code>0.0</code>): If set to float strictly between 0 and 1, only tokens with a conditional probability greater than epsilon_cutoff will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on the size of the model.</li>
<li><strong><code>eta_cutoff</code></strong> (default: <code>0.0</code>): Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly between 0 and 1, a token is only considered if it is greater than either eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.</li>
<li><strong><code>diversity_penalty</code></strong> (default: <code>0.0</code>): The value used to control the diversity of the generated text. The higher the value, the more diverse the text will be. If set to 0, no diversity is enforced.This value is subtracted from a beam(s) score if it generates a token same as any beam from other group at aparticular time. Note that diversity_penalty is only effective if group beam search is enabled.</li>
<li><strong><code>repetition_penalty</code></strong> (default: <code>1.0</code>): The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf">this paper</a> for more details.</li>
<li><strong><code>encoder_repetition_penalty</code></strong> (default: <code>1.0</code>): The paramater for encoder_repetition_penalty. An exponential penalty on sequences that are not in the original input. 1.0 means no penalty.</li>
<li><strong><code>length_penalty</code></strong> (default: <code>1.0</code>): Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log likelihood of the sequence (i.e. negative), length_penalty &gt; 0.0 promotes longer sequences, while length_penalty &lt; 0.0 encourages shorter sequences.</li>
<li><strong><code>no_repeat_ngram_size</code></strong> (default: <code>0</code>): If set to int &gt; 0, all ngrams of that size can only occur once.</li>
<li><strong><code>bad_words_ids</code></strong> (default: <code>null</code>): List of token ids that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use tokenizer(bad_word, add_prefix_space=True).input_ids.</li>
<li><strong><code>force_words_ids</code></strong> (default: <code>null</code>): List of token ids that are forced to be generated by the model. In order to get the tokens of the words that should appear in the generated text, use tokenizer(force_word, add_prefix_space=True).input_ids.</li>
<li><strong><code>renormalize_logits</code></strong> (default: <code>false</code>): Whether to renormalize the logits after temperature and top_k/top_p filtering. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>forced_bos_token_id</code></strong> (default: <code>null</code>): The id of the token to force as the first generated token after the decoder_start_token_id.Useful for multilingual models like mBART where the first generated token needs to be the target languagetoken.</li>
<li><strong><code>forced_eos_token_id</code></strong> (default: <code>null</code>): The id of the token to force as the last generated token when max_length is reached. Optionally, use a list to set multiple end-of-sequence tokens.</li>
<li><strong><code>remove_invalid_values</code></strong> (default: <code>false</code>): Whether to remove possible nan and inf outputs of the model to prevent the generation method to crash. Note that using remove_invalid_values can slow down generation. Options: <code>true</code>, <code>false</code>.</li>
<li><strong><code>exponential_decay_length_penalty</code></strong> (default: <code>null</code>): This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been generated. The tuple shall consist of: (start_index, decay_factor) where start_index indicates where penalty starts and decay_factor represents the factor of exponential decay</li>
<li><strong><code>suppress_tokens</code></strong> (default: <code>null</code>): A list of tokens that will be suppressed at generation. The SupressTokens logit processor will set their log probs to -inf so that they are not sampled.</li>
<li><strong><code>begin_suppress_tokens</code></strong> (default: <code>null</code>): A list of tokens that will be suppressed at the beginning of the generation. The SupressBeginTokens logit processor will set their log probs to -inf so that they are not sampled.</li>
<li><strong><code>forced_decoder_ids</code></strong> (default: <code>null</code>): A list of forced decoder ids. The ForcedDecoderIds logit processor will set the log probs of all tokens that are not in the list to -inf so that they are not sampled.</li>
<li><strong><code>sequence_bias</code></strong> (default: <code>null</code>): A dictionary of token ids to bias the generation towards. The SequenceBias logit processor will add the bias to the log probs of the tokens in the dictionary. Positive biases increase the odds of the sequence being selected, while negative biases do the opposite. </li>
<li><strong><code>guidance_scale</code></strong> (default: <code>null</code>): The guidance scale for classifier free guidance (CFG). CFG is enabled by setting guidance_scale &gt; 1. Higher guidance scale encourages the model to generate samples that are more closely linked to the input prompt, usually at the expense of poorer quality.</li>
<li><strong><code>pad_token_id</code></strong> (default: <code>null</code>): The id of the padding token. If not set, the padding token id of the tokenizer is used.</li>
<li><strong><code>bos_token_id</code></strong> (default: <code>null</code>): The id of the beginning of sentence token. If not set, the bos token id of the tokenizer is used.</li>
<li><strong><code>eos_token_id</code></strong> (default: <code>null</code>): The id of the end of sentence token. If not set, the eos token id of the tokenizer is used.</li>
</ul>
<h3 id="generation-strategies">Generation Strategies<a class="headerlink" href="#generation-strategies" title="Permanent link">&para;</a></h3>
<p>Text generation can be performed in a variety of ways for inference. Broadly, there are 5 strategies:</p>
<ul>
<li><strong>Greedy Decoding (default)</strong>: Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word at each time step <code>t</code>.</li>
<li><strong>Beam Search</strong>: Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely <code>num_beams</code> of hypotheses at each time step <code>t</code> and eventually choosing the hypothesis that has the overall highest probability.</li>
<li><strong>Sampling</strong>: Sampling means randomly picking the next word according to its conditional probability distribution. Language generation using sampling is not deterministic.</li>
<li><strong>Top-k Sampling</strong>: In Top-k sampling, the <code>k</code> most likely next words are filtered and the probability mass is redistributed among only those <code>k</code> next words.</li>
<li><strong>Top-p (nucleus) sampling</strong>: Instead of sampling only from the most likely K words, Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability <code>p</code>. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution.</li>
</ul>
<p>If you want to enable a decoding strategy other than <strong>greedy decoding</strong>, you can set the following parameters in the generation config to enable them.</p>
<ul>
<li><strong>Greedy Decoding (default)</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong>Multinomial Sampling</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong>Beam-Search Decoding</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Must be &gt; 1</span>
<span class="w">  </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</code></pre></div>
<ul>
<li><strong>Contrastive Search</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">penalty_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"> </span><span class="c1"># Must be &gt; 0</span>
<span class="w">  </span><span class="nt">top_k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Must be &gt; 1</span>
</code></pre></div>
<ul>
<li><strong>Beam-Search Multinomial Sampling</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Must be &gt; 1</span>
<span class="w">  </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<ul>
<li><strong>Diverse Beam-Search Decoding</strong>:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nt">generation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_beams</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Must be &gt; 1</span>
<span class="w">  </span><span class="nt">num_beam_groups</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Must be &gt; 1</span>
</code></pre></div>
<p>To read more about how these decoding strategies work in a visual manner, check out <a href="https://huggingface.co/blog/how-to-generate">this</a> excellent blogpost by HuggingFace.</p>
<h2 id="post-fine-tuning">Post Fine-Tuning<a class="headerlink" href="#post-fine-tuning" title="Permanent link">&para;</a></h2>
<h3 id="uploading-fine-tuned-llm-weights-to-huggingface-hub">Uploading Fine-Tuned LLM weights to HuggingFace Hub<a class="headerlink" href="#uploading-fine-tuned-llm-weights-to-huggingface-hub" title="Permanent link">&para;</a></h3>
<p>Once you've fine-tuned your model, you can upload your fine-tuned model artifacts straight
to HuggingFace Hub, either in a public model repository that anyone can access, or to a private
repository. This works for both artifacts produced during full fine-tuning, as well as adapter
based fine-tuning. From there, you can pull the weights straight into a downstream inference service,
or even use it directly through Ludwig for inference.</p>
<div class="highlight"><pre><span></span><code>ludwig<span class="w"> </span>upload<span class="w"> </span>hf_hub<span class="w"> </span>--repo_id<span class="w"> </span>&lt;repo_id&gt;<span class="w"> </span>--model_path<span class="w"> </span>&lt;/path/to/saved/model&gt;
</code></pre></div>
<p>To learn more on how to do this, click <a href="../../user_guide/api/LudwigModel/#upload_to_hf_hub">here</a>.</p>


  




                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <!--
  Copyright (c) 2016-2022 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Footer -->
<footer class="md-footer">

    <!-- Link to previous and/or next page -->
    
    <nav class="md-footer__inner md-grid" aria-label="footer.title">

        <!-- Link to previous page -->
        
        
        <a href="../model_type/" class="md-footer__link md-footer__link--prev"
            aria-label="Previous: Model Types" rel="prev">
            <div class="md-footer__button md-icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
                <div class="md-ellipsis">
                    <span class="md-footer__direction">
                        Previous
                    </span>
                    Model Types
                </div>
            </div>
        </a>
        

        <!-- Link to next page -->
        
        
        <a href="../preprocessing/" class="md-footer__link md-footer__link--next"
            aria-label="Next: Preprocessing" rel="next">
            <div class="md-footer__title">
                <div class="md-ellipsis">
                    <span class="md-footer__direction">
                        Next
                    </span>
                    Preprocessing
                </div>
            </div>
            <div class="md-footer__button md-icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
        </a>
        
    </nav>
    

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">
            <!--
  Copyright (c) 2016-2021 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Copyright and theme information -->
<div class="md-copyright">
    
    <div class="md-copyright__highlight">
        Copyright &copy; 2018 - 2020 Uber Technologies Inc., 2021 - 2022 Linux Foundation Data & AI
    </div>
    
    

    Website by <a href="http://w4nderlu.st">w4nderlust</a> powered by
    <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>,
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">Material for MkDocs</a>,
    <a href="http://www.styleshout.com/" target="_blank" rel="noopener">styleshout</a> and
    <a href="http://cables.gl/" target="_blank" rel="noopener">cables</a>.
    
</div>

            <!-- Social links -->
            
        </div>
    </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes", "navigation.tabs.sticky"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.aecac24b.min.js"></script>
      
    
  </body>
</html>