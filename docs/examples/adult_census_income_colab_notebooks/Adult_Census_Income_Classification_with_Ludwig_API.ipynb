{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adult Census Income Classification - Ludwig API\n",
        "\n",
        "We recommend using a GPU runtime for this example. In the Colab menu bar, choose Runtime > Change Runtime Type and choose GPU under Hardware Accelerator.\n",
        "\n",
        "In this notebook, we will show how to use the Ludwig CLI to:\n",
        "\n",
        "1. Download a Dataset\n",
        "1. Train a Ludwig Model\n",
        "1. Evaluate the trained model\n",
        "1. Visualize training and test metrics\n",
        "1. Make predictions on New Data"
      ],
      "metadata": {
        "id": "63peGQ1F580h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "g7QrNaqk51-b",
        "outputId": "60ba2da1-a4e8-4bc8-9e81-86d220afea1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 10.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 398 kB 71.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 59.6 MB/s \n",
            "\u001b[?25h  Building wheel for ludwig (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# TODO: revert to ludwig-ai repo when issue #1885 fixed\n",
        "# Prerequisite: Installs the latest version of Ludwig in the Colab environment\n",
        "#!python -m pip install git+https://github.com/ludwig-ai/ludwig.git --quiet\n",
        "\n",
        "!python -m pip install git+https://github.com/jimthompson5802/ludwig.git@fix-iss1885-roc-curve-viz-error  --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset\n",
        "\n",
        "[Adult Census Income](https://archive.ics.uci.edu/ml/datasets/adult) is an extract of 1994 Census data for predicting whether a person's income exceeds $50K per year.  The data set consists of over 49K records with 14 attributes with missing data.\n",
        "\n",
        "\n",
        "The columns in the dataset are\n",
        "\n",
        "| column         | description                                                  |\n",
        "|----------------|--------------------------------------------------------------|\n",
        "| age            | numeric variable, age of person                              |\n",
        "| workclass      | categorical variable, Type of empolyment                     |\n",
        "| fnlwgt         | numeric variable, no defintion                               |\n",
        "| education      | categorical variable, education level                        |\n",
        "| education-num  | nmeric variable, no definition                               |\n",
        "| marital-status | categorical variable, marital status                         |\n",
        "| occupation     | categorical variable, occupation                             |\n",
        "| relationship   | categorical variable, Relationship to household              |\n",
        "| race           | categorical variable, race                                   |\n",
        "| sex            | categorical variable, gender                                 |\n",
        "| capital-gain   | numeric variable, no definition                              |\n",
        "| capital-loss   | numeric variable, no definition                              |\n",
        "| hours-per-week | numeric variable, hours worked per week                      |\n",
        "| native-country | categorical variable, Country of origin                      |\n",
        "| income         | binary variable, \" <=50K\" or \" >50K\"                         |\n",
        "| split          | numeric variable, indicating data split training(0), test(2) |\n"
      ],
      "metadata": {
        "id": "7bjMaDAR6XKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ludwig.datasets import adult_census_income\n",
        "\n",
        "# Loads the dataset as a pandas.DataFrame\n",
        "train_df, test_df, _ = adult_census_income.load(split=True)"
      ],
      "metadata": {
        "id": "YWyD8hHK6C3B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display sample training and testdata"
      ],
      "metadata": {
        "id": "OX7MENPe6tkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# setup pandas output options\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# make reproducible output sample\n",
        "np.random.seed(31)\n",
        "\n",
        "# Print sample of trainig and test data sets.\n",
        "print(f\"Training Dataset(train_df):\\n{train_df.sample(n=5)}\")\n",
        "print(f\"\\nTest Dataset(test_df):\\n{test_df.sample(n=5)}\")"
      ],
      "metadata": {
        "id": "qRRNixOm6oKh",
        "outputId": "1042a4d6-4215-4a6a-c16c-c3be4da22926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset(train_df):\n",
            "       age   workclass  fnlwgt     education  education-num  \\\n",
            "24279   59   State-gov  349910     Bachelors             13   \n",
            "8364    43           ?  142030       HS-grad              9   \n",
            "25546   41     Private  207779       HS-grad              9   \n",
            "6173    79     Private  120707     Doctorate             16   \n",
            "29227   40   State-gov  141858   Prof-school             15   \n",
            "\n",
            "            marital-status        occupation    relationship    race      sex  \\\n",
            "24279   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
            "8364              Divorced                 ?       Unmarried   White   Female   \n",
            "25546            Separated     Other-service   Not-in-family   White   Female   \n",
            "6173    Married-civ-spouse    Prof-specialty         Husband   White     Male   \n",
            "29227   Married-civ-spouse    Prof-specialty         Husband   White     Male   \n",
            "\n",
            "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
            "24279         10605             0              50   United-States    >50K  \n",
            "8364              0             0              40   United-States   <=50K  \n",
            "25546             0             0              40   United-States   <=50K  \n",
            "6173          20051             0              35     El-Salvador    >50K  \n",
            "29227             0             0              72   United-States   <=50K  \n",
            "\n",
            "Test Dataset(test_df):\n",
            "       age   workclass  fnlwgt      education  education-num  marital-status  \\\n",
            "36308   42     Private  188465        HS-grad              9        Divorced   \n",
            "43832   26   Local-gov  345779      Bachelors             13   Never-married   \n",
            "47974   19           ?  138153   Some-college             10   Never-married   \n",
            "42653   20     Private  376416        HS-grad              9   Never-married   \n",
            "46602   41     Private  303521      Bachelors             13        Divorced   \n",
            "\n",
            "            occupation    relationship    race      sex  capital-gain  \\\n",
            "36308    Other-service   Not-in-family   White   Female             0   \n",
            "43832   Prof-specialty   Not-in-family   White     Male             0   \n",
            "47974                ?       Own-child   White   Female             0   \n",
            "42653            Sales       Own-child   White     Male             0   \n",
            "46602   Prof-specialty   Not-in-family   White     Male          4650   \n",
            "\n",
            "       capital-loss  hours-per-week  native-country  income  \n",
            "36308             0              25   United-States   <=50K  \n",
            "43832             0              43   United-States   <=50K  \n",
            "47974             0              20   United-States   <=50K  \n",
            "42653             0              40   United-States   <=50K  \n",
            "46602             0              45   United-States   <=50K  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        "## Define ludwig config and create Ludwig model object"
      ],
      "metadata": {
        "id": "fn7GMaIB7DNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from ludwig.api import LudwigModel\n",
        "\n",
        "# define model configuration\n",
        "config = {'combiner': {'dropout': 0.2,\n",
        "              'num_fc_layers': 3,\n",
        "              'output_size': 128,\n",
        "              'type': 'concat'},\n",
        " 'input_features': [{'name': 'age', 'type': 'number'},\n",
        "                    {'name': 'workclass', 'type': 'category'},\n",
        "                    {'name': 'fnlwgt', 'type': 'number'},\n",
        "                    {'name': 'education', 'type': 'category'},\n",
        "                    {'name': 'education-num', 'type': 'number'},\n",
        "                    {'name': 'marital-status', 'type': 'category'},\n",
        "                    {'name': 'occupation', 'type': 'category'},\n",
        "                    {'name': 'relationship', 'type': 'category'},\n",
        "                    {'name': 'race', 'type': 'category'},\n",
        "                    {'name': 'sex', 'type': 'category'},\n",
        "                    {'name': 'capital-gain', 'type': 'number'},\n",
        "                    {'name': 'capital-loss', 'type': 'number'},\n",
        "                    {'name': 'hours-per-week', 'type': 'number'},\n",
        "                    {'name': 'native-country', 'type': 'category'}],\n",
        " 'output_features': [{'name': 'income',\n",
        "                      'num_fc_layers': 4,\n",
        "                      'output_size': 32,\n",
        "                      'preprocessing': {'fallback_true_label': ' >50K'},\n",
        "                      'type': 'binary'}],\n",
        " 'preprocessing': {'number': {'missing_value_strategy': 'fill_with_mean',\n",
        "                              'normalization': 'zscore'}},\n",
        " 'trainer': {'epochs': 10, 'optimizer': {'type': 'adam'}}}\n",
        "\n",
        "# instantiate Ludwig model object\n",
        "model = LudwigModel(config=config, logging_level=logging.INFO)"
      ],
      "metadata": {
        "id": "a02Ssj7Q7gzc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "RwOtoX1i8qdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains the model. This cell might take a few minutes.\n",
        "train_stats, preprocessed_data, output_directory = model.train(dataset=train_df)"
      ],
      "metadata": {
        "id": "LzJN-Iyz8Eh8",
        "outputId": "8e9c5fab-185c-4136-9092-55785c0eb4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "╒════════════════════════╕\n",
            "│ EXPERIMENT DESCRIPTION │\n",
            "╘════════════════════════╛\n",
            "\n",
            "╒══════════════════╤═════════════════════════════════════════════════════════════════════════════════════════╕\n",
            "│ Experiment name  │ api_experiment                                                                          │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ Model name       │ run                                                                                     │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ Output directory │ /content/results/api_experiment_run                                                     │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ ludwig_version   │ '0.5rc2'                                                                                │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ command          │ ('/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f '                     │\n",
            "│                  │  '/root/.local/share/jupyter/runtime/kernel-af89011e-dcc6-41c6-b07b-7b90ce88209b.json') │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ random_seed      │ 42                                                                                      │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                                 │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ torch_version    │ '1.10.0+cu111'                                                                          │\n",
            "├──────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n",
            "│ compute          │ {'num_nodes': 1}                                                                        │\n",
            "╘══════════════════╧═════════════════════════════════════════════════════════════════════════════════════════╛\n",
            "\n",
            "╒═══════════════╕\n",
            "│ LUDWIG CONFIG │\n",
            "╘═══════════════╛\n",
            "\n",
            "{   'combiner': {   'dropout': 0.2,\n",
            "                    'num_fc_layers': 3,\n",
            "                    'output_size': 128,\n",
            "                    'type': 'concat'},\n",
            "    'input_features': [   {   'column': 'age',\n",
            "                              'name': 'age',\n",
            "                              'proc_column': 'age_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'workclass',\n",
            "                              'name': 'workclass',\n",
            "                              'proc_column': 'workclass_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'fnlwgt',\n",
            "                              'name': 'fnlwgt',\n",
            "                              'proc_column': 'fnlwgt_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'education',\n",
            "                              'name': 'education',\n",
            "                              'proc_column': 'education_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'education-num',\n",
            "                              'name': 'education-num',\n",
            "                              'proc_column': 'education_num_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'marital-status',\n",
            "                              'name': 'marital-status',\n",
            "                              'proc_column': 'marital_status_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'occupation',\n",
            "                              'name': 'occupation',\n",
            "                              'proc_column': 'occupation_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'relationship',\n",
            "                              'name': 'relationship',\n",
            "                              'proc_column': 'relationship_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'race',\n",
            "                              'name': 'race',\n",
            "                              'proc_column': 'race_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'sex',\n",
            "                              'name': 'sex',\n",
            "                              'proc_column': 'sex_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'},\n",
            "                          {   'column': 'capital-gain',\n",
            "                              'name': 'capital-gain',\n",
            "                              'proc_column': 'capital_gain_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'capital-loss',\n",
            "                              'name': 'capital-loss',\n",
            "                              'proc_column': 'capital_loss_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'hours-per-week',\n",
            "                              'name': 'hours-per-week',\n",
            "                              'proc_column': 'hours_per_week_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'number'},\n",
            "                          {   'column': 'native-country',\n",
            "                              'name': 'native-country',\n",
            "                              'proc_column': 'native_country_mZFLky',\n",
            "                              'tied': None,\n",
            "                              'type': 'category'}],\n",
            "    'output_features': [   {   'column': 'income',\n",
            "                               'dependencies': [],\n",
            "                               'loss': {   'confidence_penalty': 0,\n",
            "                                           'positive_class_weight': None,\n",
            "                                           'robust_lambda': 0,\n",
            "                                           'weight': 1},\n",
            "                               'name': 'income',\n",
            "                               'num_fc_layers': 4,\n",
            "                               'output_size': 32,\n",
            "                               'preprocessing': {   'fallback_true_label': ' '\n",
            "                                                                           '>50K',\n",
            "                                                    'missing_value_strategy': 'drop_row'},\n",
            "                               'proc_column': 'income_acRTnD',\n",
            "                               'reduce_dependencies': 'sum',\n",
            "                               'reduce_input': 'sum',\n",
            "                               'threshold': 0.5,\n",
            "                               'type': 'binary'}],\n",
            "    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\n",
            "                                      'audio_file_length_limit_in_s': 7.5,\n",
            "                                      'in_memory': True,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'norm': None,\n",
            "                                      'padding_value': 0},\n",
            "                         'bag': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'binary': {   'missing_value_strategy': 'fill_with_false'},\n",
            "                         'category': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 10000},\n",
            "                         'date': {   'datetime_format': None,\n",
            "                                     'fill_value': '',\n",
            "                                     'missing_value_strategy': 'fill_with_const'},\n",
            "                         'force_split': False,\n",
            "                         'h3': {   'fill_value': 576495936675512319,\n",
            "                                   'missing_value_strategy': 'fill_with_const'},\n",
            "                         'image': {   'in_memory': True,\n",
            "                                      'infer_image_dimensions': True,\n",
            "                                      'infer_image_max_height': 256,\n",
            "                                      'infer_image_max_width': 256,\n",
            "                                      'infer_image_num_channels': True,\n",
            "                                      'infer_image_sample_size': 100,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'num_processes': 1,\n",
            "                                      'resize_method': 'interpolate',\n",
            "                                      'scaling': 'pixel_normalization'},\n",
            "                         'number': {   'fill_value': 0,\n",
            "                                       'missing_value_strategy': 'fill_with_mean',\n",
            "                                       'normalization': 'zscore'},\n",
            "                         'oversample_minority': None,\n",
            "                         'sequence': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'max_sequence_length': 256,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 20000,\n",
            "                                         'padding': 'right',\n",
            "                                         'padding_symbol': '<PAD>',\n",
            "                                         'tokenizer': 'space',\n",
            "                                         'unknown_symbol': '<UNK>',\n",
            "                                         'vocab_file': None},\n",
            "                         'set': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
            "                         'stratify': None,\n",
            "                         'text': {   'fill_value': '<UNK>',\n",
            "                                     'lowercase': True,\n",
            "                                     'max_sequence_length': 256,\n",
            "                                     'missing_value_strategy': 'fill_with_const',\n",
            "                                     'most_common': 20000,\n",
            "                                     'padding': 'right',\n",
            "                                     'padding_symbol': '<PAD>',\n",
            "                                     'pretrained_model_name_or_path': None,\n",
            "                                     'tokenizer': 'space_punct',\n",
            "                                     'unknown_symbol': '<UNK>',\n",
            "                                     'vocab_file': None},\n",
            "                         'timeseries': {   'fill_value': '',\n",
            "                                           'missing_value_strategy': 'fill_with_const',\n",
            "                                           'padding': 'right',\n",
            "                                           'padding_value': 0,\n",
            "                                           'timeseries_length_limit': 256,\n",
            "                                           'tokenizer': 'space'},\n",
            "                         'undersample_majority': None,\n",
            "                         'vector': {   'fill_value': '',\n",
            "                                       'missing_value_strategy': 'fill_with_const'}},\n",
            "    'trainer': {   'batch_size': 128,\n",
            "                   'decay': False,\n",
            "                   'decay_rate': 0.96,\n",
            "                   'decay_steps': 10000,\n",
            "                   'early_stop': 1000,\n",
            "                   'epochs': 10,\n",
            "                   'eval_batch_size': None,\n",
            "                   'gradient_clipping': None,\n",
            "                   'increase_batch_size_on_plateau': 0,\n",
            "                   'increase_batch_size_on_plateau_max': 512,\n",
            "                   'increase_batch_size_on_plateau_patience': 5,\n",
            "                   'increase_batch_size_on_plateau_rate': 2,\n",
            "                   'learning_rate': 0.001,\n",
            "                   'learning_rate_warmup_epochs': 1,\n",
            "                   'optimizer': {   'betas': (0.9, 0.999),\n",
            "                                    'eps': 1e-08,\n",
            "                                    'type': 'adam'},\n",
            "                   'reduce_learning_rate_on_plateau': 0,\n",
            "                   'reduce_learning_rate_on_plateau_patience': 5,\n",
            "                   'reduce_learning_rate_on_plateau_rate': 0.5,\n",
            "                   'regularization_lambda': 0,\n",
            "                   'regularization_type': 'l2',\n",
            "                   'staircase': False,\n",
            "                   'steps_per_checkpoint': 0,\n",
            "                   'validation_field': 'combined',\n",
            "                   'validation_metric': 'loss'}}\n",
            "\n",
            "╒═══════════════╕\n",
            "│ PREPROCESSING │\n",
            "╘═══════════════╛\n",
            "\n",
            "Using full dataframe\n",
            "Building dataset (it may take a while)\n",
            "Binary feature income has at least 1 unconventional boolean value: Cannot automatically map value  <=50K to a boolean and no `fallback_true_label` specified.. We will now interpret  >50K as 1 and the other values as 0. If this is incorrect, please use the category feature type or manually specify the true value with `preprocessing.fallback_true_label`.\n",
            "Building dataset: DONE\n",
            "Writing preprocessed training set cache\n",
            "Writing preprocessed test set cache\n",
            "Writing preprocessed validation set cache\n",
            "Writing train set metadata\n",
            "\n",
            "Dataset sizes:\n",
            "╒════════════╤════════╕\n",
            "│ Dataset    │   Size │\n",
            "╞════════════╪════════╡\n",
            "│ Training   │  22866 │\n",
            "├────────────┼────────┤\n",
            "│ Validation │   3204 │\n",
            "├────────────┼────────┤\n",
            "│ Test       │   6491 │\n",
            "╘════════════╧════════╛\n",
            "\n",
            "╒═══════╕\n",
            "│ MODEL │\n",
            "╘═══════╛\n",
            "\n",
            "Warnings and other logs:\n",
            "  embedding_size (50) is greater than vocab_size (10). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (17). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (8). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (16). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (7). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (6). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (3). Setting embedding size to be equal to vocab_size.\n",
            "  embedding_size (50) is greater than vocab_size (43). Setting embedding size to be equal to vocab_size.\n",
            "\n",
            "╒══════════╕\n",
            "│ TRAINING │\n",
            "╘══════════╛\n",
            "\n",
            "Note: steps_per_checkpoint (was 179) is now set to the number of steps per epoch: 179.\n",
            "\n",
            "Training for 1790 step(s), approximately 10 epoch(s).\n",
            "Starting with step 0, epoch: 0\n",
            "Training:  10%|▉         | 175/1790 [00:01<00:12, 126.44it/s]\n",
            "Running evaluation for step: 179, epoch: 0\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 201.69it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 187.10it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 201.11it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.4246 │    0.8322 │     0.7887 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.4094 │    0.8396 │     0.7906 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.4204 │    0.8354 │     0.7842 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.4246 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.4094 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.4204 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved.\n",
            "\n",
            "Training:  20%|█▉        | 357/1790 [00:04<00:15, 91.20it/s]\n",
            "Running evaluation for step: 358, epoch: 1\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 182.82it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 205.00it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 200.48it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3908 │    0.8444 │     0.8227 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3783 │    0.8524 │     0.8149 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3870 │    0.8483 │     0.8238 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3908 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3783 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3870 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved.\n",
            "\n",
            "Training:  29%|██▉       | 525/1790 [00:07<00:10, 119.42it/s]\n",
            "Running evaluation for step: 537, epoch: 2\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 207.10it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 196.34it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 203.10it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3919 │    0.8434 │     0.8204 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3783 │    0.8496 │     0.8159 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3863 │    0.8471 │     0.8253 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3919 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3783 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3863 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 179 step(s) ago.\n",
            "\n",
            "Training:  40%|███▉      | 709/1790 [00:10<00:08, 123.31it/s]\n",
            "Running evaluation for step: 716, epoch: 3\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 199.98it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 188.38it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 189.93it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3888 │    0.8455 │     0.8199 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3753 │    0.8552 │     0.8208 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3836 │    0.8501 │     0.8233 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3888 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3753 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3836 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved.\n",
            "\n",
            "Training:  50%|████▉     | 890/1790 [00:12<00:07, 113.72it/s]\n",
            "Running evaluation for step: 895, epoch: 4\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 183.22it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 173.62it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 184.66it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3860 │    0.8469 │     0.8225 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3733 │    0.8568 │     0.8152 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3822 │    0.8510 │     0.8258 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3860 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3733 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3822 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved.\n",
            "\n",
            "Training:  60%|██████    | 1074/1790 [00:15<00:06, 115.93it/s]\n",
            "Running evaluation for step: 1074, epoch: 5\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 201.76it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 181.60it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 186.91it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3853 │    0.8478 │     0.8246 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3749 │    0.8549 │     0.8162 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3820 │    0.8516 │     0.8253 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3853 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3749 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3820 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 179 step(s) ago.\n",
            "\n",
            "Training:  70%|███████   | 1253/1790 [00:18<00:04, 111.42it/s]\n",
            "Running evaluation for step: 1253, epoch: 6\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 197.50it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 195.75it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 201.84it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3865 │    0.8467 │     0.8241 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3772 │    0.8526 │     0.8155 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3831 │    0.8502 │     0.8270 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3865 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3772 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3831 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 358 step(s) ago.\n",
            "\n",
            "Training:  80%|███████▉  | 1431/1790 [00:21<00:03, 114.19it/s]\n",
            "Running evaluation for step: 1432, epoch: 7\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 201.01it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 189.61it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 198.04it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3902 │    0.8478 │     0.8241 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3802 │    0.8553 │     0.8205 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3896 │    0.8498 │     0.8244 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3902 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3802 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3896 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 537 step(s) ago.\n",
            "\n",
            "Training:  89%|████████▉ | 1600/1790 [00:24<00:01, 118.57it/s]\n",
            "Running evaluation for step: 1611, epoch: 8\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 196.73it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 180.92it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 190.15it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3844 │    0.8486 │     0.8245 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3736 │    0.8560 │     0.8177 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3830 │    0.8516 │     0.8250 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3844 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3736 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3830 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 716 step(s) ago.\n",
            "\n",
            "Training: 100%|█████████▉| 1782/1790 [00:27<00:00, 112.45it/s]\n",
            "Running evaluation for step: 1790, epoch: 9\n",
            "Evaluation train: 100%|██████████| 179/179 [00:00<00:00, 194.07it/s]\n",
            "Evaluation vali : 100%|██████████| 26/26 [00:00<00:00, 185.04it/s]\n",
            "Evaluation test : 100%|██████████| 51/51 [00:00<00:00, 201.84it/s]\n",
            "╒══════════╤════════╤═══════════╤════════════╕\n",
            "│ income   │   loss │   roc_auc │   accuracy │\n",
            "╞══════════╪════════╪═══════════╪════════════╡\n",
            "│ train    │ 0.3825 │    0.8489 │     0.8251 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ vali     │ 0.3736 │    0.8570 │     0.8196 │\n",
            "├──────────┼────────┼───────────┼────────────┤\n",
            "│ test     │ 0.3828 │    0.8518 │     0.8293 │\n",
            "╘══════════╧════════╧═══════════╧════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.3825 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3736 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.3828 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 895 step(s) ago.\n",
            "\n",
            "\n",
            "╒═════════════════╕\n",
            "│ TRAINING REPORT │\n",
            "╘═════════════════╛\n",
            "\n",
            "Best validation model step: 895, epoch: 5\n",
            "Best validation model loss on validation set combined: 0.37326157093048096\n",
            "Best validation model loss on test set combined: 0.3822372257709503\n",
            "\n",
            "Finished: api_experiment_run\n",
            "Saved to: /content/results/api_experiment_run\n",
            "\n",
            "╒══════════╕\n",
            "│ FINISHED │\n",
            "╘══════════╛\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "49uG76tx9Kdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates predictions and performance statistics for the test set.\n",
        "test_stats, predictions, output_directory = model.evaluate(\n",
        "  test_df,\n",
        "  collect_predictions=True,\n",
        "  collect_overall_stats=True\n",
        ")"
      ],
      "metadata": {
        "id": "4iH2U0Ca8w5l",
        "outputId": "4fdc2acd-8f70-4e07-dab4-d6f539704113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: 100%|██████████| 1790/1790 [02:13<00:00, 13.40it/s] \n",
            "Evaluation: 100%|██████████| 128/128 [00:00<00:00, 210.50it/s]\n",
            "\n",
            "===== income =====\n",
            "accuracy: 0.7781463265419006\n",
            "average_precision_macro: 0.2816030759737681\n",
            "average_precision_micro: 0.2816030759737681\n",
            "average_precision_samples: 0.2816030759737681\n",
            "loss: 7131.49951171875\n",
            "overall_stats: { 'avg_f1_score_macro': 0.5010492335429741,\n",
            "  'avg_f1_score_micro': 0.7781463055094896,\n",
            "  'avg_f1_score_weighted': 0.6972073767161714,\n",
            "  'avg_precision_macro': 0.8317491624976324,\n",
            "  'avg_precision_micro': 0.7781463055094896,\n",
            "  'avg_precision_weighted': 0.7781463055094896,\n",
            "  'avg_recall_macro': 0.5334742846891197,\n",
            "  'avg_recall_micro': 0.7781463055094896,\n",
            "  'avg_recall_weighted': 0.7781463055094896,\n",
            "  'kappa_score': 0.09819924379169997,\n",
            "  'token_accuracy': 0.7781463055094896}\n",
            "per_class_stats: {False: {   'accuracy': 0.7781463055094896,\n",
            "    'f1_score': 0.8728795664109242,\n",
            "    'fall_out': 0.9303172126885075,\n",
            "    'false_discovery_rate': 0.2239188935477815,\n",
            "    'false_negative_rate': 0.0027342179332529026,\n",
            "    'false_negatives': 34,\n",
            "    'false_omission_rate': 0.11258278145695366,\n",
            "    'false_positive_rate': 0.9303172126885075,\n",
            "    'false_positives': 3578,\n",
            "    'hit_rate': 0.9972657820667471,\n",
            "    'informedness': 0.06694856937823945,\n",
            "    'markedness': 0.6634983249952648,\n",
            "    'matthews_correlation_coefficient': 0.2107611530697515,\n",
            "    'miss_rate': 0.0027342179332529026,\n",
            "    'negative_predictive_value': 0.8874172185430463,\n",
            "    'positive_predictive_value': 0.7760811064522185,\n",
            "    'precision': 0.7760811064522185,\n",
            "    'recall': 0.9972657820667471,\n",
            "    'sensitivity': 0.9972657820667471,\n",
            "    'specificity': 0.06968278731149245,\n",
            "    'true_negative_rate': 0.06968278731149245,\n",
            "    'true_negatives': 268,\n",
            "    'true_positive_rate': 0.9972657820667471,\n",
            "    'true_positives': 12401},\n",
            "  True: {   'accuracy': 0.7781463055094896,\n",
            "    'f1_score': 0.1292189006750241,\n",
            "    'fall_out': 0.0027342179332529026,\n",
            "    'false_discovery_rate': 0.11258278145695366,\n",
            "    'false_negative_rate': 0.9303172126885075,\n",
            "    'false_negatives': 3578,\n",
            "    'false_omission_rate': 0.2239188935477815,\n",
            "    'false_positive_rate': 0.0027342179332529026,\n",
            "    'false_positives': 34,\n",
            "    'hit_rate': 0.06968278731149245,\n",
            "    'informedness': 0.06694856937823945,\n",
            "    'markedness': 0.6634983249952648,\n",
            "    'matthews_correlation_coefficient': 0.2107611530697515,\n",
            "    'miss_rate': 0.9303172126885075,\n",
            "    'negative_predictive_value': 0.7760811064522185,\n",
            "    'positive_predictive_value': 0.8874172185430463,\n",
            "    'precision': 0.8874172185430463,\n",
            "    'recall': 0.06968278731149245,\n",
            "    'sensitivity': 0.06968278731149245,\n",
            "    'specificity': 0.9972657820667471,\n",
            "    'true_negative_rate': 0.9972657820667471,\n",
            "    'true_negatives': 12401,\n",
            "    'true_positive_rate': 0.06968278731149245,\n",
            "    'true_positives': 268}}\n",
            "precision_recall_curve: { 'precisions': [0.23622627602727106, 0.8874172185430463, 1.0],\n",
            "  'recalls': [1.0, 0.06968278731149245, 0.0]}\n",
            "roc_auc: 0.5334742665290833\n",
            "roc_auc_macro: 0.5334742846891197\n",
            "roc_auc_micro: 0.5334742846891197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Metrics"
      ],
      "metadata": {
        "id": "IEBzrsnh9ig_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gFEqycNs9Syj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC Curve"
      ],
      "metadata": {
        "id": "1VuVG2Zl9xjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ludwig.visualize import roc_curves\n",
        "\n"
      ],
      "metadata": {
        "id": "B6HWvzFB9vCN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H2yc3pKe9wVJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "Adult_Census_Income_Classification_with_Ludwig_API.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}