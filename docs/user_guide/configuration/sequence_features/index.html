
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Deep learning toolbox">
      
      
      
        <meta name="author" content="Piero Molino">
      
      
        <link rel="canonical" href="https://ludwig-ai.github.io/ludwig-docs/user_guide/configuration/sequence_features/">
      
      <link rel="icon" href="../../../favicon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.6">
    
    
      
        <title>Sequence Features - Ludwig</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2c0c5eaf.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.7fa14f5b.min.css">
        
          
          
          <meta name="theme-color" content="#757575">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../../stylesheets/monokai.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="grey" data-md-color-accent="grey">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#sequence-features-preprocessing" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Ludwig" class="md-header__button md-logo" aria-label="Ludwig" data-md-component="logo">
      
<img alt="logo" src="../../../images/ludwig_logo.svg"
     style="height:1rem;width:4rem;">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ludwig
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sequence Features
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/ludwig-ai/ludwig/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav aria-label="Navigation" class="md-nav md-nav--primary"
     data-md-level="0">
    <label class="md-nav__title" for="__drawer">
        <a aria-label="Ludwig" class="md-nav__button md-logo"
           href="https://ludwig-ai.github.io/ludwig-docs/"
           title="Ludwig">
            <img alt="logo" src="../../../images/ludwig_logo.svg"
                 style="width:10rem;height:auto;">
        </a>
    </label>
    
    <div class="md-nav__source">
        
<a href="https://github.com/ludwig-ai/ludwig/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ludwig-ai/ludwig
  </div>
</a>
    </div>
    
    <ul class="md-nav__list" data-md-scrollfix>
        
        
        
        

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        About
      </a>
    </li>
  

        
        
        
        

  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

        
        
        
        

  
  
  
    <li class="md-nav__item">
      <a href="../../../examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

        
        
        
        

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
      
      <label class="md-nav__link" for="nav-4">
        User Guide
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="User Guide" data-md-level="1">
        <label class="md-nav__title" for="nav-4">
          <span class="md-nav__icon md-icon"></span>
          User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../user_guide_intro/" class="md-nav__link">
        User Guide Intro
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../command_line_interface/" class="md-nav__link">
        Command Line Interface
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../data_preprocessing/" class="md-nav__link">
        Data Preprocessing
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../data_postprocessing/" class="md-nav__link">
        Data Postprocessing
      </a>
    </li>
  

          
            
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4_5" type="checkbox" id="nav-4_5" checked>
      
      <label class="md-nav__link" for="nav-4_5">
        Configuration
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Configuration" data-md-level="2">
        <label class="md-nav__title" for="nav-4_5">
          <span class="md-nav__icon md-icon"></span>
          Configuration
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../configuration_intro/" class="md-nav__link">
        Configuration Intro
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../input_features/" class="md-nav__link">
        Input Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../combiner/" class="md-nav__link">
        Combiner
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../output_features/" class="md-nav__link">
        Output Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../training/" class="md-nav__link">
        Training
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        Preprocessing
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../binary_features/" class="md-nav__link">
        Binary Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../numerical_features/" class="md-nav__link">
        Numerical Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../category_features/" class="md-nav__link">
        Category Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../set_features/" class="md-nav__link">
        Set Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../bag_features/" class="md-nav__link">
        Bag Features
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Sequence Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Sequence Features
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sequence-features-preprocessing" class="md-nav__link">
    Sequence Features Preprocessing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-input-features-and-encoders" class="md-nav__link">
    Sequence Input Features and Encoders
  </a>
  
    <nav class="md-nav" aria-label="Sequence Input Features and Encoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#embed-encoder" class="md-nav__link">
    Embed Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-cnn-encoder" class="md-nav__link">
    Parallel CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stacked-cnn-encoder" class="md-nav__link">
    Stacked CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stacked-parallel-cnn-encoder" class="md-nav__link">
    Stacked Parallel CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn-encoder" class="md-nav__link">
    RNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-rnn-encoder" class="md-nav__link">
    CNN RNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-encoder" class="md-nav__link">
    Transformer Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#passthrough-encoder" class="md-nav__link">
    Passthrough Encoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-output-features-and-decoders" class="md-nav__link">
    Sequence Output Features and Decoders
  </a>
  
    <nav class="md-nav" aria-label="Sequence Output Features and Decoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tagger-decoder" class="md-nav__link">
    Tagger Decoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generator-decoder" class="md-nav__link">
    Generator Decoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-features-measures" class="md-nav__link">
    Sequence Features Measures
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../text_features/" class="md-nav__link">
        Text Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../time_series_features/" class="md-nav__link">
        Time Series Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../audio_features/" class="md-nav__link">
        Audio Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../image_features/" class="md-nav__link">
        Image Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../date_features/" class="md-nav__link">
        Date Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../h3_features/" class="md-nav__link">
        H3 Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../vector_features/" class="md-nav__link">
        Vector Features
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../combiners/" class="md-nav__link">
        Combiners
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed_execution_backends/" class="md-nav__link">
        Distributed Execution Backends
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../hyperparameter_optimization/" class="md-nav__link">
        Hyper-parameter optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../programmatic_api/" class="md-nav__link">
        Programmatic API
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../visualizations/" class="md-nav__link">
        Visualizations
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../serving/" class="md-nav__link">
        Serving
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../datasets/" class="md-nav__link">
        Datasets
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/" class="md-nav__link">
        Integrations
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

        
        
        
        

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" >
      
      <label class="md-nav__link" for="nav-5">
        Developer Guide
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Developer Guide" data-md-level="1">
        <label class="md-nav__title" for="nav-5">
          <span class="md-nav__icon md-icon"></span>
          Developer Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/developer_guide_intro/" class="md-nav__link">
        Developer Guide Intro
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/codebase_structure/" class="md-nav__link">
        Codebase Structure
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/add_an_encoder/" class="md-nav__link">
        Add an Encoder
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/add_a_decoder/" class="md-nav__link">
        Add a Decoder
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/add_a_feature_type/" class="md-nav__link">
        Add a Feature Type
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/hyper_parameter_optimization/" class="md-nav__link">
        Hyper-parameter Optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/add_an_integration/" class="md-nav__link">
        Add an Integration
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/add_a_dataset/" class="md-nav__link">
        Add an Dataset
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../developer_guide/style_guidelines_and_tests/" class="md-nav__link">
        Style Guidelines and Tests
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

        
        
        
        

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6" >
      
      <label class="md-nav__link" for="nav-6">
        API
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="nav-6">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/LudwigModel/" class="md-nav__link">
        LudwigModel
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/visualization/" class="md-nav__link">
        Visualization
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

        
        
        
        

  
  
  
    <li class="md-nav__item">
      <a href="../../../community/" class="md-nav__link">
        Community
      </a>
    </li>
  

        
        
        
        

  
  
  
    <li class="md-nav__item">
      <a href="../../../faq/" class="md-nav__link">
        FAQ
      </a>
    </li>
  

        
    </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sequence-features-preprocessing" class="md-nav__link">
    Sequence Features Preprocessing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-input-features-and-encoders" class="md-nav__link">
    Sequence Input Features and Encoders
  </a>
  
    <nav class="md-nav" aria-label="Sequence Input Features and Encoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#embed-encoder" class="md-nav__link">
    Embed Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-cnn-encoder" class="md-nav__link">
    Parallel CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stacked-cnn-encoder" class="md-nav__link">
    Stacked CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stacked-parallel-cnn-encoder" class="md-nav__link">
    Stacked Parallel CNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn-encoder" class="md-nav__link">
    RNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-rnn-encoder" class="md-nav__link">
    CNN RNN Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-encoder" class="md-nav__link">
    Transformer Encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#passthrough-encoder" class="md-nav__link">
    Passthrough Encoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-output-features-and-decoders" class="md-nav__link">
    Sequence Output Features and Decoders
  </a>
  
    <nav class="md-nav" aria-label="Sequence Output Features and Decoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tagger-decoder" class="md-nav__link">
    Tagger Decoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generator-decoder" class="md-nav__link">
    Generator Decoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-features-measures" class="md-nav__link">
    Sequence Features Measures
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ludwig-ai/ludwig-docs/edit/master/src/docs/user_guide/configuration/sequence_features.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  <h1>Sequence Features</h1>
                
                <h3 id="sequence-features-preprocessing">Sequence Features Preprocessing<a class="headerlink" href="#sequence-features-preprocessing" title="Permanent link">&para;</a></h3>
<p>Sequence features are transformed into an integer valued matrix of size <code>n x l</code> (where <code>n</code> is the size of the dataset and <code>l</code> is the minimum of the length of the longest sequence and a <code>sequence_length_limit</code> parameter) and added to HDF5 with a key that reflects the name of column in the dataset.
The way sequences are mapped into integers consists in first using a tokenizer to map from strings to sequences of tokens (by default this is done by splitting on spaces).
Then a dictionary of all the different token strings present in the column of the dataset is collected, then they are ranked by frequency and an increasing integer ID is assigned to them from the most frequent to the most rare (with 0 being assigned to <code>&lt;PAD&gt;</code> used for padding and 1 assigned to <code>&lt;UNK&gt;</code> item).
The column name is added to the JSON file, with an associated dictionary containing</p>
<ol>
<li>the mapping from integer to string (<code>idx2str</code>)</li>
<li>the mapping from string to id (<code>str2idx</code>)</li>
<li>the mapping from string to frequency (<code>str2freq</code>)</li>
<li>the maximum length of all sequences (<code>sequence_length_limit</code>)</li>
<li>additional preprocessing information (by default how to fill missing values and what token to use to fill missing values)</li>
</ol>
<p>The parameters available for preprocessing are</p>
<ul>
<li><code>sequence_length_limit</code> (default <code>256</code>): the maximum length of the sequence. Sequences that are longer than this value will be truncated, while sequences that are shorter will be padded.</li>
<li><code>most_common</code> (default <code>20000</code>): the maximum number of most common tokens to be considered. if the data contains more than this amount, the most infrequent tokens will be treated as unknown.</li>
<li><code>padding_symbol</code> (default <code>&lt;PAD&gt;</code>): the string used as a padding symbol. Is is mapped to the integer ID 0 in the vocabulary.</li>
<li><code>unknown_symbol</code> (default <code>&lt;UNK&gt;</code>): the string used as a unknown symbol. Is is mapped to the integer ID 1 in the vocabulary.</li>
<li><code>padding</code> (default <code>right</code>): the direction of the padding. <code>right</code> and <code>left</code> are available options.</li>
<li><code>tokenizer</code> (default <code>space</code>): defines how to map from the raw string content of the dataset column to a sequence of elements. For the available options refer to the <a href="#tokenizers">Tokenizers</a>section.</li>
<li><code>lowercase</code> (default <code>false</code>): if the string has to be lowercase before being handled by the tokenizer.</li>
<li><code>vocab_file</code> (default <code>null</code>)  filepath string to a UTF-8 encoded file containing the sequence's vocabulary.  On each line the first string until <code>\t</code> or <code>\n</code> is considered a word.</li>
<li><code>missing_value_strategy</code> (default <code>fill_with_const</code>): what strategy to follow when there's a missing value in a binary column. The value should be one of <code>fill_with_const</code> (replaces the missing value with a specific value specified with the <code>fill_value</code> parameter), <code>fill_with_mode</code> (replaces the missing values with the most frequent value in the column), <code>fill_with_mean</code> (replaces the missing values with the mean of the values in the column), <code>backfill</code> (replaces the missing values with the next valid value).</li>
<li><code>fill_value</code> (default <code>""</code>): the value to replace the missing values with in case the <code>missing_value_strategy</code> is <code>fill_value</code>.</li>
</ul>
<h3 id="sequence-input-features-and-encoders">Sequence Input Features and Encoders<a class="headerlink" href="#sequence-input-features-and-encoders" title="Permanent link">&para;</a></h3>
<p>Sequence features have several encoders and each of them has its own parameters.
Inputs are of size <code>b</code> while outputs are of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the dimensionally of the output of the encoder.
In case a representation for each element of the sequence is needed (for example for tagging them, or for using an attention mechanism), one can specify the parameter <code>reduce_output</code> to be  <code>null</code> and the output will be a <code>b x s x h</code> tensor where <code>s</code> is the length of the sequence.
Some encoders, because of their inner workings, may require additional parameters to be specified in order to obtain one representation for each element of the sequence.
For instance the <code>parallel_cnn</code> encoder, by default pools and flattens the sequence dimension and then passes the flattened vector through fully connected layers, so in order to obtain the full tesnor one has to specify <code>reduce_output: null</code>.</p>
<p>Sequence input feature parameters are</p>
<ul>
<li><code>encoder</code> (default <code>parallel_cnn</code>): the name of the encoder to use to encode the sequence. The available ones are  <code>embed</code>, <code>parallel_cnn</code>, <code>stacked_cnn</code>, <code>stacked_parallel_cnn</code>, <code>rnn</code>, <code>cnnrnn</code>, <code>transformer</code> and <code>passthrough</code> (equivalent to specify <code>null</code> or <code>'None'</code>).</li>
<li><code>tied_weights</code> (default <code>null</code>): name of the input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li>
</ul>
<h4 id="embed-encoder">Embed Encoder<a class="headerlink" href="#embed-encoder" title="Permanent link">&para;</a></h4>
<p>The embed encoder simply maps each integer in the sequence to an embedding, creating a <code>b x s x h</code> tensor where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>h</code> is the embedding size.
The tensor is reduced along the <code>s</code> dimension to obtain a single vector of size <code>h</code> for each element of the batch.
If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p>
<div class="codehilite"><pre><span></span><code>       +------+
       |Emb 12|
       +------+
+--+   |Emb 7 |
|12|   +------+
|7 |   |Emb 43|   +-----------+
|43|   +------+   |Aggregation|
|65+---&gt;Emb 65+---&gt;Reduce     +-&gt;
|23|   +------+   |Operation  |
|4 |   |Emb 23|   +-----------+
|1 |   +------+
+--+   |Emb 4 |
       +------+
       |Emb 1 |
       +------+
</code></pre></div>

<p>These are the parameters available for the embed encoder</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>reduce_output</code> (default <code>sum</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using an embed encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">embed</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sum</span>
</code></pre></div>

<h4 id="parallel-cnn-encoder">Parallel CNN Encoder<a class="headerlink" href="#parallel-cnn-encoder" title="Permanent link">&para;</a></h4>
<p>The parallel cnn encoder is inspired by <a href="https://arxiv.org/abs/1408.5882">Yoon Kim's Convolutional Neural Network for Sentence Classification</a>.
It works by first mapping the input integer sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a number of parallel 1d convolutional layers with different filter size (by default 4 layers with filter size 2, 3, 4 and 5), followed by max pooling and concatenation.
This single vector concatenating the outputs of the parallel convolutional layers is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer.
If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p>
<div class="codehilite"><pre><span></span><code>                   +-------+   +----+
                +--&gt;1D Conv+---&gt;Pool+-+
       +------+ |  |Width 2|   +----+ |
       |Emb 12| |  +-------+          |
       +------+ |                     |
+--+   |Emb 7 | |  +-------+   +----+ |
|12|   +------+ +--&gt;1D Conv+---&gt;Pool+-+
|7 |   |Emb 43| |  |Width 3|   +----+ |           +---------+
|43|   +------+ |  +-------+          | +------+  |Fully    |
|65+---&gt;Emb 65+-+                     +-&gt;Concat+--&gt;Connected+-&gt;
|23|   +------+ |  +-------+   +----+ | +------+  |Layers   |
|4 |   |Emb 23| +--&gt;1D Conv+---&gt;Pool+-+           +---------+
|1 |   +------+ |  |Width 4|   +----+ |
+--+   |Emb 4 | |  +-------+          |
       +------+ |                     |
       |Emb 1 | |  +-------+   +----+ |
       +------+ +--&gt;1D Conv+---&gt;Pool+-+
                   |Width 5|   +----+
                   +-------+
</code></pre></div>

<p>These are the available for an parallel cnn encoder:</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>conv_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of parallel convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>filter_size</code>, <code>num_filters</code>, <code>pool</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 2}, {filter_size: 3}, {filter_size: 4}, {filter_size: 5}]</code>.</li>
<li><code>num_conv_layers</code> (default <code>null</code>): if <code>conv_layers</code> is <code>null</code>, this is the number of parallel convolutional layers.</li>
<li><code>filter_size</code> (default <code>3</code>): if a <code>filter_size</code> is not already specified in <code>conv_layers</code> this is the default <code>filter_size</code> that will be used for each layer. It indicates how wide is the 1d convolutional filter.</li>
<li><code>num_filters</code> (default <code>256</code>): if a <code>num_filters</code> is not already specified in <code>conv_layers</code> this is the default <code>num_filters</code> that will be used for each layer. It indicates the number of filters, and by consequence the output channels of the 1d convolution.</li>
<li><code>pool_function</code> (default <code>max</code>):  pooling function: <code>max</code> will select the maximum value.  Any of these--<code>average</code>, <code>avg</code> or <code>mean</code>--will compute the mean value.</li>
<li><code>pool_size</code> (default <code>null</code>): if a <code>pool_size</code> is not already specified in <code>conv_layers</code> this is the default <code>pool_size</code> that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>,  <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>null</code>): if <code>fc_layers</code> is <code>null</code>, this is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>sum</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the sequence dimension), <code>last</code> (returns the last vector of the sequence dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using a parallel cnn encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">parallel_cnn</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">filter_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="nt">num_filters</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">pool_function</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
<span class="nt">pool_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sum</span>
</code></pre></div>

<h4 id="stacked-cnn-encoder">Stacked CNN Encoder<a class="headerlink" href="#stacked-cnn-encoder" title="Permanent link">&para;</a></h4>
<p>The stacked cnn encoder is inspired by <a href="https://arxiv.org/abs/1509.01626">Xiang Zhang at all's Character-level Convolutional Networks for Text Classification</a>.
It works by first mapping the input integer sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of 1d convolutional layers with different filter size (by default 6 layers with filter size 7, 7, 3, 3, 3 and 3), followed by an optional final pool and by a flatten operation.
This single flatten vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer.
If you want to output the full <code>b x s x h</code> tensor, you can specify the <code>pool_size</code> of all your <code>conv_layers</code> to be <code>null</code>  and <code>reduce_output: null</code>, while if <code>pool_size</code> has a value different from <code>null</code> and <code>reduce_output: null</code> the returned tensor will be of shape <code>b x s' x h</code>, where <code>s'</code> is width of the output of the last convolutional layer.</p>
<div class="codehilite"><pre><span></span><code>       +------+
       |Emb 12|
       +------+
+--+   |Emb 7 |
|12|   +------+
|7 |   |Emb 43|   +----------------+  +---------+
|43|   +------+   |1D Conv         |  |Fully    |
|65+---&gt;Emb 65+---&gt;Layers          +--&gt;Connected+-&gt;
|23|   +------+   |Different Widths|  |Layers   |
|4 |   |Emb 23|   +----------------+  +---------+
|1 |   +------+
+--+   |Emb 4 |
       +------+
       |Emb 1 |
       +------+
</code></pre></div>

<p>These are the parameters available for the stack cnn encoder:</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>conv_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>filter_size</code>, <code>num_filters</code>, <code>pool_size</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3, regularize: false}, {filter_size: 7, pool_size: 3, regularize: false}, {filter_size: 3, pool_size: null, regularize: false}, {filter_size: 3, pool_size: null, regularize: false}, {filter_size: 3, pool_size: null, regularize: true}, {filter_size: 3, pool_size: 3, regularize: true}]</code>.</li>
<li><code>num_conv_layers</code> (default <code>null</code>): if <code>conv_layers</code> is <code>null</code>, this is the number of stacked convolutional layers.</li>
<li><code>filter_size</code> (default <code>3</code>): if a <code>filter_size</code> is not already specified in <code>conv_layers</code> this is the default <code>filter_size</code> that will be used for each layer. It indicates how wide is the 1d convolutional filter.</li>
<li><code>num_filters</code> (default <code>256</code>): if a <code>num_filters</code> is not already specified in <code>conv_layers</code> this is the default <code>num_filters</code> that will be used for each layer. It indicates the number of filters, and by consequence the output channels of the 1d convolution.</li>
<li><code>strides</code> (default <code>1</code>): stride length of the convolution</li>
<li><code>padding</code> (default <code>same</code>):  one of <code>valid</code> or <code>same</code>.</li>
<li><code>dilation_rate</code> (default <code>1</code>): dilation rate to use for dilated convolution</li>
<li><code>pool_function</code> (default <code>max</code>):  pooling function: <code>max</code> will select the maximum value.  Any of these--<code>average</code>, <code>avg</code> or <code>mean</code>--will compute the mean value.</li>
<li><code>pool_size</code> (default <code>null</code>): if a <code>pool_size</code> is not already specified in <code>conv_layers</code> this is the default <code>pool_size</code> that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li>
<li><code>pool_strides</code> (default <code>null</code>): factor to scale down</li>
<li><code>pool_padding</code> (default <code>same</code>): one of <code>valid</code> or <code>same</code></li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>null</code>): if <code>fc_layers</code> is <code>null</code>, this is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>max</code>): defines how to reduce the output tensor of the convolutional layers along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using a parallel cnn encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">stacked_cnn</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">filter_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="nt">num_filters</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">strides</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">padding</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">same</span>
<span class="nt">dilation_rate</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">pool_function</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
<span class="nt">pool_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">pool_strides</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">pool_padding</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">same</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
</code></pre></div>

<h4 id="stacked-parallel-cnn-encoder">Stacked Parallel CNN Encoder<a class="headerlink" href="#stacked-parallel-cnn-encoder" title="Permanent link">&para;</a></h4>
<p>The stacked parallel cnn encoder is a combination of the Parallel CNN and the Stacked CNN encoders where each layer of the stack is a composed of parallel convolutional layers.
It works by first mapping the input integer sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of several parallel 1d convolutional layers with different filter size, followed by an optional final pool and by a flatten operation.
This single flatten vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer.
If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p>
<div class="codehilite"><pre><span></span><code>                   +-------+                      +-------+
                +--&gt;1D Conv+-+                 +--&gt;1D Conv+-+
       +------+ |  |Width 2| |                 |  |Width 2| |
       |Emb 12| |  +-------+ |                 |  +-------+ |
       +------+ |            |                 |            |
+--+   |Emb 7 | |  +-------+ |                 |  +-------+ |
|12|   +------+ +--&gt;1D Conv+-+                 +--&gt;1D Conv+-+
|7 |   |Emb 43| |  |Width 3| |                 |  |Width 3| |                   +---------+
|43|   +------+ |  +-------+ | +------+  +---+ |  +-------+ | +------+  +----+  |Fully    |
|65+---&gt;Emb 65+-+            +-&gt;Concat+--&gt;...+-+            +-&gt;Concat+--&gt;Pool+--&gt;Connected+-&gt;
|23|   +------+ |  +-------+ | +------+  +---+ |  +-------+ | +------+  +----+  |Layers   |
|4 |   |Emb 23| +--&gt;1D Conv+-+                 +--&gt;1D Conv+-+                   +---------+
|1 |   +------+ |  |Width 4| |                 |  |Width 4| |
+--+   |Emb 4 | |  +-------+ |                 |  +-------+ |
       +------+ |            |                 |            |
       |Emb 1 | |  +-------+ |                 |  +-------+ |
       +------+ +--&gt;1D Conv+-+                 +--&gt;1D Conv+-+
                   |Width 5|                      |Width 5|
                   +-------+                      +-------+
</code></pre></div>

<p>These are the available parameters for the stack parallel cnn encoder:</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>stacked_layers</code> (default <code>null</code>): it is a of lists of list of dictionaries containing the parameters of the stack of parallel convolutional layers. The length of the list determines the number of stacked parallel convolutional layers, length of the sub-lists determines the number of parallel conv layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>filter_size</code>, <code>num_filters</code>, <code>pool_size</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>stacked_layers</code> and <code>num_stacked_layers</code> are <code>null</code>, a default list will be assigned to <code>stacked_layers</code> with the value <code>[[{filter_size: 2}, {filter_size: 3}, {filter_size: 4}, {filter_size: 5}], [{filter_size: 2}, {filter_size: 3}, {filter_size: 4}, {filter_size: 5}], [{filter_size: 2}, {filter_size: 3}, {filter_size: 4}, {filter_size: 5}]]</code>.</li>
<li><code>num_stacked_layers</code> (default <code>null</code>): if <code>stacked_layers</code> is <code>null</code>, this is the number of elements in the stack of parallel convolutional layers.</li>
<li><code>filter_size</code> (default <code>3</code>): if a <code>filter_size</code> is not already specified in <code>conv_layers</code> this is the default <code>filter_size</code> that will be used for each layer. It indicates how wide is the 1d convolutional filter.</li>
<li><code>num_filters</code> (default <code>256</code>): if a <code>num_filters</code> is not already specified in <code>conv_layers</code> this is the default <code>num_filters</code> that will be used for each layer. It indicates the number of filters, and by consequence the output channels of the 1d convolution.</li>
<li><code>pool_function</code> (default <code>max</code>):  pooling function: <code>max</code> will select the maximum value.  Any of these--<code>average</code>, <code>avg</code> or <code>mean</code>--will compute the mean value.</li>
<li><code>pool_size</code> (default <code>null</code>): if a <code>pool_size</code> is not already specified in <code>conv_layers</code> this is the default <code>pool_size</code> that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>null</code>): if <code>fc_layers</code> is <code>null</code>, this is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>sum</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using a parallel cnn encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">stacked_parallel_cnn</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">stacked_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_stacked_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">filter_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="nt">num_filters</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">pool_function</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
<span class="nt">pool_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
</code></pre></div>

<h4 id="rnn-encoder">RNN Encoder<a class="headerlink" href="#rnn-encoder" title="Permanent link">&para;</a></h4>
<p>The rnn encoder works by first mapping the input integer sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of recurrent layers (by default 1 layer), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions.
If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p>
<div class="codehilite"><pre><span></span><code>       +------+
       |Emb 12|
       +------+
+--+   |Emb 7 |
|12|   +------+
|7 |   |Emb 43|                 +---------+
|43|   +------+   +----------+  |Fully    |
|65+---&gt;Emb 65+---&gt;RNN Layers+--&gt;Connected+-&gt;
|23|   +------+   +----------+  |Layers   |
|4 |   |Emb 23|                 +---------+
|1 |   +------+
+--+   |Emb 4 |
       +------+
       |Emb 1 |
       +------+
</code></pre></div>

<p>These are the available parameters for the rnn encoder:</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>num_layers</code> (default <code>1</code>): the number of stacked recurrent layers.</li>
<li><code>state_size</code> (default <code>256</code>): the size of the state of the rnn.</li>
<li><code>cell_type</code> (default <code>rnn</code>): the type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>lstm_block</code>, <code>lstm</code>, <code>ln</code>, <code>lstm_cudnn</code>, <code>gru</code>, <code>gru_block</code>, <code>gru_cudnn</code>. For reference about the differences between the cells please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell">TensorFlow's documentation</a>. We suggest to use the <code>block</code> variants on CPU and the <code>cudnn</code> variants on GPU because of their increased speed.</li>
<li><code>bidirectional</code> (default <code>false</code>): if <code>true</code> two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated.</li>
<li><code>activation</code> (default <code>'tanh'</code>): activation function to use</li>
<li><code>recurrent_activation</code> (default <code>'sigmoid'</code>): activation function to use in the recurrent step</li>
<li><code>unit_forget_bias</code> (default <code>true</code>): If <code>true</code>, add 1 to the bias of the forget gate at initialization</li>
<li><code>recurrent_initializer</code> (default <code>'orthogonal'</code>): initializer for recurrent matrix weights</li>
<li><code>recurrent_regularizer</code> (default <code>null</code>): regularizer function applied to recurrent matrix weights</li>
<li><code>dropout</code> (default <code>0.0</code>): dropout rate</li>
<li><code>recurrent_dropout</code> (default <code>0.0</code>): dropout rate for recurrent state</li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>,  <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>null</code>): if <code>fc_layers</code> is <code>null</code>, this is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>fc_activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>fc_dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>last</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using a parallel cnn encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">rnn</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="l l-Scalar l-Scalar-Plain">representation&#39;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">num_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">state_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">cell_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">rnn</span>
<span class="nt">bidirectional</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tanh</span>
<span class="nt">recurrent_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sigmoid</span>
<span class="nt">unit_forget_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">recurrent_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">orthogonal</span>
<span class="nt">recurrent_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">recurrent_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">fc_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">last</span>
</code></pre></div>

<h4 id="cnn-rnn-encoder">CNN RNN Encoder<a class="headerlink" href="#cnn-rnn-encoder" title="Permanent link">&para;</a></h4>
<p>The <code>cnnrnn</code> encoder works by first mapping the input integer sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of convolutional layers (by default 2), that is followed by a stack of recurrent layers (by default 1), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions.
If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p>
<div class="codehilite"><pre><span></span><code>       +------+
       |Emb 12|
       +------+
+--+   |Emb 7 |
|12|   +------+
|7 |   |Emb 43|                                +---------+
|43|   +------+   +----------+   +----------+  |Fully    |
|65+---&gt;Emb 65+---&gt;CNN Layers+---&gt;RNN Layers+--&gt;Connected+-&gt;
|23|   +------+   +----------+   +----------+  |Layers   |
|4 |   |Emb 23|                                +---------+
|1 |   +------+
+--+   |Emb 4 |
       +------+
       |Emb 1 |
       +------+
</code></pre></div>

<p>These are the available parameters of the cnn rnn encoder:</p>
<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>conv_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>filter_size</code>, <code>num_filters</code>, <code>pool_size</code>, <code>norm</code>, <code>activation</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3, regularize: false}, {filter_size: 7, pool_size: 3, regularize: false}, {filter_size: 3, pool_size: null, regularize: false}, {filter_size: 3, pool_size: null, regularize: false}, {filter_size: 3, pool_size: null, regularize: true}, {filter_size: 3, pool_size: 3, regularize: true}]</code>.</li>
<li><code>num_conv_layers</code> (default <code>1</code>): the number of stacked convolutional layers.</li>
<li><code>num_filters</code> (default <code>256</code>): if a <code>num_filters</code> is not already specified in <code>conv_layers</code> this is the default <code>num_filters</code> that will be used for each layer. It indicates the number of filters, and by consequence the output channels of the 1d convolution.</li>
<li><code>filter_size</code> (default <code>5</code>): if a <code>filter_size</code> is not already specified in <code>conv_layers</code> this is the default <code>filter_size</code> that will be used for each layer. It indicates how wide is the 1d convolutional filter.</li>
<li><code>strides</code> (default <code>1</code>): stride length of the convolution</li>
<li><code>padding</code> (default <code>same</code>):  one of <code>valid</code> or <code>same</code>.</li>
<li><code>dilation_rate</code> (default <code>1</code>): dilation rate to use for dilated convolution</li>
<li><code>conv_activation</code> (default <code>relu</code>): activation for the convolution layer</li>
<li><code>conv_dropout</code> (default <code>0.0</code>): dropout rate for the convolution layer</li>
<li><code>pool_function</code> (default <code>max</code>):  pooling function: <code>max</code> will select the maximum value.  Any of these--<code>average</code>, <code>avg</code> or <code>mean</code>--will compute the mean value.</li>
<li><code>pool_size</code> (default 2 ): if a <code>pool_size</code> is not already specified in <code>conv_layers</code> this is the default <code>pool_size</code> that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li>
<li><code>pool_strides</code> (default <code>null</code>): factor to scale down</li>
<li><code>pool_padding</code> (default <code>same</code>): one of <code>valid</code> or <code>same</code></li>
<li><code>num_rec_layers</code> (default <code>1</code>): the number of recurrent layers</li>
<li><code>state_size</code> (default <code>256</code>): the size of the state of the rnn.</li>
<li><code>cell_type</code> (default <code>rnn</code>): the type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>lstm_block</code>, <code>lstm</code>, <code>ln</code>, <code>lstm_cudnn</code>, <code>gru</code>, <code>gru_block</code>, <code>gru_cudnn</code>. For reference about the differences between the cells please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell">TensorFlow's documentation</a>. We suggest to use the <code>block</code> variants on CPU and the <code>cudnn</code> variants on GPU because of their increased speed.</li>
<li><code>bidirectional</code> (default <code>false</code>): if <code>true</code> two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated.</li>
<li><code>activation</code> (default <code>'tanh'</code>): activation function to use</li>
<li><code>recurrent_activation</code> (default <code>'sigmoid'</code>): activation function to use in the recurrent step</li>
<li><code>unit_forget_bias</code> (default <code>true</code>): If <code>true</code>, add 1 to the bias of the forget gate at initialization</li>
<li><code>recurrent_initializer</code> (default <code>'orthogonal'</code>): initializer for recurrent matrix weights</li>
<li><code>recurrent_regularizer</code> (default <code>null</code>): regularizer function applied to recurrent matrix weights</li>
<li><code>dropout</code> (default <code>0.0</code>): dropout rate</li>
<li><code>recurrent_dropout</code> (default <code>0.0</code>): dropout rate for recurrent state</li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>,  <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>null</code>): if <code>fc_layers</code> is <code>null</code>, this is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>fc_activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>fc_dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>last</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the inputs features list using a cnn rnn encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnnrnn</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_conv_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">num_filters</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">filter_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="nt">strides</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">padding</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">same</span>
<span class="nt">dilation_rate</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">conv_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">conv_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">pool_function</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">max</span>
<span class="nt">pool_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">pool_strides</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">pool_padding</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">same</span>
<span class="nt">num_rec_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">state_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">cell_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">rnn</span>
<span class="nt">bidirectional</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tanh</span>
<span class="nt">recurrent_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sigmoid</span>
<span class="nt">unit_forget_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">recurrent_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">orthogonal</span>
<span class="nt">recurrent_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">recurrent_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">fc_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">last</span>
</code></pre></div>

<h4 id="transformer-encoder">Transformer Encoder<a class="headerlink" href="#transformer-encoder" title="Permanent link">&para;</a></h4>
<p>The <code>transformer</code> encoder implements a stack of transformer blocks, replicating the architecture introduced in the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper, and adds am optional stack of fully connected layers at the end.</p>
<div class="codehilite"><pre><span></span><code>       +------+                     
       |Emb 12|                     
       +------+                     
+--+   |Emb 7 |                     
|12|   +------+                     
|7 |   |Emb 43|   +-------------+   +---------+ 
|43|   +------+   |             |   |Fully    |
|65+---+Emb 65+---&gt; Transformer +---&gt;Connected+-&gt;
|23|   +------+   | Blocks      |   |Layers   |
|4 |   |Emb 23|   +-------------+   +---------+
|1 |   +------+                     
+--+   |Emb 4 |                     
       +------+                     
       |Emb 1 |                     
       +------+                     
</code></pre></div>

<ul>
<li><code>representation'</code> (default <code>dense</code>): the possible values are <code>dense</code> and <code>sparse</code>. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings.</li>
<li><code>embedding_size</code> (default <code>256</code>): it is the maximum embedding size, the actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for <code>&lt;UNK&gt;</code>).</li>
<li><code>embeddings_trainable</code> (default <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code> as <code>sparse</code> one-hot encodings are not trainable.</li>
<li><code>pretrained_embeddings</code> (default <code>null</code>): by default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the <a href="https://nlp.stanford.edu/projects/glove/">GloVe format</a>. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</li>
<li><code>embeddings_on_cpu</code> (default <code>false</code>): by default embeddings matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be really big and this parameter forces the placement of the embedding matrix in regular memory and the CPU is used to resolve them, slightly slowing down the process as a result of data transfer between CPU and GPU memory.</li>
<li><code>num_layers</code> (default <code>1</code>): number of transformer blocks.</li>
<li><code>hidden_size</code> (default <code>256</code>): the size of the hidden representation within the transformer block. It is usually the same of the <code>embedding_size</code>, but if the two values are different, a projection layer will be added before the first transformer block.</li>
<li><code>num_heads</code> (default <code>8</code>): number of heads of the self attention in the transformer block.</li>
<li><code>transformer_fc_size</code> (default <code>256</code>): Size of the fully connected layer after self attention in the transformer block. This is usually the same as <code>hidden_size</code> and <code>embedding_size</code>.</li>
<li><code>dropout</code> (default <code>0.1</code>): dropout rate for the transformer block</li>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>,  <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>fc_layers</code> and <code>num_fc_layers</code> are <code>null</code>, a default list will be assigned to <code>fc_layers</code> with the value <code>[{fc_size: 512}, {fc_size: 256}]</code> (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>num_fc_layers</code> (default <code>0</code>): This is the number of stacked fully connected layers (only applies if <code>reduce_output</code> is not <code>null</code>).</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>fc_activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>fc_dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>reduce_output</code> (default <code>last</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the inputs features list using a Transformer encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">transformer</span>
<span class="nt">tied_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">representation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">dense</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embeddings_trainable</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">pretrained_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">embeddings_on_cpu</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">num_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">hidden_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">num_heads</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="nt">transformer_fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">fc_activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">fc_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">last</span>
</code></pre></div>

<h4 id="passthrough-encoder">Passthrough Encoder<a class="headerlink" href="#passthrough-encoder" title="Permanent link">&para;</a></h4>
<p>The passthrough decoder simply transforms each input value into a float value and adds a dimension to the input tensor, creating a <code>b x s x 1</code> tensor where <code>b</code> is the batch size and <code>s</code> is the length of the sequence.
The tensor is reduced along the <code>s</code> dimension to obtain a single vector of size <code>h</code> for each element of the batch.
If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.
This encoder is not really useful for <code>sequence</code> or <code>text</code> features, but may be useful for <code>timeseries</code> features, as it allows for using them without any processing in later stages of the model, like in a sequence combiner for instance.</p>
<div class="codehilite"><pre><span></span><code>+--+   
|12|   
|7 |                    +-----------+
|43|   +------------+   |Aggregation|
|65+---&gt;Cast float32+---&gt;Reduce     +-&gt;
|23|   +------------+   |Operation  |
|4 |                    +-----------+
|1 |   
+--+   
</code></pre></div>

<p>These are the parameters available for the passthrough encoder</p>
<ul>
<li><code>reduce_output</code> (default <code>null</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li>
</ul>
<p>Example sequence feature entry in the input features list using a passthrough encoder:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">encoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">passthrough</span>
<span class="nt">reduce_output</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
</code></pre></div>

<h3 id="sequence-output-features-and-decoders">Sequence Output Features and Decoders<a class="headerlink" href="#sequence-output-features-and-decoders" title="Permanent link">&para;</a></h3>
<p>Sequential features can be used when sequence tagging (classifying each element of an input sequence) or sequence generation needs to be performed.
There are two decoders available for those to tasks names <code>tagger</code> and <code>generator</code>.</p>
<p>These are the available parameters of a sequence output feature</p>
<ul>
<li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li>
<li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to <a href="#output-features-dependencies">Output Features Dependencies</a>.</li>
<li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li>
<li><code>loss</code> (default <code>{type: softmax_cross_entropy, class_similarities_temperature: 0, class_weights: 1, confidence_penalty: 0, distortion: 1, labels_smoothing: 0, negative_samples: 0, robust_lambda: 0, sampler: null, unique: false}</code>): is a dictionary containing a loss <code>type</code>. The available losses <code>type</code> are <code>softmax_cross_entropy</code> and <code>sampled_softmax_cross_entropy</code>. For details on both losses, please refer to the <a href="#category-output-features-and-encoders">category feature output feature section</a>.</li>
</ul>
<h4 id="tagger-decoder">Tagger Decoder<a class="headerlink" href="#tagger-decoder" title="Permanent link">&para;</a></h4>
<p>In the case of <code>tagger</code> the decoder is a (potentially empty) stack of fully connected layers, followed by a projection into a tensor of size <code>b x s x c</code>, where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy.
This decoder requires its input to be shaped as <code>b x s x h</code>, where <code>h</code> is an hidden dimension, which is the output of a sequence, text or timeseries input feature without reduced outputs or the output of a sequence-based combiner.
If a <code>b x h</code> input is provided instead, an error will be raised during model building.</p>
<div class="codehilite"><pre><span></span><code>Combiner
Output

+---+                 +----------+   +-------+
|emb|   +---------+   |Projection|   |Softmax|
+---+   |Fully    |   +----------+   +-------+
|...+---&gt;Connected+---&gt;...       +---&gt;...    |
+---+   |Layers   |   +----------+   +-------+
|emb|   +---------+   |Projection|   |Softmax|
+---+                 +----------+   +-------+
</code></pre></div>

<p>These are the available parameters of a tagger decoder:</p>
<ul>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>, <code>dropout</code>, <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the decoder will be used instead.</li>
<li><code>num_fc_layers</code> (default 0): this is the number of stacked fully connected layers that the input to the feature passes through. Their output is projected in the feature's output space.</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>attention</code> (default <code>false</code>): If <code>true</code>, applies a multi-head self attention layer befre prediction.</li>
<li><code>attention_embedding_size</code> (default <code>256</code>): the embedding size of the multi-head self attention layer.</li>
<li><code>attention_num_heads</code> (default <code>8</code>): number of attention heads in the multi-head self attention layer.</li>
</ul>
<p>Example sequence feature entry using a tagger decoder (with default parameters) in the output features list:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">decoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tagger</span>
<span class="nt">reduce_input</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">dependencies</span><span class="p">:</span> <span class="p p-Indicator">[]</span>
<span class="nt">reduce_dependencies</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sum</span>
<span class="nt">loss</span><span class="p">:</span>
    <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">softmax_cross_entropy</span>
    <span class="nt">confidence_penalty</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">robust_lambda</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">class_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">class_similarities</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="nt">class_similarities_temperature</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">labels_smoothing</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">negative_samples</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">sampler</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="nt">distortion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">unique</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">attention</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">attention_embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">attention_num_heads</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8</span>
</code></pre></div>

<h4 id="generator-decoder">Generator Decoder<a class="headerlink" href="#generator-decoder" title="Permanent link">&para;</a></h4>
<p>In the case of <code>generator</code> the decoder is a (potentially empty) stack of fully connected layers, followed by an rnn that generates outputs feeding on its own previous predictions and generates a tensor of size <code>b x s' x c</code>, where <code>b</code> is the batch size, <code>s'</code> is the length of the generated sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy.
During training teacher forcing is adopted, meaning the list of targets is provided as both inputs and outputs (shifted by 1), while at evaluation time greedy decoding (generating one token at a time and feeding it as input for the next step) is performed by beam search, using a beam of 1 by default.
By default a generator expects a <code>b x h</code> shaped input tensor, where <code>h</code> is a hidden dimension.
The <code>h</code> vectors are (after an optional stack of fully connected layers) fed into the rnn generator.
One exception is when the generator uses attention, as in that case the expected size of the input tensor is <code>b x s x h</code>, which is the output of a sequence, text or timeseries input feature without reduced outputs or the output of a sequence-based combiner.
If a <code>b x h</code> input is provided to a generator decoder using an rnn with attention instead, an error will be raised during model building.</p>
<div class="codehilite"><pre><span></span><code>                            Output     Output
                               1  +-+    ... +--+    END
                               ^    |     ^     |     ^
+--------+   +---------+       |    |     |     |     |
|Combiner|   |Fully    |   +---+--+ | +---+---+ | +---+--+
|Output  +---&gt;Connected+---+RNN   +---&gt;RNN... +---&gt;RNN   |
|        |   |Layers   |   +---^--+ | +---^---+ | +---^--+
+--------+   +---------+       |    |     |     |     |
                              GO    +-----+     +-----+
</code></pre></div>

<ul>
<li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li>
</ul>
<p>These are the available parameters of a Generator decoder:</p>
<ul>
<li><code>fc_layers</code> (default <code>null</code>): it is a list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>fc_size</code>, <code>norm</code>, <code>activation</code>, <code>dropout</code>, <code>initializer</code> and <code>regularize</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the decoder will be used instead.</li>
<li><code>num_fc_layers</code> (default 0): this is the number of stacked fully connected layers that the input to the feature passes through. Their output is projected in the feature's output space.</li>
<li><code>fc_size</code> (default <code>256</code>): if a <code>fc_size</code> is not already specified in <code>fc_layers</code> this is the default <code>fc_size</code> that will be used for each layer. It indicates the size of the output of a fully connected layer.</li>
<li><code>use_bias</code> (default <code>true</code>): boolean, whether the layer uses a bias vector.</li>
<li><code>weights_initializer</code> (default <code>'glorot_uniform'</code>): initializer for the weights matrix. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>bias_initializer</code> (default <code>'zeros'</code>):  initializer for the bias vector. Options are: <code>constant</code>, <code>identity</code>, <code>zeros</code>, <code>ones</code>, <code>orthogonal</code>, <code>normal</code>, <code>uniform</code>, <code>truncated_normal</code>, <code>variance_scaling</code>, <code>glorot_normal</code>, <code>glorot_uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>he_normal</code>, <code>he_uniform</code>, <code>lecun_normal</code>, <code>lecun_uniform</code>. Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. To know the parameters of each initializer, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers">TensorFlow's documentation</a>.</li>
<li><code>weights_regularizer</code> (default <code>null</code>): regularizer function applied to the weights matrix.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>bias_regularizer</code> (default <code>null</code>): regularizer function applied to the bias vector.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>activity_regularizer</code> (default <code>null</code>): regurlizer function applied to the output of the layer.  Valid values are <code>l1</code>, <code>l2</code> or <code>l1_l2</code>.</li>
<li><code>norm</code> (default <code>null</code>): if a <code>norm</code> is not already specified in <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. It indicates the norm of the output and it can be <code>null</code>, <code>batch</code> or <code>layer</code>.</li>
<li><code>norm_params</code> (default <code>null</code>): parameters used if <code>norm</code> is either <code>batch</code> or <code>layer</code>.  For information on parameters used with <code>batch</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">Tensorflow's documentation on batch normalization</a> or for <code>layer</code> see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">Tensorflow's documentation on layer normalization</a>.</li>
<li><code>activation</code> (default <code>relu</code>): if an <code>activation</code> is not already specified in <code>fc_layers</code> this is the default <code>activation</code> that will be used for each layer. It indicates the activation function applied to the output.</li>
<li><code>dropout</code> (default <code>0</code>): dropout rate</li>
<li><code>cell_type</code> (default <code>rnn</code>): the type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>lstm_block</code>, <code>lstm</code>, <code>ln</code>, <code>lstm_cudnn</code>, <code>gru</code>, <code>gru_block</code>, <code>gru_cudnn</code>. For reference about the differences between the cells please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell">TensorFlow's documentation</a>. We suggest to use the <code>block</code> variants on CPU and the <code>cudnn</code> variants on GPU because of their increased speed.</li>
<li><code>state_size</code> (default <code>256</code>): the size of the state of the rnn.</li>
<li><code>embedding_size</code> (default <code>256</code>): if <code>tied_target_embeddings</code> is <code>false</code>, the input embeddings and the weights of the softmax_cross_entropy weights before the softmax_cross_entropy are not tied together and can have different sizes, this parameter describes the size of the embeddings of the inputs of the generator.</li>
<li><code>beam_width</code> (default <code>1</code>): sampling from the rnn generator is performed using beam search. By default, with a beam of one, only a greedy sequence using always the most probably next token is generated, but the beam size can be increased. This usually leads to better performance at the expense of more computation and slower generation.</li>
<li><code>attention</code> (default <code>null</code>): the recurrent generator may use an attention mechanism. The available ones are <code>bahdanau</code> and <code>luong</code> (for more information refer to <a href="https://www.tensorflow.org/api_guides/python/contrib.seq2seq#Attention">TensorFlow's documentation</a>). When <code>attention</code> is not <code>null</code> the expected size of the input tensor is <code>b x s x h</code>, which is the output of a sequence, text or timeseries input feature without reduced outputs or the output of a sequence-based combiner. If a <code>b x h</code> input is provided to a generator decoder using an rnn with attention instead, an error will be raised during model building.</li>
<li><code>tied_embeddings</code> (default <code>null</code>): if <code>null</code> the embeddings of the targets are initialized randomly, while if the values is the name of an input feature, the embeddings of that input feature will be used as embeddings of the target. The <code>vocabulary_size</code> of that input feature has to be the same of the output feature one and it has to have an embedding matrix (binary and numerical features will not have one, for instance). In this case the <code>embedding_size</code> will be the same as the <code>state_size</code>. This is useful for implementing autoencoders where the encoding and decoding part of the model share parameters.</li>
<li><code>max_sequence_length</code> (default <code>0</code>):</li>
</ul>
<p>Example sequence feature entry using a generator decoder (with default parameters) in the output features list:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence_column_name</span>
<span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sequence</span>
<span class="nt">decoder</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">generator</span>
<span class="nt">reduce_input</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sum</span>
<span class="nt">dependencies</span><span class="p">:</span> <span class="p p-Indicator">[]</span>
<span class="nt">reduce_dependencies</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sum</span>
<span class="nt">loss</span><span class="p">:</span>
    <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">softmax_cross_entropy</span>
    <span class="nt">confidence_penalty</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">robust_lambda</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">class_weights</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">class_similarities</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="nt">class_similarities_temperature</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">labels_smoothing</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">negative_samples</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="nt">sampler</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="nt">distortion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">unique</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="nt">fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">num_fc_layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">fc_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">use_bias</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">weights_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">glorot_uniform</span>
<span class="nt">bias_initializer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">zeros</span>
<span class="nt">weights_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">bias_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activity_regularizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">norm_params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">activation</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">relu</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">cell_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">rnn</span>
<span class="nt">state_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">embedding_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="nt">beam_width</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">attention</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">tied_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">max_sequence_length</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
</code></pre></div>

<h3 id="sequence-features-measures">Sequence Features Measures<a class="headerlink" href="#sequence-features-measures" title="Permanent link">&para;</a></h3>
<p>The measures that are calculated every epoch and are available for category features are <code>accuracy</code> (counts the number of datapoints where all the elements of the predicted sequence are correct over the number of all datapoints), <code>token_accuracy</code> (computes the number of elements in all the sequences that are correctly predicted over the number of all the elements in all the sequences), <code>last_accuracy</code> (accuracy considering only the last element of the sequence, it is useful for being sure special end-of-sequence tokens are generated or tagged), <code>edit_distance</code> (the levenshtein distance between the predicted and ground truth sequence), <code>perplexity</code> (the perplexity of the ground truth sequence according to the model) and the <code>loss</code> itself.
You can set either of them as <code>validation_measure</code> in the <code>training</code> section of the configuration if you set the <code>validation_field</code> to be the name of a sequence feature.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        

<!-- Application footer -->
<footer class="md-footer">

    <!-- Link to previous and/or next page -->
    
    <div class="md-footer-nav">
        <nav aria-label="Footer"
             class="md-footer-nav__inner md-grid">
            
            <a class="md-footer-nav__link md-footer-nav__link--prev"
               href="../bag_features/" rel="prev"
               title="Bag Features">
                <div class="md-footer-nav__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
                </div>
                <div class="md-footer-nav__title">
                    <div class="md-ellipsis">
                 <span class="md-footer-nav__direction">
                   Previous
                 </span>
                        Bag Features
                    </div>
                </div>
            </a>
            
            
            <a class="md-footer-nav__link md-footer-nav__link--next"
               href="../text_features/" rel="next"
               title="Text Features">
                <div class="md-footer-nav__title">
                    <div class="md-ellipsis">
                 <span class="md-footer-nav__direction">
                   Next
                 </span>
                        Text Features
                    </div>
                </div>
                <div class="md-footer-nav__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
                </div>
            </a>
            
        </nav>
    </div>
    

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">

            <!-- Copyright and theme information -->
            <div class="md-footer-copyright">
                <div class="footer-logo-smallpad"></div>
                
                <div class="md-footer-copyright__highlight">
                    Copyright &copy; 2018 - 2020 Uber Technologies Inc., 2021 Linux Foundation Data & AI
                </div>
                
                Website by <a href="http://w4nderlu.st">w4nderlust</a> powered by
                <a href="https://www.mkdocs.org">MkDocs</a>,
                <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a>,
                <a href="http://www.styleshout.com/">styleshout</a> and
                <a href="http://cables.gl/">cables</a>.
            </div>

            <!-- Social links -->
            
            
            
        </div>
    </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../../../assets/javascripts/workers/search.fb4a9340.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.a1c7c35e.min.js"></script>
      
    
  </body>
</html>