{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ludwig","text":"<p> Declarative machine learning: End-to-end machine learning pipelines using simple and flexible data-driven configurations. </p> <p> </p>"},{"location":"#what-is-ludwig","title":"What is Ludwig?","text":"<p>Ludwig is a declarative machine learning framework that makes it easy to define machine learning pipelines using a simple and flexible data-driven configuration system. Ludwig is suitable for a wide variety of AI tasks, and is hosted by the Linux Foundation AI &amp; Data.</p> <p>The configuration declares the input and output features, with their respective data types. Users can also specify additional parameters to preprocess, encode, and decode features, load from pre-trained models, compose the internal model architecture, set training parameters, or run hyperparameter optimization.</p> <p></p> <p>Ludwig will build an end-to-end machine learning pipeline automatically, using whatever is explicitly specified in the configuration, while falling back to smart defaults for any parameters that are not.</p>"},{"location":"#declarative-machine-learning","title":"Declarative Machine Learning","text":"<p>Ludwig\u2019s declarative approach to machine learning empowers you to have full control of the components of the machine learning pipeline that you care about, while leaving it up to Ludwig to make reasonable decisions for the rest.</p> <p></p> <p>Analysts, scientists, engineers, and researchers use Ludwig to explore state-of-the-art model architectures, run hyperparameter search, scale up to larger than available memory datasets and multi-node clusters, and finally serve the best model in production.</p> <p>Finally, the use of abstract interfaces throughout the codebase makes it easy for users to extend Ludwig by adding new models, metrics, losses, and preprocessing functions that can be registered to make them immediately useable in the same unified configuration system.</p>"},{"location":"#main-features","title":"Main Features","text":"<ul> <li>Data-Driven configuration system</li> </ul> <p>A config YAML file that describes the schema of your data (input features,   output features, and their types) is all you need to start training deep   learning models. Ludwig uses declared features to compose a deep learning   model accordingly.</p> <pre><code>input_features:\n- name: data_column_1\ntype: number\n- name: data_column_2\ntype: category\n- name: data_column_3\ntype: text\n- name: data_column_4\ntype: image\n...\n\noutput_features:\n- name: data_column_5\ntype: number\n- name: data_column_6\ntype: category\n...\n</code></pre> <ul> <li>Training, prediction, and evaluation from the command line</li> </ul> <p>Simple commands can be used to train models and predict new data.</p> <pre><code>ludwig train --config config.yaml --dataset data.csv\nludwig predict --model_path results/experiment_run/model --dataset test.csv\nludwig eval --model_path results/experiment_run/model --dataset test.csv\n</code></pre> <ul> <li>Programmatic API</li> </ul> <p>Ludwig also provides a simple programmatic API for all of the functionality   described above and more.</p> <pre><code>from ludwig.api import LudwigModel\n\n# train a model\nconfig = {\n    \"input_features\": [...],\n    \"output_features\": [...],\n}\nmodel = LudwigModel(config)\ndata = pd.read_csv(\"data.csv\")\ntrain_stats, _, model_dir = model.train(data)\n\n# or load a model\nmodel = LudwigModel.load(model_dir)\n\n# obtain predictions\npredictions = model.predict(data)\n</code></pre> <ul> <li>Distributed training</li> </ul> <p>Scale to very large datasets, train on multiple GPUs and multiple machines   in a distributed setting using Ray, with no code or   config changes required. Runs natively in the cloud on Kubernetes   using KubeRay.</p> <ul> <li>Serving</li> </ul> <p>Serve models using FastAPI with a single command:</p> <pre><code>ludwig serve --model_path ./results/experiment_run/model\ncurl http://0.0.0.0:8000/predict -X POST -F \"movie_title=Friends With Money\" -F \"content_rating=R\" -F \"genres=Art House &amp; International, Comedy, Drama\" -F \"runtime=88.0\" -F \"top_critic=TRUE\" -F \"review_content=The cast is terrific, the movie isn't.\"\n</code></pre> <p>For optimized performance, compile end-to-end models with TorchScript   and serve with Nviida Triton.</p> <ul> <li>Hyperparameter optimization</li> </ul> <p>Run hyperparameter optimization locally or using Ray Tune.</p> <pre><code>ludwig hyperopt --config config.yaml --dataset data.csv\n</code></pre> <ul> <li>AutoML</li> </ul> <p>Ludwig AutoML takes a dataset, the target column, and a time budget, and   returns a trained Ludwig model.</p> <ul> <li>Third-Party integrations</li> </ul> <p>Ludwig provides an extendable interface to integrate with third-party   systems for tracking experiments. Third-party integrations exist for Comet   ML, Weights &amp; Biases, WhyLabs, and MLFlow.</p> <ul> <li>Extensibility</li> </ul> <p>Ludwig is built from the ground up with extensibility in mind. It is easy to   add new data types by implementing clear, well-documented abstract classes   that define functions to preprocess, encode, and decode data.</p> <p>Furthermore, new <code>torch nn.Module</code> models can be easily added by them to a   registry. This encourages reuse and sharing new models with the community.   Refer to the Developer Guide   for further details.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>For a full tutorial, check out the official getting started guide, or take a look at end-to-end Examples.</p>"},{"location":"#step-1-install","title":"Step 1: Install","text":"<p>Install from PyPi. Be aware that Ludwig requires Python 3.7+.</p> <pre><code>pip install ludwig\n</code></pre>"},{"location":"#step-2-define-a-configuration","title":"Step 2: Define a configuration","text":"<p>Create a config that describes the schema of your data.</p> <p>Assume we have a text classification task, with data containing a sentence and class column like the following.</p> sentence class Former president Barack Obama ... politics Juventus hired Cristiano Ronaldo ... sport LeBron James joins the Lakers ... sport ... ... <p>A configuration will look like this.</p> <pre><code>input_features:\n- name: sentence\ntype: text\n\noutput_features:\n- name: class\ntype: category\n</code></pre> <p>Starting from a simple config like the one above, any and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements.</p> <pre><code>input_features:\n- name: sentence\ntype: text\nencoder: transformer\nlayers: 6\nembedding_size: 512\n\noutput_features:\n- name: class\ntype: category\nloss: cross_entropy\n\ntrainer:\nepochs: 50\nbatch_size: 64\noptimizer:\ntype: adamw\nbeat1: 0.9\nlearning_rate: 0.001\n\nbackend:\ntype: ray\ncache_format: parquet\nprocessor:\ntype: dask\ntrainer:\nuse_gpu: true\nnum_workers: 4\nresources_per_worker:\nCPU: 4\nGPU: 1\n\nhyperopt:\nmetric: f1\nsampler: random\nparameters:\ntitle.num_layers:\nlower: 1\nupper: 5\ntrainer.learning_rate:\nvalues: [0.01, 0.003, 0.001]\n</code></pre> <p>For details on what can be configured, check out Ludwig Configuration docs.</p>"},{"location":"#step-3-train-a-model","title":"Step 3: Train a model","text":"<p>Simple commands can be used to train models and predict new data.</p> <pre><code>ludwig train --config config.yaml --dataset data.csv\n</code></pre>"},{"location":"#step-4-predict-and-evaluate","title":"Step 4: Predict and evaluate","text":"<p>The training process will produce a model that can be used for evaluating on and obtaining predictions for new data.</p> <pre><code>ludwig predict --model path/to/trained/model --dataset heldout.csv\nludwig evaluate --model path/to/trained/model --dataset heldout.csv\n</code></pre>"},{"location":"#step-5-visualize","title":"Step 5: Visualize","text":"<p>Ludwig provides a suite of visualization tools allows you to analyze models' training and test performance and to compare them.</p> <pre><code>ludwig visualize --visualization compare_performance --test_statistics path/to/test_statistics_model_1.json path/to/test_statistics_model_2.json\n</code></pre> <p>For the full set of visualization see the Visualization Guide.</p>"},{"location":"#step-6-happy-modeling","title":"Step 6: Happy modeling","text":"<p>Try applying Ludwig to your data. Reach out if you have any questions.</p>"},{"location":"#advantages","title":"Advantages","text":"<ul> <li>Minimal machine learning boilerplate</li> </ul> <p>Ludwig takes care of the engineering complexity of machine learning out of   the box, enabling research scientists to focus on building models at the   highest level of abstraction. Data preprocessing, hyperparameter   optimization, device management, and distributed training for   <code>torch.nn.Module</code> models come completely free.</p> <ul> <li>Easily build your benchmarks</li> </ul> <p>Creating a state-of-the-art baseline and comparing it with a new model is a   simple config change.</p> <ul> <li>Easily apply new architectures to multiple problems and datasets</li> </ul> <p>Apply new models across the extensive set of tasks and datasets that Ludwig   supports. Ludwig includes a   full benchmarking toolkit accessible to   any user, for running experiments with multiple models across multiple   datasets with just a simple configuration.</p> <ul> <li>Highly configurable data preprocessing, modeling, and metrics</li> </ul> <p>Any and all aspects of the model architecture, training loop, hyperparameter   search, and backend infrastructure can be modified as additional fields in   the declarative configuration to customize the pipeline to meet your   requirements. For details on what can be configured, check out   Ludwig Configuration   docs.</p> <ul> <li>Multi-modal, multi-task learning out-of-the-box</li> </ul> <p>Mix and match tabular data, text, images, and even audio into complex model   configurations without writing code.</p> <ul> <li>Rich model exporting and tracking</li> </ul> <p>Automatically track all trials and metrics with tools like Tensorboard,   Comet ML, Weights &amp; Biases, MLFlow, and Aim Stack.</p> <ul> <li>Automatically scale training to multi-GPU, multi-node clusters</li> </ul> <p>Go from training on your local machine to the cloud without code changes.</p> <ul> <li>Low-code interface for state-of-the-art models, including pre-trained Huggingface Transformers</li> </ul> <p>Ludwig also natively integrates with pre-trained models, such as the ones   available in Huggingface Transformers.   Users can choose from a vast collection of state-of-the-art pre-trained   PyTorch models to use without needing to write any code at all. For example,   training a BERT-based sentiment analysis model with Ludwig is as simple as:</p> <pre><code>ludwig train --dataset sst5 --config_str \u201c{input_features: [{name: sentence, type: text, encoder: bert}], output_features: [{name: label, type: category}]}\u201d\n</code></pre> <ul> <li>Low-code interface for AutoML</li> </ul> <p>Ludwig AutoML   allows users to obtain trained models by providing just a dataset, the   target column, and a time budget.</p> <pre><code>auto_train_results = ludwig.automl.auto_train(dataset=my_dataset_df, target=target_column_name, time_limit_s=7200)\n</code></pre> <ul> <li>Easy productionisation</li> </ul> <p>Ludwig makes it easy to serve deep learning models, including on GPUs.   Launch a REST API for your trained Ludwig model.</p> <pre><code>ludwig serve --model_path=/path/to/model\n</code></pre> <p>Ludwig supports exporting models to efficient Torschscript bundles.</p> <pre><code>ludwig export_torchscript -\u2013model_path=/path/to/model\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Text Classification</li> <li>Tabular Data Classification</li> <li>Image Classification</li> <li>Multimodal Classification</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Named Entity Recognition Tagging</li> <li>Natural Language Understanding</li> <li>Machine Translation</li> <li>Chit-Chat Dialogue Modeling through seq2seq</li> <li>Sentiment Analysis</li> <li>One-shot Learning with Siamese Networks</li> <li>Visual Question Answering</li> <li>Spoken Digit Speech Recognition</li> <li>Speaker Verification</li> <li>Binary Classification (Titanic)</li> <li>Timeseries forecasting</li> <li>Timeseries forecasting (Weather)</li> <li>Movie rating prediction</li> <li>Multi-label classification</li> <li>Multi-Task Learning</li> <li>Simple Regression: Fuel Efficiency Prediction</li> <li>Fraud Detection</li> </ul>"},{"location":"#more-information","title":"More Information","text":"<p>Read our publications on Ludwig, declarative ML, and Ludwig\u2019s SoTA benchmarks.</p> <p>Learn more about how Ludwig works, how to get started, and work through more examples.</p> <p>If you are interested in contributing, have questions, comments, or thoughts to share, or if you just want to be in the know, please consider joining the Ludwig Slack and follow us on Twitter!</p>"},{"location":"#getting-involved","title":"Getting Involved","text":"<ul> <li>Slack</li> <li>Twitter</li> <li>Medium</li> <li>GitHub Issues</li> </ul>"},{"location":"community/","title":"Community","text":""},{"location":"community/#chat","title":"Chat","text":"<p>We use Slack as a chat solution for allowing both Ludwig users and developers to interact in a timely, more synchronous way.</p> <p>Click here to receive an invitation.</p>"},{"location":"community/#forum","title":"Forum","text":"<p>We use GitHub Discussions to provide a forum for the community to discuss. Everything that is not an issue and relates Ludwig can be discussed here: use-cases, requests for help and suggestions, discussions on the future of the project, and other similar topics. The forum is ideal for asynchronous communication.</p>"},{"location":"community/#community-policy","title":"Community Policy","text":"<p>We craft Ludwig with love and care, to the best of our skills and knowledge, and this project would not be possible without the contribution of an incredible community.</p> <p>Members of the Ludwig community provide fixes, new features, functionality, and documentation, which not only improves Ludwig but also shapes technical direction.</p> <p>We are really grateful for that and in exchange we strive to make the development process as open as possible and communication with the development team easy and direct.</p> <p>We strive to create an inclusive community where everyone is respected. Harassment and any other form of non-inclusive behavior will not be tolerated.</p>"},{"location":"community/#issues","title":"Issues","text":"<p>If you encounter an issue when using Ludwig, please add it to our GitHub Issues tracker. Please make sure we are able to replicate the issue by providing the model definition + command + data or code + data.</p> <p>If your data cannot be shared, please use the <code>synthesize_dataset</code> command line utility to create a synthetic data with the same feature types.</p> <p>Example:</p> <pre><code>ludwig synthesize_dataset --features=\"[ \\\n  {name: text, type: text}, \\\n  {name: category, type: category}, \\\n  {name: number, type: number}, \\\n  {name: binary, type: binary}, \\\n  {name: set, type: set}, \\\n  {name: bag, type: bag}, \\\n  {name: sequence, type: sequence}, \\\n  {name: timeseries, type: timeseries}, \\\n  {name: date, type: date}, \\\n  {name: h3, type: h3}, \\\n  {name: vector, type: vector}, \\\n  {name: image, type: image} \\\n]\" --dataset_size=10 --output_path=synthetic_dataset.csv\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#where-can-i-find-ludwigs-development-roadmap","title":"Where can I find Ludwig's development roadmap?","text":"<p>Larger projects are tracked in GitHub Projects.</p> <p>Smaller feature requests are tracked in Github Issues.</p> <p>We try our best to keep these up to date, but if there's a specific feature or model you are interested in, feel free to ping the Ludwig Slack.</p>"},{"location":"faq/#how-can-i-help","title":"How can I help?","text":"<p>Join the Ludwig Slack! Sometimes we'll organize community fixit, documentation, and bug bash efforts, or consider taking on an easy bug to start.</p>"},{"location":"faq/#can-i-use-ludwig-for-my-project","title":"Can I use Ludwig for my project?","text":"<p>Yes! The people behind Ludwig are veterans of research and open source. If your work does get published, please consider citing Ludwig and submitting improvements back to Ludwig.</p> <p>How to cite:</p> <pre><code>@misc{Molino2019,\n  author = {Piero Molino and Yaroslav Dudin and Sai Sumanth Miryala},\n  title = {Ludwig: a type-based declarative deep learning toolbox},\n  year = {2019},\n  eprint = {arXiv:1909.07930},\n}\n</code></pre>"},{"location":"faq/#can-i-use-ludwig-models-in-production","title":"Can I use Ludwig models in production?","text":"<p>Yes! Ludwig models can be exported to Neuropod, MLFlow, and Torchscript. <code>ludwig serve</code> provides basic POST/GET serving endpoints, powered by FastAPI.</p> <p>If you are interested in a more sophisticated, hosted cloud infrastructure solution with reliable SLAs, check out Predibase.</p>"},{"location":"faq/#does-ludwigs-architecture-work-for-model-x","title":"Does Ludwig's Architecture work for model X?","text":"<p>It's likely.</p> <p>Ludwig's encoder-combiner-decoder framework is designed to generally mapping some input to some output.</p> <ul> <li>Encoders parse raw input data into tensors (potentially using a model).</li> <li>Combiners combine the outputs of Input Encoders (potentially using a model).</li> <li>Decoders decode the outputs of Encoders and Combiners into output tensors   (potentially using a model).</li> </ul> <p>Decoder-only, encoder-only, encoder-decoder, vanilla feed-forward, transformers, GBMs, and more have all been implemented in Ludwig.</p>"},{"location":"faq/#what-does-ludwig-not-support-yet","title":"What does Ludwig not support (yet)?","text":"<p>Domains of deep learning that Ludwig does not support (yet):</p> <ul> <li>Self-supervised learning.</li> <li>Reinforcement learning.</li> <li>Generative image and audio models (generative text models are supported).</li> </ul> <p>We are actively working on supporting self-supervised learning.</p>"},{"location":"faq/#do-all-datasets-need-to-be-loaded-in-memory","title":"Do all datasets need to be loaded in memory?","text":"<p>Locally, it depends on the type of feature: image features can be dynamically loaded from disk from an opened hdf5 file, while other types of features are loaded entirely in memory for speed.</p> <p>Ludwig supports training with very large datasets on Ray using Ray Datasets. Read more about using Ludwig on Ray.</p> <p>If you are interested in a premium hosted Ludwig infrastructure and APIs, with a richer set of APIs to support modeling with large datasets, check out Predibase.</p>"},{"location":"faq/#who-develops-ludwig","title":"Who develops Ludwig?","text":"<p>Ludwig was created in 2019 by Piero Molino, with help from Yaroslav Dudin, and Sai Sumanth Miryala while at Uber AI.</p> <p>Today, Ludwig is open source, supported by the Linux Foundation, with source code hosted on Github.</p> <p>Ludwig is actively developed and maintained by Ludwig Maintainers, which consists mostly of staff at Predibase, and community contributors, all of whom are listed in each of Ludwig's release notes.</p> <p>Happy contributing!</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration-structure","title":"Configuration Structure","text":"<p>Ludwig models are configured by a single config with the following parameters:</p> <pre><code>model_type: ecd\ninput_features: []\noutput_features: []\ncombiner: {}\npreprocessing: {}\ndefaults: {}\ntrainer: {}\nhyperopt: {}\nbackend: {}\n</code></pre> <p>The config specifies input features, output features, preprocessing, model architecture, training loop, hyperparameter search, and backend infrastructure -- everything that's needed to build, train, and evaluate a model:</p> <ul> <li>model_type: the model variant used for training. Defaults to ECD, which is a neural network based architecture. Also supports GBM, a gradient-boosted machine (tree based model).</li> <li>input_features: which columns from your training dataset will be used as inputs to the model, what their data types are, how they should be preprocessed, and how they should be encoded.</li> <li>output_features: the targets we want the model to learn to predict. The data type of the output feature defines the task (<code>number</code> is a regression task, <code>category</code> is a multi-class classification task, etc.).</li> <li>combiner: the backbone model architecture that takes as input all encoded input features and transforms them into a single embedding vector. The combiner effectively combines individual feature-level models into a model that can accept any number of inputs. GBM models do not make use of the combiner.</li> <li>preprocessing: global preprocessing options including how to split the dataset and how to sample the data.</li> <li>defaults default feature configuration. Useful when you have many input features of the same type, and want to apply the same preprocessing, encoders, etc. to all of them. Overridden by the feature-level configuration if provided.</li> <li>trainer: hyperparameters used to control the training process, including batch size, learning rate, number of training epochs, etc.</li> <li>hyperopt: hyperparameter optimization options. Any param from the previous sections can be treated as a hyperparameter and explored in combination with other config params.</li> <li>backend: infrastructure and runtime options, including what libraries and distribution strategies will be used during training, how many cluster resources to use per training worker, how many total workers, whether to use GPUs, etc.</li> </ul> <p>The Ludwig configuration mixes ease of use, by means of reasonable defaults, with flexibility, by means of detailed control over the parameters of your model. Only <code>input_features</code> and <code>output_features</code> are required while all other fields use reasonable defaults, but can be optionally set or modified manually.</p> <p>The config can be expressed as a python dictionary (<code>--config_str</code> for Ludwig's CLI), or as a YAML file (<code>--config</code>).</p> YAMLPython Dict <pre><code>input_features:\n-\nname: Pclass\ntype: category\n-\nname: Sex\ntype: category\n-\nname: Age\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: SibSp\ntype: number\n-\nname: Parch\ntype: number\n-\nname: Fare\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: Embarked\ntype: category\n\noutput_features:\n-\nname: Survived\ntype: binary\n</code></pre> <pre><code>{\n    \"input_features\": [\n        {\n            \"name\": \"Pclass\",\n            \"type\": \"category\"\n        },\n        {\n            \"name\": \"Sex\",\n            \"type\": \"category\"\n        },\n        {\n            \"name\": \"Age\",\n            \"type\": \"number\",\n            \"preprocessing\": {\n                \"missing_value_strategy\": \"fill_with_mean\"\n            }\n        },\n        {\n            \"name\": \"SibSp\",\n            \"type\": \"number\"\n        },\n        {\n            \"name\": \"Parch\",\n            \"type\": \"number\"\n        },\n        {\n            \"name\": \"Fare\",\n            \"type\": \"number\",\n            \"preprocessing\": {\n                \"missing_value_strategy\": \"fill_with_mean\"\n            }\n        },\n        {\n            \"name\": \"Embarked\",\n            \"type\": \"category\"\n        }\n    ],\n    \"output_features\": [\n        {\n            \"name\": \"Survived\",\n            \"type\": \"binary\"\n        }\n    ]\n}\n</code></pre>"},{"location":"configuration/#rendered-defaults","title":"Rendered Defaults","text":"<p>Ludwig has many parameter options, but with the exception of input and output feature names and types, all of the other parameters are optional. When a parameter is unspecified, Ludwig assigns it a reasonable default value. Ludwig defines \"reasonable\" to mean that it is unlikely to produce bad results, and will train in a reasonable amount of time on commodity hardware. In other words, Ludwig defaults are intended to be good baseline configs upon which more advanced options can be layed on top.</p> <p>Here's an example of a minimal config generated from the following command:</p> <pre><code>ludwig init_config --dataset ludwig://sst2 --target label --output sst2.yaml\n</code></pre> <pre><code>input_features:\n- name: sentence\ntype: text\noutput_features:\n- name: label\ntype: binary\n</code></pre> <p>And here is the fully rendered config generated from the following command:</p> <pre><code>ludwig render_config --config sst2.yaml --output sst2_rendered.yaml\n</code></pre> <pre><code>input_features:\n-   preprocessing:\npretrained_model_name_or_path: null\ntokenizer: space_punct\nvocab_file: null\nmax_sequence_length: 256\nmost_common: 20000\npadding_symbol: &lt;PAD&gt;\nunknown_symbol: &lt;UNK&gt;\npadding: right\nlowercase: true\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nngram_size: 2\ncache_encoder_embeddings: false\nencoder:\ntype: parallel_cnn\nskip: false\ndropout: 0.0\nactivation: relu\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nshould_embed: true\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nreduce_output: sum\nnum_conv_layers: null\nconv_layers: null\nnum_filters: 256\nfilter_size: 3\npool_function: max\npool_size: null\noutput_size: 256\nnorm: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nactive: true\nname: sentence\ntype: text\ncolumn: sentence\nproc_column: sentence_v4k3LH\ntied: null\noutput_features:\n-   decoder:\ntype: regressor\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: binary_weighted_cross_entropy\nweight: 1.0\npositive_class_weight: null\nrobust_lambda: 0\nconfidence_penalty: 0\nactive: true\nname: label\ntype: binary\ncolumn: label\nproc_column: label_2Xl8CP\nreduce_input: sum\ndefault_validation_metric: roc_auc\ndependencies: []\nreduce_dependencies: sum\ninput_size: null\nnum_classes: null\ncalibration: false\npreprocessing:\nmissing_value_strategy: drop_row\nfallback_true_label: null\nfill_value: null\ncomputed_fill_value: null\nthreshold: 0.5\nmodel_type: ecd\ntrainer:\nvalidation_field: label\nvalidation_metric: roc_auc\nearly_stop: 5\nlearning_rate: 0.001\nlearning_rate_scheduler:\ndecay: null\ndecay_rate: 0.96\ndecay_steps: 10000\nstaircase: false\nreduce_on_plateau: 0\nreduce_on_plateau_patience: 10\nreduce_on_plateau_rate: 0.1\nwarmup_evaluations: 0\nwarmup_fraction: 0.0\nreduce_eval_metric: loss\nreduce_eval_split: training\nepochs: 100\ncheckpoints_per_epoch: 0\ntrain_steps: null\nsteps_per_checkpoint: 0\nbatch_size: auto\nmax_batch_size: 1099511627776\neval_batch_size: null\nevaluate_training_set: false\noptimizer:\ntype: adam\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\namsgrad: false\nregularization_type: l2\nregularization_lambda: 0.0\nshould_shuffle: true\nincrease_batch_size_on_plateau: 0\nincrease_batch_size_on_plateau_patience: 5\nincrease_batch_size_on_plateau_rate: 2.0\nincrease_batch_size_eval_metric: loss\nincrease_batch_size_eval_split: training\ngradient_clipping:\nclipglobalnorm: 0.5\nclipnorm: null\nclipvalue: null\nlearning_rate_scaling: linear\nbucketing_field: null\nuse_mixed_precision: false\npreprocessing:\nsample_ratio: 1.0\noversample_minority: null\nundersample_majority: null\nsplit:\ntype: random\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\ndefaults:\naudio:\npreprocessing:\naudio_file_length_limit_in_s: 7.5\nmissing_value_strategy: bfill\nfill_value: null\ncomputed_fill_value: null\nin_memory: true\npadding_value: 0.0\nnorm: null\ntype: fbank\nwindow_length_in_s: 0.04\nwindow_shift_in_s: 0.02\nnum_fft_points: null\nwindow_type: hamming\nnum_filter_bands: 80\nencoder:\ntype: parallel_cnn\nskip: false\ndropout: 0.0\nactivation: relu\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nshould_embed: true\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nreduce_output: sum\nnum_conv_layers: null\nconv_layers: null\nnum_filters: 256\nfilter_size: 3\npool_function: max\npool_size: null\noutput_size: 256\nnorm: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nbag:\npreprocessing:\ntokenizer: space\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nlowercase: false\nmost_common: 10000\nencoder:\ntype: embed\nskip: false\ndropout: 0.0\nactivation: relu\nvocab: null\nrepresentation: dense\nembedding_size: 50\nforce_embedding_size: false\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\noutput_size: 10\nnorm: null\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\nbinary:\ndecoder:\ntype: regressor\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: binary_weighted_cross_entropy\nweight: 1.0\npositive_class_weight: null\nrobust_lambda: 0\nconfidence_penalty: 0\npreprocessing:\nmissing_value_strategy: fill_with_false\nfallback_true_label: null\nfill_value: null\ncomputed_fill_value: null\nencoder:\ntype: passthrough\nskip: false\ncategory:\ndecoder:\ntype: classifier\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nnum_classes: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: softmax_cross_entropy\nweight: 1.0\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nlowercase: false\nmost_common: 10000\nencoder:\ntype: dense\nskip: false\ndropout: 0.0\nvocab: null\nembedding_initializer: null\nembedding_size: 50\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\ndate:\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: ''\ncomputed_fill_value: ''\ndatetime_format: null\nencoder:\ntype: embed\nskip: false\ndropout: 0.0\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembedding_size: 10\nembeddings_on_cpu: false\noutput_size: 10\nnorm: null\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\nh3:\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: 576495936675512319\ncomputed_fill_value: 576495936675512319\nencoder:\ntype: embed\nskip: false\ndropout: 0.0\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembedding_size: 10\nembeddings_on_cpu: false\nreduce_output: sum\noutput_size: 10\nnorm: null\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\nimage:\npreprocessing:\nmissing_value_strategy: bfill\nfill_value: null\ncomputed_fill_value: null\nheight: null\nwidth: null\nnum_channels: null\nresize_method: interpolate\ninfer_image_num_channels: true\ninfer_image_dimensions: true\ninfer_image_max_height: 256\ninfer_image_max_width: 256\ninfer_image_sample_size: 100\nstandardize_image: null\nin_memory: true\nnum_processes: 1\nrequires_equal_dimensions: false\nencoder:\ntype: stacked_cnn\nskip: false\nconv_dropout: 0.0\nconv_activation: relu\nheight: null\nwidth: null\nnum_channels: null\nout_channels: 32\nkernel_size: 3\nstride: 1\npadding_mode: zeros\npadding: valid\ndilation: 1\ngroups: 1\npool_function: max\npool_kernel_size: 2\npool_stride: null\npool_padding: 0\npool_dilation: 1\noutput_size: 128\nconv_use_bias: true\nconv_norm: null\nconv_norm_params: null\nnum_conv_layers: null\nconv_layers: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_use_bias: true\nfc_bias_initializer: zeros\nfc_weights_initializer: xavier_uniform\nfc_norm: null\nfc_norm_params: null\nnum_fc_layers: 1\nfc_layers: null\naugmentation: []\nnumber:\ndecoder:\ntype: regressor\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: mean_squared_error\nweight: 1.0\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: 0.0\ncomputed_fill_value: 0.0\nnormalization: zscore\noutlier_strategy: null\noutlier_threshold: 3.0\ncomputed_outlier_fill_value: 0.0\nencoder:\ntype: passthrough\nskip: false\nsequence:\ndecoder:\ntype: generator\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\nvocab_size: null\nmax_sequence_length: null\ncell_type: gru\ninput_size: 256\nreduce_input: sum\nnum_layers: 1\nloss:\ntype: sequence_softmax_cross_entropy\nweight: 1.0\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nunique: false\npreprocessing:\ntokenizer: space\nvocab_file: null\nmax_sequence_length: 256\nmost_common: 20000\npadding_symbol: &lt;PAD&gt;\nunknown_symbol: &lt;UNK&gt;\npadding: right\nlowercase: false\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nngram_size: 2\ncache_encoder_embeddings: false\nencoder:\ntype: embed\nskip: false\ndropout: 0.0\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nweights_initializer: uniform\nreduce_output: sum\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nset:\ndecoder:\ntype: classifier\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nnum_classes: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: sigmoid_cross_entropy\nweight: 1.0\nclass_weights: null\npreprocessing:\ntokenizer: space\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nlowercase: false\nmost_common: 10000\nencoder:\ntype: embed\nskip: false\ndropout: 0.0\nactivation: relu\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembedding_size: 50\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\noutput_size: 10\nnorm: null\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\ntext:\ndecoder:\ntype: generator\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\nvocab_size: null\nmax_sequence_length: null\ncell_type: gru\ninput_size: 256\nreduce_input: sum\nnum_layers: 1\nloss:\ntype: sequence_softmax_cross_entropy\nweight: 1.0\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nunique: false\npreprocessing:\npretrained_model_name_or_path: null\ntokenizer: space_punct\nvocab_file: null\nmax_sequence_length: 256\nmost_common: 20000\npadding_symbol: &lt;PAD&gt;\nunknown_symbol: &lt;UNK&gt;\npadding: right\nlowercase: true\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\ncomputed_fill_value: &lt;UNK&gt;\nngram_size: 2\ncache_encoder_embeddings: false\nencoder:\ntype: parallel_cnn\nskip: false\ndropout: 0.0\nactivation: relu\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nshould_embed: true\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nreduce_output: sum\nnum_conv_layers: null\nconv_layers: null\nnum_filters: 256\nfilter_size: 3\npool_function: max\npool_size: null\noutput_size: 256\nnorm: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\ntimeseries:\npreprocessing:\ntokenizer: space\ntimeseries_length_limit: 256\npadding_value: 0.0\npadding: right\nmissing_value_strategy: fill_with_const\nfill_value: ''\ncomputed_fill_value: ''\nencoder:\ntype: parallel_cnn\nskip: false\ndropout: 0.0\nactivation: relu\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nshould_embed: true\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nreduce_output: sum\nnum_conv_layers: null\nconv_layers: null\nnum_filters: 256\nfilter_size: 3\npool_function: max\npool_size: null\noutput_size: 256\nnorm: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nvector:\ndecoder:\ntype: projector\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\noutput_size: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nactivation: null\nclip: null\nloss:\ntype: mean_squared_error\nweight: 1.0\npreprocessing:\nvector_size: null\nmissing_value_strategy: fill_with_const\nfill_value: ''\ncomputed_fill_value: ''\nencoder:\ntype: dense\nskip: false\ndropout: 0.0\nactivation: relu\ninput_size: null\noutput_size: 256\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm: null\nnorm_params: null\nnum_layers: 1\nfc_layers: null\nhyperopt: null\nbackend: null\nludwig_version: '0.7'\ncombiner:\ntype: concat\ndropout: 0.0\nactivation: relu\nflatten_inputs: false\nresidual: false\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnum_fc_layers: 0\noutput_size: 256\nnorm: null\nnorm_params: null\nfc_layers: null\n</code></pre>"},{"location":"configuration/backend/","title":"Backend","text":"<p>The same Ludwig config / Python code that runs on your local machine can be executed remotely in a distributed manner with zero code changes. This distributed execution includes preprocessing, training, and batch prediction.</p> <p>In most cases, Ludwig will be able to automatically detect if you're running in an environment that supports distributed execution, but you can also make this explicit on the command line with the <code>--backend</code> arg or by providing a <code>backend</code> section to the Ludwig config YAML:</p> <pre><code>backend:\ntype: ray\ncache_dir: s3://my_bucket/cache\ncache_credentials: /home/user/.credentials.json\nprocessor:\ntype: dask\ntrainer:\nstrategy: horovod\nloader: {}\n</code></pre> <p>Parameters:</p> <ul> <li><code>type</code>: How the job will be distributed, one of <code>local</code>, <code>ray</code>, <code>horovod</code>.</li> <li><code>cache_dir</code>: Where the preprocessed data will be written on disk, defaults to the location of the input dataset. See Cloud Storage for more details</li> <li><code>cache_credentials</code>: Optional dictionary of credentials (or path to credential JSON file) used to write to the cache. See Cloud Storage for more details</li> <li><code>processor</code>: (Ray only) parameters to configure execution of distributed data processing.</li> <li><code>trainer</code>: (Ray only) parameters to configure execution of distributed training.</li> <li><code>loader</code>: (Ray only) parameters to configure data loading from processed data to training batches.</li> </ul>"},{"location":"configuration/backend/#processor","title":"Processor","text":"<p>The <code>processor</code> section configures distributed data processing. The <code>local</code> backend uses the Pandas dataframe library, which runs in a single process with the entire datasets in memory. To make the data processing scalable to large datasets, we support two distributed dataframe libraries with the <code>ray</code> backend:</p> <ul> <li><code>dask</code>: (default) a lazily executed version of distributed Pandas.</li> <li><code>modin</code>: an eagerly executed version of distributed Pandas.</li> </ul>"},{"location":"configuration/backend/#dask","title":"Dask","text":"<p>Dask is the default distributed data processing library when using Ludwig on Ray. It executes distributed Pandas operations on partitions of the data in parallel. One beneficial property of Dask is that it is executed lazily, which allows it to stream very large datasets without needing to hold the entire dataset in distributed memory at once.</p> <p>One downside to Dask is that it can require some tuning to get the best performance. There are two knobs we expose in Ludwig for tuning Dask:</p> <ul> <li><code>parallelism</code>: the number of partitions to divide the dataset into (defaults to letting Dask figure this out automatically).</li> <li><code>persist</code>: whether intermediate stages of preprocessing should be cached in distributed memory (default: <code>true</code>).</li> </ul> <p>Increasing <code>parallelism</code> can reduce memory pressure during preprocessing for large datasets and increase parallelism (horizontal scaling). The downside to too much parallelism is that there is some overhead for each partition-level operation (serialization and deserialization), which can dominate the runtime if set too high.</p> <p>Setting <code>persist</code> to <code>false</code> can be useful if the dataset is too large for all the memory and disk of the entire Ray cluster. Only set this to <code>false</code> if you're seeing issues running out of memory or disk space.</p> <p>Example:</p> <pre><code>backend:\ntype: ray\nprocessor:\ntype: dask\nparallelism: 100\npersist: true\n</code></pre>"},{"location":"configuration/backend/#modin","title":"Modin","text":"<p>Modin is an eagerly-executed distributed dataframe library that closely mirrors the behavior of Pandas. Because it behaves almost identically to Pandas but is able to distribute the dataset across the Ray cluster, there are fewer things to configure to optimize its performance.</p> <p>Support for Modin is currently experimental.</p> <p>Example:</p> <pre><code>backend:\ntype: ray\nprocessor:\ntype: modin\n</code></pre>"},{"location":"configuration/backend/#trainer","title":"Trainer","text":"<p>The <code>trainer</code> section configures distributed training strategy. Currently we support the following strategies (described in detail below) with more coming in the future:</p> <ul> <li>Horovod</li> <li>Distributed Data Parallel (DDP)</li> <li>Fully Sharded Data Parallel (FSDP)</li> </ul> <p>The following parameters can be configured for any distributed strategy:</p> <ul> <li><code>strategy</code>: one of <code>horovod</code>, <code>ddp</code>, or <code>fsdp</code>.</li> <li><code>use_gpu</code>: whether to use GPUs for training (defaults to <code>true</code> when the cluster has at least one GPU).</li> <li><code>num_workers</code>: how many Horovod workers to use for training (defaults to the number of GPUs, or 1 if no GPUs are found).</li> <li><code>resources_per_worker</code>: the Ray resources to assign to each Horovod worker (defaults to 1 CPU and 1 GPU if available).</li> <li><code>logdir</code>: path to the file directory where logs should be persisted.</li> <li><code>max_retries</code>: number of retries when Ray actors fail (defaults to 3).</li> </ul> <p>See the Ray Train API for more details on these parameters.</p> <p>Example:</p> <pre><code>backend:\ntype: ray\ntrainer:\nstrategy: horovod\nuse_gpu: true\nnum_workers: 4\nresources_per_worker:\nCPU: 2\nGPU: 1\n</code></pre> <p>In most cases, you shouldn't need to set these values explicitly, as Ludwig will set them on your behalf to maximize the resources available in the cluster. There are two cases in which it makes sense to set these values explicitly, however:</p> <ul> <li>Your Ray cluster makes use of autoscaling (in which case Ludwig will not know how many GPUs are available with scaling).</li> <li>Your Ray cluster is running multiple workloads at once (in which case you may wish to reserve resources for other jobs).</li> </ul>"},{"location":"configuration/backend/#horovod","title":"Horovod","text":"<p>Horovod is a distributed data-parallel framework that is optimized for bandwidth-constrained computing environments. It makes use of Nvidia's NCCL for fast GPU-to-GPU communication.</p> <p>Example:</p> <pre><code>backend:\ntype: ray\ntrainer:\nstrategy: horovod\n</code></pre>"},{"location":"configuration/backend/#distributed-data-parallel-ddp","title":"Distributed Data Parallel (DDP)","text":"<p>Distributed Data Parallel or DDP is PyTorch's native data-parallel library that functions very similarly to Horovod, but does not require installing any additional packages to use.</p> <p>In benchmarks, we found DDP and Horovod to perform near identically, so if you're not already using Horovod, DDP is the easiest way to get started with distributed training in Ludwig.</p> <pre><code>backend:\ntype: ray\ntrainer:\nstrategy: ddp\n</code></pre>"},{"location":"configuration/backend/#fully-sharded-data-parallel-fsdp","title":"Fully Sharded Data Parallel (FSDP)","text":"<p>Fully Sharded Data Parallel or FSDP is PyTorch's native data-parallel + model-parallel library for training very large models whose parameters are spread across multiple GPUs.</p> <p>The primary scenario to use FSDP is when the model you're training is too large to fit into a single GPU (e.g., fine-tuning a large language model like BLOOM)). When the model is small enough to fit in a single GPU, however, benchmarking has shown it's generally better to use Horovod or DDP.</p> <pre><code>backend:\ntype: ray\ntrainer:\nstrategy: fsdp\n</code></pre>"},{"location":"configuration/backend/#loader","title":"Loader","text":"<p>The <code>loader</code> section configures the \"last mile\" data ingest from processed data (typically cached in the Parquet format) to tensor batches used for training the model.</p> <p>When training a deep learning model at scale, the chain is only as strong as its weakest link -- in other words, the data loading pipeline needs to be at least as fast as the GPU forward / backward passes, otherwise the whole process will be bottlenecked by the data loader.</p> <p>In most cases, Ludwig's defaults will be sufficient to get good data loading performance, but if you notice GPU utilization dropping even after scaling the batch size, or going long periods at 0% utilization before spiking up again, you may need to tune the <code>loader</code> parameters below to improve throughput:</p> <ul> <li><code>fully_executed</code>: Force full evaluation of the preprocessed dataset by loading all blocks into cluster memory / storage (defaults to <code>true</code>). Disable this if the dataset is much larger than the total amount of cluster memory allocated to the Ray object store and you notice that object spilling is occurring frequently during training.</li> <li><code>window_size_bytes</code>: Load and shuffle the preprocessed dataset in discrete windows of this size (defaults to <code>null</code>, meaning data will not be windowed). Try configuring this is if shuffling is taking a very long time, indicated by every epoch of training taking many minutes to start. In general, larger window sizes result in more uniform shuffling (which can lead to better model performance in some cases), while smaller window sizes will be faster to load. This setting is particularly useful when running hyperopt over a large dataset.</li> </ul> <p>Example:</p> <pre><code>backend:\ntype: ray\nloader:\nfully_executed: false\nwindow_size_bytes: 500000000\n</code></pre>"},{"location":"configuration/combiner/","title":"Combiner","text":"<p>Combiners take the outputs of all input features encoders and combine them before providing the combined representation to the output feature decoders.</p> <p>You can specify which one to use in the <code>combiner</code> section of the configuration, and if you don't specify a combiner, the <code>concat</code> combiner will be used.</p>"},{"location":"configuration/combiner/#combiner-types","title":"Combiner Types","text":""},{"location":"configuration/combiner/#concat-combiner","title":"Concat Combiner","text":"<p>The <code>concat</code> combiner assumes all outputs from encoders are tensors of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the hidden dimension, which can be different for each input. If any inputs have more than 2 dimensions, a sequence or set feature for example, set the <code>flatten_inputs</code> parameter to <code>true</code>. It concatenates along the <code>h</code> dimension, and then (optionally) passes the concatenated tensor through a stack of fully connected layers. It returns the final <code>b x h'</code> tensor where <code>h'</code> is the size of the last fully connected layer or the sum of the sizes of the <code>h</code> of all inputs in the case there are no fully connected layers. If only a single input feature and no fully connected layer is specified, the output of the input feature encoder is passed through the combiner unchanged.</p> <pre><code>combiner:\ntype: concat\ndropout: 0.0\nnum_fc_layers: 0\noutput_size: 256\nnorm: null\nactivation: relu\nflatten_inputs: false\nresidual: false\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of stacked fully connected layers to apply. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>output_size</code> (default: <code>256</code>) : Output size of a fully connected layer.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>flatten_inputs</code> (default: <code>false</code>): Whether to flatten input tensors to a vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>residual</code> (default: <code>false</code>): Whether to add a residual connection to each fully connected layer block. Requires all fully connected layers to have the same <code>output_size</code>. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module. See Normalization for details.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> </ul>"},{"location":"configuration/combiner/#sequence-concat-combiner","title":"Sequence Concat Combiner","text":"<p>The <code>sequence_concat</code> combiner assumes at least one output from encoders is a tensors of size <code>b x s x h</code> where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>h</code> is the hidden dimension. A sequence-like (sequence, text or time series) input feature can be specified with the <code>main_sequence_feature</code> parameter which takes the name of sequence-like input feature as its value. If no <code>main_sequence_feature</code> is specified, the combiner will look through all the features in the order they are defined in the configuration and will look for a feature with a rank 3 tensor output (sequence, text or time series). If it cannot find one it will raise an exception, otherwise the output of that feature will be used for concatenating the other features along the sequence <code>s</code> dimension.</p> <p>If there are other input features with a rank 3 output tensor, the combiner will concatenate them alongside the <code>s</code> dimension, which means that all of them must have identical <code>s</code> dimension, otherwise a dimension mismatch error will be returned thrown during training when a datapoint with two sequential features of different lengths are provided.</p> <p>Other features that have a <code>b x h</code> rank 2 tensor output will be replicated <code>s</code> times and concatenated to the <code>s</code> dimension. The final output is a <code>b x s x h'</code> tensor where <code>h'</code> is the size of the concatenation of the <code>h</code> dimensions of all input features.</p> <pre><code>combiner:\ntype: sequence_concat\nmain_sequence_feature: null\nreduce_output: null\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>main_sequence_feature</code> (default: <code>null</code>) :  Name of a sequence, text, or time series feature to concatenate the outputs of the other features to. If no <code>main_sequence_feature</code> is specified, the combiner will look through all the features in the order they are defined in the configuration and will look for a feature with a rank 3 tensor output (sequence, text or time series). If it cannot find one it will raise an exception, otherwise the output of that feature will be used for concatenating the other features along the sequence <code>s</code> dimension. If there are other input features with a rank 3 output tensor, the combiner will concatenate them alongside the <code>s</code> dimension. All sequence-like input features must have identical <code>s</code> dimension, otherwise an error will be thrown.</p> </li> <li> <p><code>reduce_output</code> (default: <code>null</code>): Strategy to use to aggregate the embeddings of the items of the set. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> </ul>"},{"location":"configuration/combiner/#sequence-combiner","title":"Sequence Combiner","text":"<p>The <code>sequence</code> combiner stacks a sequence concat combiner with a sequence encoder. All the considerations about input tensor ranks described for the sequence concat combiner apply also in this case, but the main difference is that this combiner uses the <code>b x s x h'</code> output of the sequence concat combiner, where <code>b</code> is the batch size, <code>s</code> is the sequence length and <code>h'</code> is the sum of the hidden dimensions of all input features, as input for any of the sequence encoders described in the sequence features encoders section. Refer to that section for more detailed information about the sequence encoders and their parameters. All considerations on the shape of the outputs for the sequence encoders also apply to sequence combiner.</p> <pre><code>combiner:\ntype: sequence\nmain_sequence_feature: null\nreduce_output: null\nencoder:\ntype: parallel_cnn\nskip: false\ndropout: 0.0\nactivation: relu\nmax_sequence_length: null\nrepresentation: dense\nvocab: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nshould_embed: true\nembedding_size: 256\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\nreduce_output: sum\nnum_conv_layers: null\nconv_layers: null\nnum_filters: 256\nfilter_size: 3\npool_function: max\npool_size: null\noutput_size: 256\nnorm: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>main_sequence_feature</code> (default: <code>null</code>) :  Name of a sequence, text, or time series feature to concatenate the outputs of the other features to. If no <code>main_sequence_feature</code> is specified, the combiner will look through all the features in the order they are defined in the configuration and will look for a feature with a rank 3 tensor output (sequence, text or time series). If it cannot find one it will raise an exception, otherwise the output of that feature will be used for concatenating the other features along the sequence <code>s</code> dimension. If there are other input features with a rank 3 output tensor, the combiner will concatenate them alongside the <code>s</code> dimension. All sequence-like input features must have identical <code>s</code> dimension, otherwise an error will be thrown.</p> </li> <li> <p><code>reduce_output</code> (default: <code>null</code>): Strategy to use to aggregate the embeddings of the items of the set. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>encoder</code> (default: <code>{\"type\": \"parallel_cnn\"}</code>): Encoder to apply to <code>main_sequence_feature</code>.</li> </ul>"},{"location":"configuration/combiner/#tabnet-combiner","title":"TabNet Combiner","text":"<p>The <code>tabnet</code> combiner implements the TabNet model, which uses attention and sparsity to achieve high performance on tabular data. It assumes all outputs from encoders are tensors of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the hidden dimension, which can be different for each input. If the input tensors have a different shape, it automatically flattens them. It returns the final <code>b x h'</code> tensor where <code>h'</code> is the user-specified output size.</p> <pre><code>combiner:\ntype: tabnet\nsize: 32\ndropout: 0.05\noutput_size: 128\nnum_steps: 3\nnum_total_blocks: 4\nnum_shared_blocks: 2\nrelaxation_factor: 1.5\nbn_epsilon: 0.001\nbn_momentum: 0.05\nbn_virtual_bs: 1024\nsparsity: 0.0001\nentmax_mode: sparsemax\nentmax_alpha: 1.5\n</code></pre> <p>Parameters:</p> <ul> <li><code>size</code> (default: <code>32</code>) : Size of the hidden layers. <code>N_a</code> in (Arik and Pfister, 2019).</li> <li><code>dropout</code> (default: <code>0.05</code>) : Dropout rate for the transformer block.</li> <li><code>output_size</code> (default: <code>128</code>) : Output size of a fully connected layer. <code>N_d</code> in (Arik and Pfister, 2019).</li> <li><code>num_steps</code> (default: <code>3</code>): Number of steps / repetitions of the the attentive transformer and feature transformer computations. <code>N_steps</code> in (Arik and Pfister, 2019).</li> <li><code>num_total_blocks</code> (default: <code>4</code>): Total number of feature transformer blocks at each step.</li> <li><code>num_shared_blocks</code> (default: <code>2</code>): Number of shared feature transformer blocks across the steps.</li> <li><code>relaxation_factor</code> (default: <code>1.5</code>): Factor that influences how many times a feature should be used across the steps of computation. a value of 1 implies it each feature should be use once, a higher value allows for multiple usages. <code>gamma</code> in (Arik and Pfister, 2019).</li> <li><code>bn_epsilon</code> (default: <code>0.001</code>): Epsilon to be added to the batch norm denominator.</li> <li><code>bn_momentum</code> (default: <code>0.05</code>): Momentum of the batch norm. 1 - <code>m_B</code> from the TabNet paper.</li> <li><code>bn_virtual_bs</code> (default: <code>1024</code>): Size of the virtual batch size used by ghost batch norm. If null, regular batch norm is used instead. <code>B_v</code> from the TabNet paper. See Ghost Batch Normalization for details.</li> <li><code>sparsity</code> (default: <code>0.0001</code>): Multiplier of the sparsity inducing loss. <code>lambda_sparse</code> in (Arik and Pfister, 2019).</li> <li><code>entmax_mode</code> (default: <code>sparsemax</code>): Entmax is a sparse family of probability mapping which generalizes softmax and sparsemax. <code>entmax_mode</code> controls the sparsity Options: <code>entmax15</code>, <code>sparsemax</code>, <code>constant</code>, <code>adaptive</code>.</li> <li><code>entmax_alpha</code> (default: <code>1.5</code>): Must be a number between 1.0 and 2.0. If entmax_mode is <code>adaptive</code>, <code>entmax_alpha</code> is used as the initial value for the learnable parameter. 1 corresponds to softmax, 2 is sparsemax.</li> </ul>"},{"location":"configuration/combiner/#transformer-combiner","title":"Transformer Combiner","text":"<p>The <code>transformer</code> combiner combines input features using a stack of Transformer blocks (from Attention Is All You Need). It assumes all outputs from encoders are tensors of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the hidden dimension, which can be different for each input. If the input tensors have a different shape, it automatically flattens them. It then projects each input tensor to the same hidden / embedding size and encodes them with a stack of Transformer layers. Finally, the transformer combiner applies a reduction to the outputs of the Transformer stack, followed by optional fully connected layers. The output is a <code>b x h'</code> tensor where <code>h'</code> is the size of the last fully connected layer or the hidden / embedding size, or a <code>b x n x h'</code> where <code>n</code> is the number of input features and <code>h'</code> is the hidden / embedding size if no reduction is applied.</p> <p>Resources to learn more about transformers:</p> <ul> <li>CS480/680 Lecture 19: Attention and Transformer Networks (VIDEO)</li> <li>Attention is all you need - Attentional Neural Network Models Masterclass (VIDEO)</li> <li>Illustrated: Self-Attention (Colab notebook)</li> </ul> <pre><code>combiner:\ntype: transformer\ndropout: 0.1\nnum_fc_layers: 0\noutput_size: 256\nnorm: null\nfc_dropout: 0.0\ntransformer_output_size: 256\nhidden_size: 256\nnum_layers: 1\nnum_heads: 8\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\nfc_activation: relu\nfc_residual: false\nreduce_output: mean\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.1</code>) : Dropout rate for the transformer block.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : The number of stacked fully connected layers (only applies if <code>reduce_output</code> is not null).</li> <li><code>output_size</code> (default: <code>256</code>) : Output size of a fully connected layer.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>transformer_output_size</code> (default: <code>256</code>): Size of the fully connected layer after self attention in the transformer block. This is usually the same as <code>hidden_size</code> and <code>embedding_size</code>.</li> <li><code>hidden_size</code> (default: <code>256</code>): The number of hidden units of the TransformerStack as well as the dimension that each incoming input feature is projected to before feeding to the TransformerStack.</li> <li><code>num_layers</code> (default: <code>1</code>): The number of transformer layers.</li> <li><code>num_heads</code> (default: <code>8</code>): Number of heads of the self attention in the transformer block.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module. See Normalization for details.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_residual</code> (default: <code>false</code>): Whether to add a residual connection to each fully connected layer block. Requires all fully connected layers to have the same <code>output_size</code>. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>mean</code>): Strategy to use to aggregate the output of the transformer. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> </ul>"},{"location":"configuration/combiner/#tabtransformer-combiner","title":"TabTransformer Combiner","text":"<p>The <code>tabtransformer</code> combiner combines input features in the following sequence of operations. The combiner projects all encoder outputs except binary and number features into an embedding space. These features are concatenated as if they were a sequence and passed through a transformer. After the transformer, the number and binary features are concatenated (which are of size 1) and then concatenated  with the output of the transformer and is passed to a stack of fully connected layers (from TabTransformer: Tabular Data Modeling Using Contextual Embeddings). It assumes all outputs from encoders are tensors of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the hidden dimension, which can be different for each input. If the input tensors have a different shape, it automatically flattens them. It then projects each input tensor to the same hidden / embedding size and encodes them with a stack of Transformer layers. Finally, the transformer combiner applies a reduction to the outputs of the Transformer stack, followed by the above concatenation and optional fully connected layers. The output is a <code>b x h'</code> tensor where <code>h'</code> is the size of the last fully connected layer or the hidden / embedding size, or a <code>b x n x h'</code> where <code>n</code> is the number of input features and <code>h'</code> is the hidden / embedding size if no reduction is applied.</p> <pre><code>combiner:\ntype: tabtransformer\ndropout: 0.1\nnum_fc_layers: 0\noutput_size: 256\nnorm: null\nfc_dropout: 0.0\nembed_input_feature_name: null\ntransformer_output_size: 256\nhidden_size: 256\nnum_layers: 1\nnum_heads: 8\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\nfc_activation: relu\nfc_residual: false\nreduce_output: concat\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.1</code>) : Dropout rate for the transformer block.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : The number of stacked fully connected layers (only applies if <code>reduce_output</code> is not null).</li> <li><code>output_size</code> (default: <code>256</code>) : Output size of a fully connected layer.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embed_input_feature_name</code> (default: <code>null</code>) : This value controls the size of the embeddings. Valid values are <code>add</code> which uses the <code>hidden_size</code> value or an integer that is set to a specific value. In the case of an integer value, it must be smaller than hidden_size.</li> <li><code>transformer_output_size</code> (default: <code>256</code>): Size of the fully connected layer after self attention in the transformer block. This is usually the same as <code>hidden_size</code> and <code>embedding_size</code>.</li> <li><code>hidden_size</code> (default: <code>256</code>): The number of hidden units of the TransformerStack as well as the dimension that each incoming input feature is projected to before feeding to the TransformerStack.</li> <li><code>num_layers</code> (default: <code>1</code>): The number of transformer layers.</li> <li><code>num_heads</code> (default: <code>8</code>): Number of heads of the self attention in the transformer block.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module. See Normalization for details.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_residual</code> (default: <code>false</code>): Whether to add a residual connection to each fully connected layer block. Requires all fully connected layers to have the same <code>output_size</code>. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>concat</code>): Strategy to use to aggregate the output of the transformer. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> </ul>"},{"location":"configuration/combiner/#comparator-combiner","title":"Comparator Combiner","text":"<p>The <code>comparator</code> combiner compares the hidden representation of two entities defined by lists of features. It assumes all outputs from encoders are tensors of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the hidden dimension, which can be different for each input. If the input tensors have a different shape, it automatically flattens them. It then concatenates the representations of each entity and projects them both to vectors of size <code>output_size</code>. Finally, it compares the two entity representations by dot product, element-wise multiplication, absolute difference and bilinear product. It returns the final <code>b x h'</code> tensor where <code>h'</code> is the size of the concatenation of the four comparisons.</p> <pre><code>combiner:\ntype: comparator\nentity_1: null\nentity_2: null\ndropout: 0.0\nnum_fc_layers: 1\noutput_size: 256\nnorm: null\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>entity_1</code> (default: <code>null</code>) : The list of input feature names <code>[feature_1, feature_2, ...]</code> constituting the first entity to compare. Required.</li> <li><code>entity_2</code> (default: <code>null</code>) : The list of input feature names <code>[feature_1, feature_2, ...]</code> constituting the second entity to compare. Required.</li> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_fc_layers</code> (default: <code>1</code>) : Number of stacked fully connected layers to apply. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>output_size</code> (default: <code>256</code>) : Output size of a fully connected layer.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module. See Normalization for details.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> </ul>"},{"location":"configuration/combiner/#common-parameters","title":"Common Parameters","text":"<p>These parameters are used across multiple combiners (and some encoders / decoders) in similar ways.</p>"},{"location":"configuration/combiner/#normalization","title":"Normalization","text":"<p>Normalization applied at the beginnging of the fully-connected stack. If a <code>norm</code> is not already specified for the <code>fc_layers</code> this is the default <code>norm</code> that will be used for each layer. One of:</p> <ul> <li><code>null</code>: no normalization</li> <li><code>batch</code>: batch normalization</li> <li><code>layer</code>: layer normalization</li> <li><code>ghost</code>: ghost batch normalization</li> </ul>"},{"location":"configuration/combiner/#batch-normalization","title":"Batch Normalization","text":"<p>Applies Batch Normalization as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. See PyTorch documentation on batch normalization for more details.</p> <pre><code>norm: batch\nnorm_params:\neps: 0.001\nmomentum: 0.1\naffine: true\ntrack_running_stats: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>eps</code> (default: <code>0.001</code>): Epsilon to be added to the batch norm denominator.</li> <li><code>momentum</code> (default: <code>0.1</code>): The value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: <code>0.1</code>.</li> <li><code>affine</code> (default: <code>true</code>): A boolean value that when set to <code>true</code>, this module has learnable affine parameters.</li> <li><code>track_running_stats</code> (default: <code>true</code>): A boolean value that when set to <code>true</code>, this module tracks the running mean and variance, and when set to <code>false</code>, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as <code>null</code>. When these buffers are <code>null</code>, this module always uses batch statistics. in both training and eval modes.</li> </ul>"},{"location":"configuration/combiner/#layer-normalization","title":"Layer Normalization","text":"<p>Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization. See PyTorch documentation on layer normalization for more details.</p> <pre><code>norm: layer\nnorm_params:\neps: 0.00001\nelementwise_affine: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>eps</code> (default: <code>0.00001</code>): A value added to the denominator for numerical stability.</li> <li><code>elementwise_affine</code> (default: <code>true</code>): A boolean value that when set to <code>true</code>, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases)</li> </ul>"},{"location":"configuration/combiner/#ghost-batch-normalization","title":"Ghost Batch Normalization","text":"<p>Ghost Batch Norm is a technique designed to address the \"generalization gap\" whereby the training process breaks down with very large batch sizes. If you are using a large batch size (typically in the thousands) to maximize GPU utilization, but the model is not converging well, enabling ghost batch norm can be a useful technique to improve convergence.</p> <p>When using ghost batch norm, you specify a <code>virtual_batch_size</code> (default <code>128</code>) representing the \"ideal\" batch size to train with (ignoring throughput or GPU utilization). The ghost batch norm will then subdivide each batch into subbatches of size <code>virtual_batch_size</code> and apply batch normalization to each.</p> <p>A notable downside to ghost batch norm is that it is more computationally expensive than traditional batch norm, so it is only recommended to use it when the batch size that maximizes throughput is significantly higher than the batch size that yields the best convergence (one or more orders of magnitude higher).</p> <p>The approach was introduced in Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks and since popularized by its use in TabNet.</p> <pre><code>norm: ghost\nnorm_params:\nvirtual_batch_size: 128\nepsilon: 0.001\nmomentum: 0.05\n</code></pre> <p>Parameters:</p> <ul> <li><code>virtual_batch_size</code> (default: <code>128</code>): Size of the virtual batch size used by ghost batch norm. If null, regular batch norm is used instead. <code>B_v</code> from the TabNet paper.</li> <li><code>epsilon</code> (default: <code>0.001</code>): Epsilon to be added to the batch norm denominator.</li> <li><code>momentum</code> (default: <code>0.05</code>): Momentum of the batch norm. 1 - <code>m_B</code> from the TabNet paper.</li> </ul>"},{"location":"configuration/defaults/","title":"Defaults","text":"<p>The top-level <code>defaults</code> section specifies type-global:</p> <ol> <li>Preprocessing</li> <li>Encoder</li> <li>Decoder</li> <li>Loss</li> </ol> <p>Any configurations set in the <code>defaults</code> section apply to all features of that particular feature type. Any default <code>preprocessing</code> and <code>encoder</code> configurations will be applied to all input features of that feature type, while <code>decoder</code> and <code>loss</code> configurations will be applied to all output features of that feature type.</p> <p>These parameters can be set for individual features through the input feature configuration or output feature configuration.</p> <p>Note</p> <p>Feature-specific configurations override global defaults: When a parameter is defined for a specific feature and also modified via defaults, the feature specific configuration overrides the value set in the defaults section for that particular parameter.</p> <pre><code>input_features:\n- name: title\ntype: text\npreprocessing:\nmost_common: 10\n- name: summary\ntype: text\n....\ndefaults:\ntext:\npreprocessing:\nmost_common: 100\n</code></pre> <p>In the example config above, the <code>most_common</code> preprocessing value for <code>title</code> will be set to 10 instead of taking on the default value of 100, while <code>summary</code> will now have its <code>most_common</code> preprocessing value set to 100.</p>"},{"location":"configuration/defaults/#defining-defaults","title":"Defining Defaults","text":"<p>The defaults section of Ludwig has the following structure:</p> <pre><code>defaults:\n&lt;feature_type_1&gt;:\npreprocessing:\nparameter_1: value\n...\nencoder:\nparameter_1: value\n...\ndecoder:\nparameter_1: value\n...\nloss:\nparameter_1: value\n...\n&lt;feature_type_2&gt;:\npreprocessing:\nparameter_1: value\n...\nencoder:\nparameter_1: value\n...\ndecoder:\nparameter_1: value\n...\nloss:\nparameter_1: value\n...\n...\n</code></pre> <p>Each of the sections <code>preprocessing</code>, <code>encoder</code>, <code>decoder</code> and <code>loss</code> are optional so you can define one or more as you need.</p>"},{"location":"configuration/defaults/#type-global-preprocessing","title":"Type-Global Preprocessing","text":"<p>Specify preprocessing policies that apply globally across all input features of a certain data type. For example:</p> <pre><code>defaults:\ncategory:\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: &lt;UNK&gt;\n</code></pre> <p>The preprocessing parameters that each data type accepts can be found in datatype-specific documentation.</p> <p>Note that different features with the same datatype may require different preprocessing. Type-global preprocessing works in tandem with feature-specific preprocessing configuration parameters, however, feature-specific configurations override the global settings.</p> <p>For example, a document classification model may have two text input features, one for the title of the document and one for the body.</p> <p>As the length of the title is much shorter than the length of the body, the parameter <code>max_sequence_length</code> should be set to <code>10</code> for the title and <code>2000</code> for the body, but we want both features to share the same vocabulary, with <code>most_common: 10000</code>.</p> <p>The way to do this is by adding a <code>preprocessing</code> key inside the title <code>input_feature</code> dictionary and one in the <code>body</code> input feature dictionary containing the desired parameter and value.</p> <pre><code>input_features:\n-   name: title\ntype: text\npreprocessing:\nmax_sequence_length: 20\n-   name: body\ntype: text\npreprocessing:\nmax_sequence_length: 2000\ndefaults:\ntext:\npreprocessing:\nmost_common: 10000\n</code></pre>"},{"location":"configuration/defaults/#tokenizers","title":"Tokenizers","text":"<p>Sequence, text, and set features tokenize features as part of preprocessing. There are several tokenization options that can be specified:</p>"},{"location":"configuration/defaults/#basic-tokenizers","title":"Basic tokenizers","text":"<ul> <li><code>characters</code>: splits every character of the input string in a separate token.</li> <li><code>space</code>: splits on space characters using the regex <code>\\s+</code>.</li> <li><code>space_punct</code>: splits on space characters and punctuation using the regex <code>\\w+|[^\\w\\s]</code>.</li> <li><code>underscore</code>: splits on the underscore character <code>_</code>.</li> <li><code>comma</code>: splits on the comma character <code>,</code>.</li> <li><code>untokenized</code>: treats the whole string as a single token.</li> <li><code>stripped</code>: treats the whole string as a single token after removing spaces at the beginning and at the end of the string.</li> <li><code>ngram</code>: this will create a vocab that consists of unigrams and bigrams.</li> </ul>"},{"location":"configuration/defaults/#spacy-tokenizers","title":"spaCy tokenizers","text":"<p>The spaCy based tokenizers are functions that use the powerful tokenization and NLP preprocessing models provided the library. Several languages are available: English (code <code>en</code>), Italian (code <code>it</code>), Spanish (code <code>es</code>), German (code <code>de</code>), French (code <code>fr</code>), Portuguese (code <code>pt</code>), Dutch (code <code>nl</code>), Greek (code <code>el</code>), Chinese (code <code>zh</code>), Danish (code <code>da</code>), Dutch (code <code>el</code>), Japanese (code <code>ja</code>), Lithuanian (code <code>lt</code>), Norwegian (code <code>nb</code>), Polish (code <code>pl</code>), Romanian (code <code>ro</code>) and Multi (code <code>xx</code>, useful in case you have a dataset containing different languages).</p> <p>For each language different functions are available:</p> <ul> <li><code>tokenize</code>: uses spaCy tokenizer,</li> <li><code>tokenize_filter</code>: uses spaCy tokenizer and filters out punctuation, numbers, stopwords and words shorter than 3 characters,</li> <li><code>tokenize_remove_stopwords</code>: uses spaCy tokenizer and filters out stopwords,</li> <li><code>lemmatize</code>: uses spaCy lemmatizer,</li> <li><code>lemmatize_filter</code>: uses spaCy lemmatizer and filters out punctuation, numbers, stopwords and words shorter than 3 characters,</li> <li><code>lemmatize_remove_stopwords</code>: uses spaCy lemmatize and filters out stopwords.</li> </ul> <p>In order to use these options, you must first download the the spaCy model:</p> <pre><code>python -m spacy download &lt;language_code&gt;\n</code></pre> <p>and provide <code>&lt;language&gt;_&lt;function&gt;</code> as <code>tokenizer</code>. Example: <code>english_tokenizer</code>, <code>italian_lemmatize_filter</code>, <code>multi_tokenize_filter</code> and so on. More details on the models can be found in the spaCy documentation. Note that spaCy tokenizers are not TorchScript-compatible.</p>"},{"location":"configuration/defaults/#torchtext-and-huggingface-tokenizers","title":"TorchText and HuggingFace tokenizers","text":"<ul> <li><code>sentencepiece</code>: XLM-RoBERTa sentencepeice tokenizer.</li> <li><code>clip</code>: CLIP Tokenizer. Based on Byte-Level BPE.</li> <li><code>gpt2bpe</code>: GPT-2 BPE Tokenizer.</li> <li><code>bert</code>: BERT tokenizer based on the WordPiece algorithm.</li> <li><code>hf_tokenizer</code>: uses the Hugging Face AutoTokenizer which uses a <code>pretrained_model_name_or_path</code> parameter to decide which tokenizer to load.</li> </ul> <p>Note: when specifying one of these tokenizers in the config, preprocessing parameters <code>most_common</code> and <code>most_common_percentile</code> won't have an effect. The full vocab of these tokenizers will be used.</p>"},{"location":"configuration/defaults/#type-global-encoder","title":"Type-Global Encoder","text":"<p>Specify the encoder type and encoder related parameters across all input features of a certain data type. This encoder will be shared across all features of this particular feature type. For example:</p> <pre><code>defaults:\ntext:\nencoder:\ntype: stacked_cnn\nembedding_size: 128\nnum_filters: 512\n</code></pre> <p>Note</p> <p>The encoder <code>type</code> is a required parameter when defining a default encoder for a feature type or changing the default value for a parameter for the encoder, since the parameters are tied to specific encoders. Only one default encoder can be defined for all features of that particular type.</p> <p>The encoder types and parameters that each data type accepts can be found in datatype-specific documentation.</p>"},{"location":"configuration/defaults/#type-global-decoder","title":"Type-Global Decoder","text":"<p>Specify the decoder type and decoder related parameters across all output features of a certain data type. For example:</p> <pre><code>defaults:\ntext:\ndecoder:\ntype: generator\noutput_size: 128\nbias_initializer: he_normal\n</code></pre> <p>Note</p> <p>The decoder <code>type</code> is a required parameter when defining a default decoder for a feature type or changing the default value for a parameter for the decoder, since the parameters are tied to specific decoders. Only one default decoder can be defined for all features of that particular type.</p> <p>The decoder types and parameters that each data type accepts can be found in datatype-specific documentation.</p>"},{"location":"configuration/defaults/#type-global-loss","title":"Type-Global Loss","text":"<p>Specify the loss type and loss related parameters across all output features of a certain data type. For example:</p> <pre><code>defaults:\ntext:\nloss:\ntype: softmax_cross_entropy\nconfidence_penalty: 0.1\n</code></pre> <p>The loss types and parameters that each data type accepts can be found in datatype-specific documentation.</p>"},{"location":"configuration/defaults/#defaults-example","title":"Defaults Example","text":"<p>Following is a full example of a Ludwig configuration with type-global defaults.</p> config.yaml<pre><code>input_features:\n- name: title\ntype: text\n- name: body\ntype: text\n- name: num_characters\ntype: number\npreprocessing:\nnormalization: zscore\ncombiner:\ntype: concat\nnum_fc_layers: 1\noutput_features:\n- name: spam\ntype: category\ndefaults:\ntext:\npreprocessing:\nmost_common: 10000\nencoder:\ntype: rnn\ncell_type: lstm\nnum_layers: 2\ntraining:\nlearning_rate: 0.001\noptimizer:\ntype: adam\n</code></pre> <p>Example CLI command:</p> <pre><code>ludwig train --dataset spam.csv --config_str \"{input_features: [{name: title, type: text}, {name: body, type: text}, {name: num_characters, type: number, preprocessing: {normalization: zscore}}], output_features: [{name: spam, type: category}], combiner: {type: concat, num_fc_layers: 1}, defaults: {text: {preprocessing: {word_vocab_size: 10000}, encoder: {type: rnn, cell_type: lstm, num_layers: 2}}}, training: {learning_rate: 0.001, optimizer: {type: adam}}\"\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/","title":"Hyperopt","text":"<p>The <code>hyperopt</code> section of the Ludwig configuration defines what metrics to optimize for, which parameters to optimize, search strategy and execution strategy.</p> <pre><code>hyperopt:\ngoal: minimize\noutput_feature: combined\nmetric: loss\nsplit: validation\nparameters:\ntitle.encoder.cell_type: ... # title is a text feature type\ntitle.encoder.num_layers: ... combiner.num_fc_layers: ...\nsection.encoder.embedding_size: ...\npreprocessing.text.vocab_size: ...\ntrainer.learning_rate: ...\ntrainer.optimizer.type: ...\n...\nsearch_alg:\ntype: variant_generator  # random, hyperopt, bohb, ...\n# search_alg parameters...\nexecutor:\ntype: ray\nnum_samples: ...\nscheduler:\ntype: fifo  # hb_bohb, asynchyperband, ...\n# scheduler parameters...\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/#hyperopt-configuration-parameters","title":"Hyperopt configuration parameters","text":"<ul> <li><code>goal</code> which indicates if to minimize or maximize a metric or a loss of any of the output features on any of the dataset splits. Available values are: <code>minimize</code> (default) or <code>maximize</code>.</li> <li><code>output_feature</code> is a <code>str</code> containing the name of the output feature that we want to optimize the metric or loss of. Available values are <code>combined</code> (default) or the name of any output feature provided in the configuration. <code>combined</code> is a special output feature that allows to optimize for the aggregated loss and metrics of all output features.</li> <li><code>metric</code> is the metric that we want to optimize for. The default one is <code>loss</code>, but depending on the type of the feature defined in <code>output_feature</code>, different metrics and losses are available. Check the metrics section of the specific output feature type to figure out what metrics are available to use.</li> <li><code>split</code> is the split of data that we want to compute our metric on. By default it is the <code>validation</code> split, but you have the flexibility to specify also <code>train</code> or <code>test</code> splits.</li> <li><code>parameters</code> section consists of a set of hyperparameters to optimize. They are provided as keys (the names of the parameters) and values associated with them (that define the search space). The values vary depending on the type of the hyperparameter. Syntax for this section is based on Ray Tune's Search Space parameters.</li> <li><code>search_alg</code> section specifies the algorithm to sample the defined <code>parameters</code> space. Candidate algorithms are those found in Ray Tune's Search Algorithms.</li> <li><code>executor</code> section specifies how to execute the hyperparameter optimization. The execution could happen locally in a serial manner or in parallel across multiple workers and with GPUs as well if available.  The <code>executor</code> section includes specification for work scheduling and the number of samples to generate.</li> </ul>"},{"location":"configuration/hyperparameter_optimization/#defining-hyperparameter-search-spaces","title":"Defining hyperparameter search spaces","text":"<p>In the <code>parameters</code> section, hyperparameters are dot( <code>.</code>) separate names. The parts of the hyperparameter names separated by <code>.</code> are references to nested sections in the Ludwig configuration. For instance, to reference the <code>learning_rate</code>, in the <code>trainer</code> section one would use the name <code>trainer.learning_rate</code>. If the parameter to reference is inside an input or output feature, the name of that feature will be used as starting point. For instance, for referencing the <code>cell_type</code> of the <code>encoder</code> for the <code>title</code> feature, use the name <code>title.encoder.cell_type</code>.</p>"},{"location":"configuration/hyperparameter_optimization/#numeric-hyperparameters","title":"Numeric Hyperparameters","text":"<ul> <li><code>space</code>: Use Ray Tune's Search Space types, e.g., <code>uniform</code>, <code>quniform</code>, <code>loguniform</code>, <code>choice</code>, etc.  Refer the cited page for details.</li> </ul> <p>For numeric <code>spaces</code>, these define the range where the value is generated</p> <ul> <li><code>lower</code>: the minimum value the parameter can have</li> <li><code>upper</code>: the maximum value the parameter can have</li> <li><code>q</code>: quantization number, used in <code>spaces</code> such as <code>quniform</code>, <code>qloguniform</code>, <code>qrandn</code>, <code>qrandint</code>, <code>qlograndint</code></li> <li><code>base</code>: defines the base of the log for <code>loguniform</code>, <code>qloguniform</code>, <code>lograndint</code> and <code>qlograndint</code></li> </ul> <p>Note</p> <p>Depending on the specific numeric <code>space</code>, the <code>upper</code> parameter may be inclusive or excluse.  Refer to the Ray Tune documentation for the specific distribution for details.</p> <p>Float example: Uniform floating point random values (in log space) between 0.001 and 0.1</p> <pre><code>trainer.learning_rate:\nspace: loguniform\nlower: 0.001\nupper: 0.1\n</code></pre> <p>Integer example: Uniform random integer values 1, 2, 3</p> <pre><code>combiner.num_fc_layers:\nspace: randint\nlower: 1\nupper: 4\n</code></pre> <p>Quantized Example: Uniform random floating point values such a 0, 0.1, 0.2, ..., 0.9</p> <pre><code>my_output_feature.decoder.dropout:\nspace: quniform\nlower: 0\nupper: 1\nq: 0.1\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/#categorical-hyperparameters","title":"Categorical Hyperparameters","text":"<ul> <li><code>space</code>: Use <code>choice</code>.</li> <li><code>categories</code>: a list of possible values. The type of each value of the list is general, i.e., they could be strings, integers, floats and anything else, even entire dictionaries.  The values will be a uniform random selection.</li> </ul> <p>Example:</p> <pre><code>title.encoder.cell_type:\nspace: choice\ncategories: [rnn, gru, lstm]\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/#hyperparameters-in-a-grid","title":"Hyperparameters in a Grid","text":"<p>For <code>space</code>: <code>grid_search</code></p> <ul> <li><code>values</code>: is a list of values to use in creating the grid search space.  The type of each value of the list is general, i.e., they could be strings, integers, floats and anything else, even entire dictionaries.</li> </ul> <p>Example:</p> <pre><code>title.encoder.cell_type:\nspace: grid_search\nvalues: [rnn, gru, lstm]\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/#more-comprehensive-example","title":"More comprehensive example","text":"<pre><code>hyperopt:\nparameters:\ntrainer.learning_rate:\nspace: loguniform\nlower: 0.001\nupper: 0.1\ncombiner.num_fc_layers:\nspace: randint\nlower: 2\nupper: 6\ntitle.encoder.cell_type:\nspace: grid_search\nvalues: [\"rnn\", \"gru\"]\ntitle.encoder.bidirectional:\nspace: choice\ncategories: [True, False]\ntitle.encoder.fc_layers:\nspace: choice\ncategories:\n- [{\"output_size\": 512}, {\"output_size\": 256}]\n- [{\"output_size\": 512}]\n- [{\"output_size\": 256}]\n</code></pre>"},{"location":"configuration/hyperparameter_optimization/#default-hyperopt-parameters","title":"Default Hyperopt Parameters","text":"<p>In addition to defining hyperopt parameters for individual input or output features (like the <code>title</code> feature in the example above), default parameters can be specified for entire feature types (for example, the encoder to use for all text features in your dataset). Read more about default hyperopt parameters here.</p>"},{"location":"configuration/hyperparameter_optimization/#nested-ludwig-config-parameters","title":"Nested Ludwig Config Parameters","text":"<p>Ludwig also allows partial or full Ludwig configs to be sampled from the hyperopt search space. Read more about nested Ludwig config parameters here.</p>"},{"location":"configuration/hyperparameter_optimization/#search-algorithm","title":"Search Algorithm","text":"<p>Ray Tune supports its own collection of search algorithms, specified by the <code>search_alg</code> section of the hyperopt config:</p> <pre><code>search_alg:\ntype: variant_generator\n</code></pre> <p>You can find the full list of supported search algorithm names in Ray Tune's create_searcher function. Please note these algorithms require installation of additional packages.  As of this version of Ludwig, Ludwig installs the packages for the search algorithm <code>hyperopt</code>.  For all other search algorithms, the user is expected to install the required packages.</p>"},{"location":"configuration/hyperparameter_optimization/#executor","title":"Executor","text":""},{"location":"configuration/hyperparameter_optimization/#ray-tune-executor","title":"Ray Tune Executor","text":"<p>The <code>ray</code> executor is used to enable Ray Tune for both local and distributed hyperopt across a cluster of machines.</p> <p>Parameters:</p> <ul> <li> <p><code>num_samples</code>: This parameter, along with the <code>space</code> specifications in the <code>parameters</code> section, controls how many trials are generated (default: 1).</p> <p>Note</p> <ul> <li>If all the hyperparameters in the <code>parameters</code> section have non-<code>grid_search</code> specifications (e.g., <code>uniform</code>, <code>randn</code>, <code>choice</code>, etc.), then the number of trials will be <code>num_samples</code>.</li> <li>If all the hyperparameters have <code>grid_search</code>, then the number of trials will be the product of the number of values specified for each hyperparameter.  In this case, <code>num_samples</code> should be set to 1.  For example, if there are three <code>grid_search</code> hyperparameters, with 2, 4 and 4 values, respectively.  The number of trials will be 2 X 4 X 4 = 32, where each trial is a unique combination of the three <code>grid_search</code> hyperparameter values.</li> <li>If there is a mixture of <code>grid_search</code> and non-<code>grid_search</code> spaces, the number of trials will be product of the number of values specified for each <code>grid_search</code> hyperpameter multiplied by the value of <code>num_samples</code>.  To illustrate this point, we take the three <code>grid_search</code> hyperparameters described in the preceding bullet item and add 2 hyperparameters with <code>uniform</code> and <code>randint</code> spaces.  With <code>num_samples = 10</code>, for each unique combination of values from the <code>grid_search</code> hyperparameters, 10 trials will be generated with random values selected for the <code>uniform</code> and <code>randint</code> hyperparameters.  This will lead to a total of 32 X 10 = 320 trials.</li> </ul> </li> <li> <p><code>cpu_resources_per_trial</code>: The number of CPU cores allocated to each trial (default: 1).</p> </li> <li><code>gpu_resources_per_trial</code>: The number of GPU devices allocated to each trial (default: 0).</li> <li><code>kubernetes_namespace</code>: When running on Kubernetes, provide the namespace of the Ray cluster to sync results between pods. See the Ray docs for more info.</li> <li><code>time_budget_s</code>: The number of seconds for the entire hyperopt run.</li> <li> <p><code>max_concurrent_trials</code>: The maximum number of trials to train concurrently. Defaults to <code>auto</code> if not specified.</p> <p>Note</p> <ul> <li>If you're using a Ray backend, <code>auto</code> will infer the max_concurrent_trials that can be set given your cluster configuration to prevent trials from stalling.</li> <li>If you're using a Local or Horovod backend, <code>auto</code> will set max_concurrent_trials to None.</li> </ul> </li> </ul>"},{"location":"configuration/hyperparameter_optimization/#scheduler","title":"Scheduler","text":"<p>Ray Tune also allows you to specify a scheduler to support features like early stopping and other population-based strategies that may pause and resume trials during trainer. Ludwig exposes the complete scheduler API in the <code>scheduler</code> section of the <code>executor</code> config.</p> <p>You can find the full list of supported schedulers in Ray Tune's create_scheduler function.</p> <p>Example:</p> <pre><code>executor:\ntype: ray\ncpu_resources_per_trial: 2\ngpu_resources_per_trial: 1\nkubernetes_namespace: ray\ntime_budget_s: 7200\nscheduler:\ntype: async_hyperband\ntime_attr: training_iteration\nreduction_factor: 4\n</code></pre> <p>Running Ray Executor:</p> <p>See the section on Running Ludwig with Ray for guidance on setting up your Ray cluster.</p>"},{"location":"configuration/hyperparameter_optimization/#full-hyperparameter-optimization-example","title":"Full hyperparameter optimization example","text":"<p>Following is a full example of a Ludwig configuration with hyperparameter optimization.</p> <p>Example YAML:</p> <pre><code>input_features:\n-\nname: title\ntype: text\nencoder: type: rnn\ncell_type: lstm\nnum_layers: 2\ncombiner:\ntype: concat\nnum_fc_layers: 1\noutput_features:\n-\nname: class\ntype: category\ndefaults:\ntext:\npreprocessing:\nword_vocab_size: 10000\ntraining:\nlearning_rate: 0.001\noptimizer:\ntype: adam\nhyperopt:\ngoal: maximize\noutput_feature: class\nmetric: accuracy\nsplit: validation\nparameters:\ntrainer.learning_rate:\nspace: loguniform\nlower: 0.0001\nupper: 0.1\ntrainer.optimizer.type:\nspace: choice\ncategories: [sgd, adam, adagrad]\npreprocessing.text.word_vocab_size:\nspace: qrandint\nlower: 700\nupper: 1200\nq: 5\ncombiner.num_fc_layers:\nspace: randint\nlower: 1\nupper: 5\ntitle.encoder.cell_type:\nspace: choice\nvalues: [rnn, gru, lstm]\nsearch_alg:\ntype: random\nexecutor:\ntype: ray\nnum_samples: 12\n</code></pre> <p>Example CLI command:</p> <pre><code>ludwig hyperopt --dataset reuters-allcats.csv --config_str \"{input_features: [{name: title, type: text, encoder: {type: rnn, cell_type: lstm, num_layers: 2}}], output_features: [{name: class, type: category}], training: {learning_rate: 0.001}, hyperopt: {goal: maximize, output_feature: class, metric: accuracy, split: validation, parameters: {trainer.learning_rate: {space: loguniform, lower: 0.0001, upper: 0.1}, title.encoder.cell_type: {space: choice, categories: [rnn, gru, lstm]}}, search_alg: {type: variant_generator},executor: {type: ray, num_samples: 10}}}\"\n</code></pre>"},{"location":"configuration/model_type/","title":"Model Types","text":"<p>The top-level <code>model_type</code> parameter specifies the type of model to use.</p> <p>The following model types are supported:</p> <ul> <li><code>ecd</code> (default): Encoder-Combiner-Decoder neural network model.</li> <li><code>gbm</code>: Gradient Boosting Machine tree-based model.</li> </ul> <pre><code>model_type: ecd\n</code></pre> <p>Every model type has trainers associated with it. See the Trainer section for details about the supported training algorithms per model type.</p>"},{"location":"configuration/model_type/#model-type-ecd","title":"Model Type: ECD","text":"<p>See the ECD documentation for details about the Encoder-Combiner-Decoder deep learning architecture.</p> <p>Check</p> <p>The full breadth of Ludwig functionality is available for the <code>ecd</code> model type.</p>"},{"location":"configuration/model_type/#model-type-gbm","title":"Model Type: GBM","text":"<p>The GBM model type is a gradient boosting machine (GBM) tree model. It is a tree model that is trained using a supported tree learner. Currently, the only supported tree learner is LightGBM.</p> <p>Attention</p> <p>Selecting the <code>gbm</code> model type introduces the following limitations:</p> <ul> <li>only binary, category, and number features are supported</li> <li>only a single output feature is supported</li> <li>the <code>combiner</code> section is ignored</li> </ul>"},{"location":"configuration/preprocessing/","title":"Preprocessing","text":"<p>The top-level <code>preprocessing</code> section specifies dataset splitting (train, validation, test), sample ratio (undersampling the minority class or oversampling the majority class) and dataset balancing.</p> <pre><code>preprocessing:\nsample_ratio: 1.0\noversample_minority: null\nundersample_majority: null\nsplit:\ntype: random\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\n</code></pre>"},{"location":"configuration/preprocessing/#dataset-splitting","title":"Dataset Splitting","text":"<p>Data splitting is an important aspect of machine learning to train and evaluate machine learning models.</p> <p>Ludwig supports splitting data into train, validation, and test sets, and this is configured using the top-level <code>preprocessing</code> section of the Ludwig config.</p> <p>There is no set guideline or metric for how the data should be split; it may depend on the size of your data or the type of problem.</p> <p>There are a few different methods that Ludwig uses to split data. Each of these methods is specified under the <code>split</code> subsection.</p> <p>The following splitting methods are currently supported by Ludwig:</p>"},{"location":"configuration/preprocessing/#random-split","title":"Random Split","text":"<p>By default, Ludwig will randomly split the data into train, validation, and test sets according to split probabilities, which by default are: <code>[0.7, 0.1, 0.2]</code>.</p> <pre><code>split:\ntype: random\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\n</code></pre> <p>However, you can specify different splitting probabilities if you'd like by setting the probabilities for each of the 3 datasets (so that they sum up to 1)</p>"},{"location":"configuration/preprocessing/#fixed-split","title":"Fixed Split","text":"<p>If you have a column denoting pre-defined splits (train, validation and test) that you want to use across experiments, Ludwig supports using fixed dataset splits.</p> <p>The following config is an example that would perform fixed splitting using a column named <code>split</code> in the dataset:</p> <pre><code>split:\ntype: fixed\ncolumn: split\n</code></pre> <p>Within the data itself, we would ensure that there is a column called <code>split</code> with the following values for each row in the column based on the split we want to map that row to:</p> <ul> <li><code>0</code>: train</li> <li><code>1</code>: validation</li> <li><code>2</code>: test</li> </ul> <p>Note</p> <p>Your dataset must contain a train split. However, the validation and test splits are encouraged, but optional.</p>"},{"location":"configuration/preprocessing/#stratified-split","title":"Stratified Split","text":"<p>Sometimes you may want to split your data according to a particular column's distribution to maintain the same representation of this distribution across all your data subsets. This may be particularly useful when you have more than one class and your dataset is imbalanced.</p> <p>In order to perform stratified splitting, you specify the name of the column you want to perform stratified splitting on and the split probabilities.</p> <p>The following config is an example that would perform stratified splitting for a column <code>color</code>:</p> <pre><code>split:\ntype: stratify\ncolumn: color\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\n</code></pre> <p>This helps ensure that the distribution of the values in <code>color</code> are roughly the same across data subsets.</p> <p>Note</p> <p>This split method is only supported with a local Pandas backend. We are actively working on including support for other data sources like Dask.</p>"},{"location":"configuration/preprocessing/#datetime-split","title":"Datetime Split","text":"<p>Another common use case is splitting a column according to a datetime column where you may want to have the data split in a temporal order.</p> <p>This is useful for situations like backtesting where a user wants to make sure that a model trained on historical data would have performed well on unseen future data.</p> <p>If we were to use a uniformly random split strategy in these cases, then the model may not generalize well if the data distribution is subject to change over time. Splitting the training from the test data along the time dimension is one way to avoid this false sense of confidence, by showing how well the model should do on unseen data from the future.</p> <p>For datetime-based splitting, we order the data by date (ascending) and then split according to the <code>split_probabilties</code>. For example, if <code>split_probabilities: [0.7, 0.1, 0.2]</code>, then the earliest 70% of the data will be used for training, the middle 10% used for validation, and the last 20% used for testing.</p> <p>The following config shows how to specify this type of splitting using a datetime column named <code>created_ts</code>:</p> <pre><code>split:\ntype: datetime\ncolumn: created_ts\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\n</code></pre>"},{"location":"configuration/preprocessing/#hash-split","title":"Hash Split","text":"<p>Hash splitting deterministically assigns each sample to a split based on a hash of a provided \"key\" column. This is a useful alternative to random splitting when such a key is available for a couple of reasons:</p> <ul> <li>To prevent data leakage: For example, imagine you are predicting which users are likely to churn in a given month. If a user appears in both the train and test splits, then it may seem that your model is generalizing better than it actually is. In these cases, hashing on the user ID column will ensure that every sample for a user is assigned to the same split.</li> <li>To ensure consistent assignment of samples to splits as the underlying dataset evolves over time: Though random splitting is determinstic between runs due to the use of a random seed, if the underlying dataset changes (e.g., new samples are added over time), then samples may move into different splits. Hashing on a primary key will ensure that all existing samples retain their original splits as new samples are added over time.</li> </ul> <pre><code>split:\ntype: hash\ncolumn: user_id\nprobabilities:\n- 0.7\n- 0.1\n- 0.2\n</code></pre>"},{"location":"configuration/preprocessing/#data-balancing","title":"Data Balancing","text":"<p>Users working with imbalanced datasets can specify an oversampling or undersampling parameter which will balance the data during preprocessing.</p> <p>Warning</p> <p>Dataset balancing is only supported for binary output features currently. We are working to add category support in a future release.</p> <p>Note</p> <p>Specifying both oversampling and undersampling parameters simultaneously is not supported.</p>"},{"location":"configuration/preprocessing/#oversampling","title":"Oversampling","text":"<p>In this example, Ludwig will oversample the minority class to achieve a 50% representation in the overall dataset.</p> <pre><code>preprocessing:\noversample_minority: 0.5\n</code></pre>"},{"location":"configuration/preprocessing/#undersampling","title":"Undersampling","text":"<p>In this example, Ludwig will undersample the majority class to achieve a 70% representation in the overall dataset.</p> <pre><code>preprocessing:\nundersample_majority: 0.7\n</code></pre>"},{"location":"configuration/preprocessing/#sample-ratio","title":"Sample Ratio","text":"<p>Sometimes users may want to train on a sample of their input training data (maybe there's too much, and we only need 20%, or we want to try out ideas on a smaller subset of our data). In order to achieve this, a user can specify a <code>sample_ratio</code> to indicate the ratio of the dataset to use for training.</p> <p>By default, the sample ratio is 1.0, so if not specified, all the data will be used for training. For example, if you only want to use 30% of my input data, you could specify a config like this:</p> <pre><code>preprocessing:\nsample_ratio: 0.3\n</code></pre>"},{"location":"configuration/preprocessing/#feature-specific-preprocessing","title":"Feature-specific preprocessing","text":"<p>To configure feature-specific preprocessing, please check datatype-specific documentation.</p>"},{"location":"configuration/trainer/","title":"Trainer","text":""},{"location":"configuration/trainer/#overview","title":"Overview","text":"<p>The <code>trainer</code> section of the configuration lets you specify parameters that configure the training process, like the number of epochs or the learning rate. By default, the ECD trainer is used.</p> ECDGBM <pre><code>trainer:\nlearning_rate: 0.001\nepochs: 100\nbatch_size: auto\nearly_stop: 5\noptimizer:\ntype: adam\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\namsgrad: false\nregularization_type: l2\nuse_mixed_precision: false\ncheckpoints_per_epoch: 0\nregularization_lambda: 0.0\ntrain_steps: null\nsteps_per_checkpoint: 0\nmax_batch_size: 1099511627776\neval_batch_size: null\nevaluate_training_set: false\nvalidation_field: null\nvalidation_metric: null\nshould_shuffle: true\nincrease_batch_size_on_plateau: 0\nincrease_batch_size_on_plateau_patience: 5\nincrease_batch_size_on_plateau_rate: 2.0\nincrease_batch_size_eval_metric: loss\nincrease_batch_size_eval_split: training\ngradient_clipping:\nclipglobalnorm: 0.5\nclipnorm: null\nclipvalue: null\nlearning_rate_scaling: linear\nbucketing_field: null\nlearning_rate_scheduler:\ndecay: null\ndecay_rate: 0.96\ndecay_steps: 10000\nstaircase: false\nreduce_on_plateau: 0\nreduce_on_plateau_patience: 10\nreduce_on_plateau_rate: 0.1\nwarmup_evaluations: 0\nwarmup_fraction: 0.0\nreduce_eval_metric: loss\nreduce_eval_split: training\n</code></pre> <pre><code>trainer:\nlearning_rate: 0.03\nearly_stop: 5\nmax_depth: 18\nboosting_type: gbdt\nbagging_fraction: 0.8\nfeature_fraction: 0.75\nextra_trees: false\nlambda_l1: 0.25\nlambda_l2: 0.2\ndrop_rate: 0.1\ntree_learner: serial\nboosting_rounds_per_checkpoint: 50\nnum_boost_round: 1000\nnum_leaves: 82\nmin_data_in_leaf: 20\npos_bagging_fraction: 1.0\nneg_bagging_fraction: 1.0\nbagging_freq: 1\nbagging_seed: 3\nfeature_fraction_bynode: 1.0\nfeature_fraction_seed: 2\nextra_seed: 6\nlinear_lambda: 0.0\nmax_drop: 50\nskip_drop: 0.5\nuniform_drop: false\ndrop_seed: 4\neval_batch_size: 1048576\nevaluate_training_set: false\nvalidation_field: null\nvalidation_metric: null\nmin_sum_hessian_in_leaf: 0.001\nmax_delta_step: 0.0\nmin_gain_to_split: 0.03\nxgboost_dart_mode: false\ntop_rate: 0.2\nother_rate: 0.1\nmin_data_per_group: 100\nmax_cat_threshold: 32\ncat_l2: 10.0\ncat_smooth: 10.0\nmax_cat_to_onehot: 4\ncegb_tradeoff: 1.0\ncegb_penalty_split: 0.0\npath_smooth: 0.0\nverbose: -1\nmax_bin: 255\nfeature_pre_filter: true\n</code></pre>"},{"location":"configuration/trainer/#trainer-parameters","title":"Trainer parameters","text":"ECDGBM <ul> <li><code>learning_rate</code> (default: <code>0.001</code>) : Controls how much to change the model in response to the estimated error each time the model weights are updated. If 'auto', the optimal learning rate is estimated by choosing the learning rate that produces the smallest non-diverging gradient update.</li> <li><code>epochs</code> (default: <code>100</code>) : Number of epochs the algorithm is intended to be run over. Overridden if <code>train_steps</code> is set</li> <li><code>batch_size</code> (default: <code>auto</code>) : The number of training examples utilized in one training step of the model. If \u2019auto\u2019, the batch size that maximized training throughput (samples / sec) will be used. For CPU training, the tuned batch size is capped at 128 as throughput benefits of large batch sizes are less noticeable without a GPU.</li> <li><code>early_stop</code> (default: <code>5</code>) : Number of consecutive rounds of evaluation without any improvement on the <code>validation_metric</code> that triggers training to stop. Can be set to -1, which disables early stopping entirely.</li> <li><code>optimizer</code> (default: <code>{\"type\": \"adam\"}</code>) : Optimizer type and its parameters. The optimizer is responsble for applying the gradients computed from the loss during backpropagation as updates to the model weights. See Optimizer parameters for details.</li> <li><code>regularization_type</code> (default: <code>l2</code>) : Type of regularization. Options: <code>l1</code>, <code>l2</code>, <code>l1_l2</code>, <code>null</code>.</li> <li><code>use_mixed_precision</code> (default: <code>false</code>) : Enable automatic mixed-precision (AMP) during training. Options: <code>true</code>, <code>false</code>.</li> <li><code>checkpoints_per_epoch</code> (default: <code>0</code>): Number of checkpoints per epoch. For example, 2 -&gt; checkpoints are written every half of an epoch. Note that it is invalid to specify both non-zero <code>steps_per_checkpoint</code> and non-zero <code>checkpoints_per_epoch</code>.</li> <li><code>regularization_lambda</code> (default: <code>0.0</code>): Strength of the regularization.</li> <li><code>train_steps</code> (default: <code>null</code>): Maximum number of training steps the algorithm is intended to be run over. Unset by default. If set, will override <code>epochs</code> and if left unset then <code>epochs</code> is used to determine training length.</li> <li><code>steps_per_checkpoint</code> (default: <code>0</code>): How often the model is checkpointed. Also dictates maximum evaluation frequency. If 0 the model is checkpointed after every epoch.</li> <li><code>max_batch_size</code> (default: <code>1099511627776</code>): Auto batch size tuning and increasing batch size on plateau will be capped at this value. The default value is 2^40.</li> <li><code>eval_batch_size</code> (default: <code>null</code>): Size of batch to pass to the model for evaluation. If it is <code>0</code> or <code>None</code>, the same value of <code>batch_size</code> is used. This is useful to speedup evaluation with a much bigger batch size than training, if enough memory is available. If \u2019auto\u2019, the biggest batch size (power of 2) that can fit in memory will be used.</li> <li><code>evaluate_training_set</code> (default: <code>false</code>): Whether to evaluate on the entire training set during evaluation. By default, training metrics will be computed at the end of each training step, and accumulated up to the evaluation phase. In practice, computing training set metrics during training is up to 30% faster than running a separate evaluation pass over the training set, but results in more noisy training metrics, particularly during the earlier epochs. It's recommended to only set this to True if you need very exact training set metrics, and are willing to pay a significant performance penalty for them. Options: <code>true</code>, <code>false</code>.</li> <li><code>validation_field</code> (default: <code>null</code>): The field for which the <code>validation_metric</code> is used for validation-related mechanics like early stopping, parameter change plateaus, as well as what hyperparameter optimization uses to determine the best trial. If unset (default), the first output feature is used. If explicitly specified, neither <code>validation_field</code> nor <code>validation_metric</code> are overwritten.</li> <li><code>validation_metric</code> (default: <code>null</code>): Metric from <code>validation_field</code> that is used. If validation_field is not explicitly specified, this is overwritten to be the first output feature type's <code>default_validation_metric</code>, consistent with validation_field. If the validation_metric is specified, then we will use the first output feature that produces this metric as the <code>validation_field</code>.</li> <li><code>should_shuffle</code> (default: <code>true</code>): Whether to shuffle batches during training when true. Options: <code>true</code>, <code>false</code>.</li> <li><code>increase_batch_size_on_plateau</code> (default: <code>0</code>): The number of times to increase the batch size on a plateau.</li> <li><code>increase_batch_size_on_plateau_patience</code> (default: <code>5</code>): How many epochs to wait for before increasing the batch size.</li> <li><code>increase_batch_size_on_plateau_rate</code> (default: <code>2.0</code>): Rate at which the batch size increases.</li> <li><code>increase_batch_size_eval_metric</code> (default: <code>loss</code>): Which metric to listen on for increasing the batch size.</li> <li><code>increase_batch_size_eval_split</code> (default: <code>training</code>): Which dataset split to listen on for increasing the batch size.</li> <li><code>gradient_clipping</code> : Parameter values for gradient clipping.</li> <li><code>gradient_clipping.clipglobalnorm</code> (default: <code>0.5</code>): Maximum allowed norm of the gradients</li> <li><code>gradient_clipping.clipnorm</code> (default: <code>null</code>): Maximum allowed norm of the gradients</li> <li><code>gradient_clipping.clipvalue</code> (default: <code>null</code>): Maximum allowed value of the gradients</li> <li><code>learning_rate_scaling</code> (default: <code>linear</code>): Scale by which to increase the learning rate as the number of distributed workers increases. Traditionally the learning rate is scaled linearly with the number of workers to reflect the proportion by which the effective batch size is increased. For very large batch sizes, a softer square-root scale can sometimes lead to better model performance. If the learning rate is hand-tuned for a given number of workers, setting this value to constant can be used to disable scale-up. Options: <code>constant</code>, <code>sqrt</code>, <code>linear</code>.</li> <li><code>bucketing_field</code> (default: <code>null</code>): Feature to use for bucketing datapoints</li> <li><code>learning_rate_scheduler</code> : Parameter values for learning rate scheduler.</li> <li><code>learning_rate_scheduler.decay</code> (default: <code>null</code>) : Turn on decay of the learning rate. Options: <code>linear</code>, <code>exponential</code>, <code>null</code>.</li> <li><code>learning_rate_scheduler.decay_rate</code> (default: <code>0.96</code>): Decay per epoch (%): Factor to decrease the Learning rate.</li> <li><code>learning_rate_scheduler.decay_steps</code> (default: <code>10000</code>): The number of steps to take in the exponential learning rate decay.</li> <li><code>learning_rate_scheduler.staircase</code> (default: <code>false</code>): Decays the learning rate at discrete intervals. Options: <code>true</code>, <code>false</code>.</li> <li><code>learning_rate_scheduler.reduce_on_plateau</code> (default: <code>0</code>) : How many times to reduce the learning rate when the algorithm hits a plateau (i.e. the performance on thetraining set does not improve</li> <li><code>learning_rate_scheduler.reduce_on_plateau_patience</code> (default: <code>10</code>): How many evaluation steps have to pass before the learning rate reduces when <code>reduce_on_plateau &gt; 0</code>.</li> <li><code>learning_rate_scheduler.reduce_on_plateau_rate</code> (default: <code>0.1</code>): Rate at which we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li> <li><code>learning_rate_scheduler.warmup_evaluations</code> (default: <code>0</code>): Number of evaluation steps to warmup the learning rate for.</li> <li><code>learning_rate_scheduler.warmup_fraction</code> (default: <code>0.0</code>): Fraction of total training steps to warmup the learning rate for.</li> <li><code>learning_rate_scheduler.reduce_eval_metric</code> (default: <code>loss</code>): Metric plateau used to trigger when we reduce the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li> <li><code>learning_rate_scheduler.reduce_eval_split</code> (default: <code>training</code>): Which dataset split to listen on for reducing the learning rate when <code>reduce_on_plateau &gt; 0</code>.</li> </ul> <p>See the LightGBM documentation for more details about the available parameters.</p> <ul> <li><code>learning_rate</code> (default: <code>0.03</code>) : Controls how much to change the model in response to the estimated error each time the model weights are updated.</li> <li><code>early_stop</code> (default: <code>5</code>) : Number of consecutive rounds of evaluation without any improvement on the <code>validation_metric</code> that triggers training to stop. Can be set to -1, which disables early stopping entirely.</li> <li><code>max_depth</code> (default: <code>18</code>) : Maximum depth of a tree in the GBM trainer. A negative value means no limit.</li> <li><code>boosting_type</code> (default: <code>gbdt</code>) : Type of boosting algorithm to use with GBM trainer. Options: <code>gbdt</code>, <code>dart</code>.</li> <li><code>bagging_fraction</code> (default: <code>0.8</code>) : Fraction of data to use for bagging with GBM trainer.</li> <li><code>feature_fraction</code> (default: <code>0.75</code>) : Fraction of features to use in the GBM trainer.</li> <li><code>extra_trees</code> (default: <code>false</code>) : Whether to use extremely randomized trees in the GBM trainer. Options: <code>true</code>, <code>false</code>.</li> <li><code>lambda_l1</code> (default: <code>0.25</code>) : L1 regularization factor for the GBM trainer.</li> <li><code>lambda_l2</code> (default: <code>0.2</code>) : L2 regularization factor for the GBM trainer.</li> <li><code>drop_rate</code> (default: <code>0.1</code>): Dropout rate for the GBM trainer. Used only with boosting_type 'dart'.</li> <li><code>tree_learner</code> (default: <code>serial</code>): Type of tree learner to use with GBM trainer. Options: <code>serial</code>, <code>feature</code>, <code>data</code>, <code>voting</code>.</li> <li><code>boosting_rounds_per_checkpoint</code> (default: <code>50</code>): Number of boosting rounds per checkpoint / evaluation round.</li> <li><code>num_boost_round</code> (default: <code>1000</code>): Number of boosting rounds to perform with GBM trainer.</li> <li><code>num_leaves</code> (default: <code>82</code>): Number of leaves to use in the tree with GBM trainer.</li> <li><code>min_data_in_leaf</code> (default: <code>20</code>): Minimum number of data points in a leaf with GBM trainer.</li> <li><code>pos_bagging_fraction</code> (default: <code>1.0</code>): Fraction of positive data to use for bagging with GBM trainer.</li> <li><code>neg_bagging_fraction</code> (default: <code>1.0</code>): Fraction of negative data to use for bagging with GBM trainer.</li> <li><code>bagging_freq</code> (default: <code>1</code>): Frequency of bagging with GBM trainer.</li> <li><code>bagging_seed</code> (default: <code>3</code>): Random seed for bagging with GBM trainer.</li> <li><code>feature_fraction_bynode</code> (default: <code>1.0</code>): Fraction of features to use for each tree node with GBM trainer.</li> <li><code>feature_fraction_seed</code> (default: <code>2</code>): Random seed for feature fraction with GBM trainer.</li> <li><code>extra_seed</code> (default: <code>6</code>): Random seed for extremely randomized trees in the GBM trainer.</li> <li><code>linear_lambda</code> (default: <code>0.0</code>): Linear tree regularization in the GBM trainer.</li> <li><code>max_drop</code> (default: <code>50</code>): Maximum number of dropped trees during one boosting iteration. Used only with boosting_type 'dart'. A negative value means no limit.</li> <li><code>skip_drop</code> (default: <code>0.5</code>): Probability of skipping the dropout during one boosting iteration. Used only with boosting_type 'dart'.</li> <li><code>uniform_drop</code> (default: <code>false</code>): Whether to use uniform dropout in the GBM trainer. Used only with boosting_type 'dart'. Options: <code>true</code>, <code>false</code>.</li> <li><code>drop_seed</code> (default: <code>4</code>): Random seed to choose dropping models in the GBM trainer. Used only with boosting_type 'dart'.</li> <li><code>eval_batch_size</code> (default: <code>1048576</code>): Size of batch to pass to the model for evaluation.</li> <li><code>evaluate_training_set</code> (default: <code>false</code>): Whether to evaluate on the entire training set during evaluation. By default, training metrics will be computed at the end of each training step, and accumulated up to the evaluation phase. In practice, computing training set metrics during training is up to 30% faster than running a separate evaluation pass over the training set, but results in more noisy training metrics, particularly during the earlier epochs. It's recommended to only set this to True if you need very exact training set metrics, and are willing to pay a significant performance penalty for them. Options: <code>true</code>, <code>false</code>.</li> <li><code>validation_field</code> (default: <code>null</code>): First output feature, by default it is set as the same field of the first output feature.</li> <li><code>validation_metric</code> (default: <code>null</code>): Metric used on <code>validation_field</code>, set by default to the output feature type's <code>default_validation_metric</code>.</li> <li><code>min_sum_hessian_in_leaf</code> (default: <code>0.001</code>): Minimum sum of hessians in a leaf with GBM trainer.</li> <li><code>max_delta_step</code> (default: <code>0.0</code>): Used to limit the max output of tree leaves in the GBM trainer. A negative value means no constraint.</li> <li><code>min_gain_to_split</code> (default: <code>0.03</code>): Minimum gain to split a leaf in the GBM trainer.</li> <li><code>xgboost_dart_mode</code> (default: <code>false</code>): Whether to use xgboost dart mode in the GBM trainer. Used only with boosting_type 'dart'. Options: <code>true</code>, <code>false</code>.</li> <li><code>top_rate</code> (default: <code>0.2</code>): The retain ratio of large gradient data in the GBM trainer. Used only with boosting_type 'goss'.</li> <li><code>other_rate</code> (default: <code>0.1</code>): The retain ratio of small gradient data in the GBM trainer. Used only with boosting_type 'goss'.</li> <li><code>min_data_per_group</code> (default: <code>100</code>): Minimum number of data points per categorical group for the GBM trainer.</li> <li><code>max_cat_threshold</code> (default: <code>32</code>): Number of split points considered for categorical features for the GBM trainer.</li> <li><code>cat_l2</code> (default: <code>10.0</code>): L2 regularization factor for categorical split in the GBM trainer.</li> <li><code>cat_smooth</code> (default: <code>10.0</code>): Smoothing factor for categorical split in the GBM trainer.</li> <li><code>max_cat_to_onehot</code> (default: <code>4</code>): Maximum categorical cardinality required before one-hot encoding in the GBM trainer.</li> <li><code>cegb_tradeoff</code> (default: <code>1.0</code>): Cost-effective gradient boosting multiplier for all penalties in the GBM trainer.</li> <li><code>cegb_penalty_split</code> (default: <code>0.0</code>): Cost-effective gradient boosting penalty for splitting a node in the GBM trainer.</li> <li><code>path_smooth</code> (default: <code>0.0</code>): Smoothing factor applied to tree nodes in the GBM trainer.</li> <li><code>verbose</code> (default: <code>-1</code>): Verbosity level for GBM trainer. Options: <code>-1</code>, <code>0</code>, <code>1</code>, <code>2</code>.</li> <li><code>max_bin</code> (default: <code>255</code>): Maximum number of bins to use for discretizing features with GBM trainer.</li> <li><code>feature_pre_filter</code> (default: <code>true</code>): Whether to ignore features that are unsplittable based on min_data_in_leaf in the GBM trainer. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/trainer/#optimizer-parameters","title":"Optimizer parameters","text":"ECDGBM <p>The available optimizers wrap the ones available in PyTorch. For details about the parameters that can be used to configure different optimizers, please refer to the PyTorch documentation.</p> <p>The <code>learning_rate</code> parameter used by the optimizer comes from the <code>trainer</code> section. Other optimizer specific parameters, shown with their Ludwig default settings, follow:</p> <p>No optimizer parameters are available for the LightGBM trainer.</p>"},{"location":"configuration/trainer/#sgd","title":"sgd","text":"<pre><code>optimizer:\ntype: sgd\nmomentum: 0.0\nweight_decay: 0.0\ndampening: 0.0\nnesterov: false\n</code></pre> <ul> <li><code>momentum</code> (default: <code>0.0</code>): Momentum factor.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> <li><code>dampening</code> (default: <code>0.0</code>): Dampening for momentum.</li> <li><code>nesterov</code> (default: <code>false</code>): Enables Nesterov momentum. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/trainer/#lbfgs","title":"lbfgs","text":"<pre><code>optimizer:\ntype: lbfgs\nmax_iter: 20\nmax_eval: null\ntolerance_grad: 1.0e-07\ntolerance_change: 1.0e-09\nhistory_size: 100\nline_search_fn: null\n</code></pre> <ul> <li><code>max_iter</code> (default: <code>20</code>): Maximum number of iterations per optimization step.</li> <li><code>max_eval</code> (default: <code>null</code>): Maximum number of function evaluations per optimization step. Default: <code>max_iter</code> * 1.25.</li> <li><code>tolerance_grad</code> (default: <code>1e-07</code>): Termination tolerance on first order optimality.</li> <li><code>tolerance_change</code> (default: <code>1e-09</code>): Termination tolerance on function value/parameter changes.</li> <li><code>history_size</code> (default: <code>100</code>): Update history size.</li> <li><code>line_search_fn</code> (default: <code>null</code>): Line search function to use. Options: <code>strong_wolfe</code>, <code>null</code>.</li> </ul>"},{"location":"configuration/trainer/#adam","title":"adam","text":"<pre><code>optimizer:\ntype: adam\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\namsgrad: false\n</code></pre> <ul> <li><code>betas</code> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li> <li><code>eps</code> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay (L2 penalty).</li> <li><code>amsgrad</code> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/trainer/#adamw","title":"adamw","text":"<pre><code>optimizer:\ntype: adamw\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\namsgrad: false\n</code></pre> <ul> <li><code>betas</code> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li> <li><code>eps</code> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> <li><code>amsgrad</code> (default: <code>false</code>): Whether to use the AMSGrad variant of this algorithm from the paper 'On the Convergence of Adam and Beyond'.  Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/trainer/#adadelta","title":"adadelta","text":"<pre><code>optimizer:\ntype: adadelta\nrho: 0.9\neps: 1.0e-06\nweight_decay: 0.0\n</code></pre> <ul> <li><code>rho</code> (default: <code>0.9</code>): Coefficient used for computing a running average of squared gradients.</li> <li><code>eps</code> (default: <code>1e-06</code>): Term added to the denominator to improve numerical stability.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> </ul>"},{"location":"configuration/trainer/#adagrad","title":"adagrad","text":"<pre><code>optimizer:\ntype: adagrad\ninitial_accumulator_value: 0\nlr_decay: 0\nweight_decay: 0\neps: 1.0e-10\n</code></pre> <ul> <li><code>initial_accumulator_value</code> (default: <code>0</code>): </li> <li><code>lr_decay</code> (default: <code>0</code>): Learning rate decay.</li> <li><code>weight_decay</code> (default: <code>0</code>): Weight decay ($L2$ penalty).</li> <li><code>eps</code> (default: <code>1e-10</code>): Term added to the denominator to improve numerical stability.</li> </ul>"},{"location":"configuration/trainer/#adamax","title":"adamax","text":"<pre><code>optimizer:\ntype: adamax\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\n</code></pre> <ul> <li><code>betas</code> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li> <li><code>eps</code> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> </ul>"},{"location":"configuration/trainer/#nadam","title":"nadam","text":"<pre><code>optimizer:\ntype: nadam\nbetas:\n- 0.9\n- 0.999\neps: 1.0e-08\nweight_decay: 0.0\nmomentum_decay: 0.004\n</code></pre> <ul> <li><code>betas</code> (default: <code>[0.9, 0.999]</code>): Coefficients used for computing running averages of gradient and its square.</li> <li><code>eps</code> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> <li><code>momentum_decay</code> (default: <code>0.004</code>): Momentum decay.</li> </ul>"},{"location":"configuration/trainer/#rmsprop","title":"rmsprop","text":"<pre><code>optimizer:\ntype: rmsprop\nmomentum: 0.0\nalpha: 0.99\neps: 1.0e-08\ncentered: false\nweight_decay: 0.0\n</code></pre> <ul> <li><code>momentum</code> (default: <code>0.0</code>): Momentum factor.</li> <li><code>alpha</code> (default: <code>0.99</code>): Smoothing constant.</li> <li><code>eps</code> (default: <code>1e-08</code>): Term added to the denominator to improve numerical stability.</li> <li><code>centered</code> (default: <code>false</code>): If True, computes the centered RMSProp, and the gradient is normalized by an estimation of its variance. Options: <code>true</code>, <code>false</code>.</li> <li><code>weight_decay</code> (default: <code>0.0</code>): Weight decay ($L2$ penalty).</li> </ul> <p>Note</p> <p>Gradient clipping is also configurable, through optimizers, with the following parameters:</p> <pre><code>clip_global_norm: 0.5\nclipnorm: null\nclip_value: null\n</code></pre>"},{"location":"configuration/trainer/#training-length","title":"Training length","text":"<p>The length of the training process is configured by:</p> ECDGBM <ul> <li><code>epochs</code> (default: 100): One epoch is one pass through the entire dataset. By     default, <code>epochs</code> is 100 which means that the training process will run for     a maximum of 100 epochs before terminating.</li> <li><code>train_steps</code> (default: <code>None</code>): The maximum number of steps to train for,     using one mini-batch per step. By default this is unset, and <code>epochs</code> will     be used to determine training length.</li> </ul> <ul> <li><code>num_boost_round</code> (default: 100): The number of boosting iterations. By default,     <code>num_boost_round</code> is 100 which means that the training process will run for     a maximum of 100 boosting iterations before terminating.</li> </ul> <p>Tip</p> <p>In general, it's a good idea to set up a long training runway, relying on early stopping criteria (<code>early_stop</code>) to stop training when there hasn't been any improvement for a long time.</p>"},{"location":"configuration/trainer/#early-stopping","title":"Early stopping","text":"<p>Machine learning models, when trained for too long, are often prone to overfitting. It's generally a good policy to set up some early stopping criteria as it's not useful to have a model train after it's maximized what it can learn, as to retain it's ability to generalize to new data.</p>"},{"location":"configuration/trainer/#how-early-stopping-works-in-ludwig","title":"How early stopping works in Ludwig","text":"<p>By default, Ludwig sets <code>trainer.early_stop=5</code>, which means that if there have been <code>5</code> consecutive rounds of evaluation where there hasn't been any improvement on the validation subset, then training will terminate.</p> <p>Ludwig runs evaluation once per checkpoint, which by default is once per epoch. Checkpoint frequency can be configured using <code>checkpoints_per_epoch</code> (default: <code>1</code>) or <code>steps_per_checkpoint</code> (default: <code>0</code>, disabled). See this section for more details.</p>"},{"location":"configuration/trainer/#changing-the-metric-early-stopping-metrics","title":"Changing the metric early stopping metrics","text":"<p>The metric that dictates early stopping is <code>trainer.validation_field</code> and <code>trainer.validation_metric</code>. By default, early stopping uses the combined loss on the validation subset.</p> <pre><code>trainer:\nvalidation_field: combined\nvalidation_metric: loss\n</code></pre> <p>However, this can be configured to use other metrics. For example, if we had an output feature called <code>recommended</code>, then we can configure early stopping on the output feature accuracy like so:</p> <pre><code>trainer:\nvalidation_field: recommended\nvalidation_metric: accuracy\n</code></pre>"},{"location":"configuration/trainer/#disabling-early-stopping","title":"Disabling early stopping","text":"<p><code>trainer.early_stop</code> can be set to <code>-1</code>, which disables early stopping entirely.</p>"},{"location":"configuration/trainer/#checkpoint-evaluation-frequency","title":"Checkpoint-evaluation frequency","text":"ECD <p>Evaluation is run every time the model is checkpointed.</p> <p>By default, checkpoint-evaluation will occur once every epoch.</p> <p>The frequency of checkpoint-evaluation can be configured using:</p> <ul> <li><code>steps_per_checkpoint</code> (default: 0): every <code>n</code> training steps</li> <li><code>checkpoints_per_epoch</code> (default: 0): <code>n</code> times per epoch</li> </ul> <p>Note</p> <p>It is invalid to specify both non-zero <code>steps_per_checkpoint</code> and non-zero <code>checkpoints_per_epoch</code>.</p> <p>Tip</p> <p>Running evaluation once per epoch is an appropriate fit for small datasets  that fit in memory and train quickly. However, this can be a poor fit for unstructured datasets, which tend to be much larger, and train more slowly due to larger models.</p> <p>Running evaluation too frequently can be wasteful while running evaluation not frequently enough can be uninformative. In large-scale training runs, it's common for evaluation to be configured to run on a sub-epoch time scale, or every few thousand steps.</p> <p>We recommend configuring evaluation such that new evaluation results are available at least several times an hour. In general, it is not necessary for models to train over the entirety of a dataset, nor evaluate over the entirety of a test set, to produce useful monitoring metrics and signals to indicate model performance.</p>"},{"location":"configuration/trainer/#increasing-throughput-on-gpus","title":"Increasing throughput on GPUs","text":""},{"location":"configuration/trainer/#increase-batch-size","title":"Increase batch size","text":"ECD <pre><code>trainer:\nbatch_size: auto\n</code></pre> <p>Users training on GPUs can often increase training throughput by increasing the <code>batch_size</code> so that more examples are computed every training step. Set <code>batch_size</code> to <code>auto</code> to use the largest batch size that can fit in memory.</p>"},{"location":"configuration/trainer/#use-mixed-precision","title":"Use mixed precision","text":"ECD <pre><code>trainer:\nuse_mixed_precision: true\n</code></pre> <p>Speeds up training by using float16 parameters where it makes sense. Mixed precision training on GPU can dramatically speedup training, with some risks to model convergence. In practice, it works particularly well when fine-tuning a pretrained model like a HuggingFace transformer. See blog here for more details.</p>"},{"location":"configuration/features/audio_features/","title":"\u2191 Audio Features","text":""},{"location":"configuration/features/audio_features/#preprocessing","title":"Preprocessing","text":"<p>Example of a preprocessing specification (assuming the audio files have a sample rate of 16000):</p> <pre><code>preprocessing:\ntype: fbank\nmissing_value_strategy: bfill\naudio_file_length_limit_in_s: 7.5\nnorm: null\nwindow_length_in_s: 0.04\nwindow_shift_in_s: 0.02\nwindow_type: hamming\nfill_value: null\nin_memory: true\npadding_value: 0.0\nnum_fft_points: null\nnum_filter_bands: 80\n</code></pre> <p>Ludwig supports reading audio files using PyTorch's Torchaudio library. This library supports <code>WAV</code>, <code>AMB</code>, <code>MP3</code>, <code>FLAC</code>, <code>OGG/VORBIS</code>, <code>OPUS</code>, <code>SPHERE</code>, and <code>AMR-NB</code> formats.</p> <p>Parameters:</p> <ul> <li><code>type</code> (default: <code>fbank</code>) : Defines the type of audio feature to be used. Options: <code>fbank</code>, <code>group_delay</code>, <code>raw</code>, <code>stft</code>, <code>stft_phase</code>. See explanations for each type here.</li> <li><code>missing_value_strategy</code> (default: <code>bfill</code>) : What strategy to follow when there's a missing value in an audio column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>audio_file_length_limit_in_s</code> (default: <code>7.5</code>): Float value that defines the maximum limit of the audio file in seconds. All files longer than this limit are cut off. All files shorter than this limit are padded with padding_value</li> <li><code>norm</code> (default: <code>null</code>): Normalization strategy for the audio files. If None, no normalization is performed. If per_file, z-norm is applied on a 'per file' level Options: <code>per_file</code>, <code>null</code>.</li> <li><code>window_length_in_s</code> (default: <code>0.04</code>): Defines the window length used for the short time Fourier transformation. This is only needed if the audio_feature_type is 'raw'.</li> <li><code>window_shift_in_s</code> (default: <code>0.02</code>): Defines the window shift used for the short time Fourier transformation (also called hop_length). This is only needed if the audio_feature_type is 'raw'. </li> <li><code>window_type</code> (default: <code>hamming</code>): Defines the type window the signal is weighted before the short time Fourier transformation. Options: <code>bartlett</code>, <code>blackman</code>, <code>hamming</code>, <code>hann</code>.</li> <li><code>fill_value</code> (default: <code>null</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> <li><code>in_memory</code> (default: <code>true</code>): Defines whether the audio dataset will reside in memory during the training process or will be dynamically fetched from disk (useful for large datasets). In the latter case a training batch of input audio will be fetched from disk each training iteration. Options: <code>true</code>, <code>false</code>.</li> <li><code>padding_value</code> (default: <code>0.0</code>): Float value that is used for padding.</li> <li><code>num_fft_points</code> (default: <code>null</code>): Defines the number of fft points used for the short time Fourier transformation</li> <li><code>num_filter_bands</code> (default: <code>80</code>): Defines the number of filters used in the filterbank. Only needed if audio_feature_type is 'fbank'</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all audio input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/audio_features/#input-features","title":"Input Features","text":"<p>Audio files are transformed into one of the following types according to <code>type</code> under the <code>preprocessing</code> configuration.</p> <ul> <li><code>raw</code>: Audio file is transformed into a float valued tensor of size <code>N x L x W</code> (where <code>N</code> is the size of the dataset and <code>L</code> corresponds to <code>audio_file_length_limit_in_s * sample_rate</code> and <code>W = 1</code>).</li> <li><code>stft</code>: Audio is transformed to the <code>stft</code> magnitude. Audio file is transformed into a float valued tensor of size <code>N x L x W</code> (where <code>N</code> is the size of the dataset, <code>L</code> corresponds to <code>ceil(audio_file_length_limit_in_s * sample_rate - window_length_in_s * sample_rate + 1/ window_shift_in_s * sample_rate) + 1</code> and <code>W</code> corresponds to <code>num_fft_points / 2</code>).</li> <li><code>fbank</code>: Audio file is transformed to FBANK features (also called log Mel-filter bank values). FBANK features are implemented according to their definition in the HTK Book: Raw Signal -&gt; Preemphasis -&gt; DC mean removal -&gt; <code>stft</code> magnitude -&gt; Power spectrum: <code>stft^2</code> -&gt; mel-filter bank values: triangular filters equally spaced on a Mel-scale are applied -&gt; log-compression: <code>log()</code>. Overall the audio file is transformed into a float valued tensor of size <code>N x L x W</code> with <code>N,L</code> being equal to the ones in <code>stft</code> and <code>W</code> being equal to <code>num_filter_bands</code>.</li> <li><code>stft_phase</code>: The phase information for each stft bin is appended to the <code>stft</code> magnitude so that the audio file is transformed into a float valued tensor of size <code>N x L x 2W</code> with <code>N,L,W</code> being equal to the ones in <code>stft</code>.</li> <li><code>group_delay</code>: Audio is transformed to group delay features according to Equation (23) in this paper. Group_delay features has the same tensor size as <code>stft</code>.</li> </ul> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example audio feature entry in the input features list:</p> <pre><code>name: audio_column_name\ntype: audio\ntied: null\nencoder: type: parallel_cnn\n</code></pre>"},{"location":"configuration/features/audio_features/#encoders","title":"Encoders","text":"<p>Audio feature encoders are the same as for Sequence Features.</p> <p>Encoder type and encoder parameters can also be defined once and applied to all audio input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/audio_features/#output-features","title":"Output Features","text":"<p>There are no audio decoders at the moment.</p> <p>If this unlocks an interesting use case for your application, please file a GitHub Issue or ping the Ludwig Slack.</p>"},{"location":"configuration/features/bag_features/","title":"\u21c5 Bag Features","text":""},{"location":"configuration/features/bag_features/#preprocessing","title":"Preprocessing","text":"<p>Bag features are expected to be provided as a string of elements separated by whitespace, e.g. \"elem5 elem0 elem5 elem1\". Bags are similar to set features, the only difference being that elements may appear multiple times. The bag feature encoder outputs a matrix, similar to a set encoder, except each element of the matrix is a float value representing the frequency of the respective element in the bag. Embeddings are aggregated by summation, weighted by the frequency of each element.</p> <pre><code>preprocessing:\ntokenizer: space\nmissing_value_strategy: fill_with_const\nlowercase: false\nmost_common: 10000\nfill_value: &lt;UNK&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>tokenizer</code> (default: <code>space</code>) : Defines how to transform the raw text content of the dataset column to a set of elements. The default value space splits the string on spaces. Common options include: underscore (splits on underscore), comma (splits on comma), json (decodes the string into a set or a list through a JSON parser). Options: <code>space</code>, <code>space_punct</code>, <code>ngram</code>, <code>characters</code>, <code>underscore</code>, <code>comma</code>, <code>untokenized</code>, <code>stripped</code>, <code>english_tokenize</code>, <code>english_tokenize_filter</code>, <code>english_tokenize_remove_stopwords</code>, <code>english_lemmatize</code>, <code>english_lemmatize_filter</code>, <code>english_lemmatize_remove_stopwords</code>, <code>italian_tokenize</code>, <code>italian_tokenize_filter</code>, <code>italian_tokenize_remove_stopwords</code>, <code>italian_lemmatize</code>, <code>italian_lemmatize_filter</code>, <code>italian_lemmatize_remove_stopwords</code>, <code>spanish_tokenize</code>, <code>spanish_tokenize_filter</code>, <code>spanish_tokenize_remove_stopwords</code>, <code>spanish_lemmatize</code>, <code>spanish_lemmatize_filter</code>, <code>spanish_lemmatize_remove_stopwords</code>, <code>german_tokenize</code>, <code>german_tokenize_filter</code>, <code>german_tokenize_remove_stopwords</code>, <code>german_lemmatize</code>, <code>german_lemmatize_filter</code>, <code>german_lemmatize_remove_stopwords</code>, <code>french_tokenize</code>, <code>french_tokenize_filter</code>, <code>french_tokenize_remove_stopwords</code>, <code>french_lemmatize</code>, <code>french_lemmatize_filter</code>, <code>french_lemmatize_remove_stopwords</code>, <code>portuguese_tokenize</code>, <code>portuguese_tokenize_filter</code>, <code>portuguese_tokenize_remove_stopwords</code>, <code>portuguese_lemmatize</code>, <code>portuguese_lemmatize_filter</code>, <code>portuguese_lemmatize_remove_stopwords</code>, <code>dutch_tokenize</code>, <code>dutch_tokenize_filter</code>, <code>dutch_tokenize_remove_stopwords</code>, <code>dutch_lemmatize</code>, <code>dutch_lemmatize_filter</code>, <code>dutch_lemmatize_remove_stopwords</code>, <code>greek_tokenize</code>, <code>greek_tokenize_filter</code>, <code>greek_tokenize_remove_stopwords</code>, <code>greek_lemmatize</code>, <code>greek_lemmatize_filter</code>, <code>greek_lemmatize_remove_stopwords</code>, <code>norwegian_tokenize</code>, <code>norwegian_tokenize_filter</code>, <code>norwegian_tokenize_remove_stopwords</code>, <code>norwegian_lemmatize</code>, <code>norwegian_lemmatize_filter</code>, <code>norwegian_lemmatize_remove_stopwords</code>, <code>lithuanian_tokenize</code>, <code>lithuanian_tokenize_filter</code>, <code>lithuanian_tokenize_remove_stopwords</code>, <code>lithuanian_lemmatize</code>, <code>lithuanian_lemmatize_filter</code>, <code>lithuanian_lemmatize_remove_stopwords</code>, <code>danish_tokenize</code>, <code>danish_tokenize_filter</code>, <code>danish_tokenize_remove_stopwords</code>, <code>danish_lemmatize</code>, <code>danish_lemmatize_filter</code>, <code>danish_lemmatize_remove_stopwords</code>, <code>polish_tokenize</code>, <code>polish_tokenize_filter</code>, <code>polish_tokenize_remove_stopwords</code>, <code>polish_lemmatize</code>, <code>polish_lemmatize_filter</code>, <code>polish_lemmatize_remove_stopwords</code>, <code>romanian_tokenize</code>, <code>romanian_tokenize_filter</code>, <code>romanian_tokenize_remove_stopwords</code>, <code>romanian_lemmatize</code>, <code>romanian_lemmatize_filter</code>, <code>romanian_lemmatize_remove_stopwords</code>, <code>japanese_tokenize</code>, <code>japanese_tokenize_filter</code>, <code>japanese_tokenize_remove_stopwords</code>, <code>japanese_lemmatize</code>, <code>japanese_lemmatize_filter</code>, <code>japanese_lemmatize_remove_stopwords</code>, <code>chinese_tokenize</code>, <code>chinese_tokenize_filter</code>, <code>chinese_tokenize_remove_stopwords</code>, <code>chinese_lemmatize</code>, <code>chinese_lemmatize_filter</code>, <code>chinese_lemmatize_remove_stopwords</code>, <code>multi_tokenize</code>, <code>multi_tokenize_filter</code>, <code>multi_tokenize_remove_stopwords</code>, <code>multi_lemmatize</code>, <code>multi_lemmatize_filter</code>, <code>multi_lemmatize_remove_stopwords</code>, <code>sentencepiece</code>, <code>clip</code>, <code>gpt2bpe</code>, <code>bert</code>, <code>hf_tokenizer</code>.</li> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a set column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>lowercase</code> (default: <code>false</code>): If true, converts the string to lowercase before tokenizing. Options: <code>true</code>, <code>false</code>.</li> <li><code>most_common</code> (default: <code>10000</code>): The maximum number of most common tokens to be considered. If the data contains more than this amount, the most infrequent tokens will be treated as unknown.</li> <li><code>fill_value</code> (default: <code>&lt;UNK&gt;</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul>"},{"location":"configuration/features/bag_features/#input-features","title":"Input Features","text":"<p>Bag features have only one encoder type available: <code>embed</code>.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example bag feature entry in the input features list:</p> <pre><code>name: bag_column_name\ntype: bag\ntied: null\nencoder: type: embed\n</code></pre> <p>Encoder type and encoder parameters can also be defined once and applied to all bag input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/bag_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/bag_features/#embed-weighted-encoder","title":"Embed Weighted Encoder","text":"<p>The embed weighted encoder first transforms the element frequency vector to sparse integer lists, which are then mapped to either dense or sparse embeddings (one-hot encodings). Lastly, embeddings are aggregated as a weighted sum where each embedding is multiplied by its respective element's frequency. Inputs are of size <code>b</code> while outputs are of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the dimensionality of the embeddings.</p> <p>The parameters are the same used for set input features except for <code>reduce_output</code> which should not be used because the weighted sum already acts as a reducer.</p> <pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 50\noutput_size: 10\nactivation: relu\nnorm: null\nrepresentation: dense\nforce_embedding_size: false\nembeddings_on_cpu: false\nembeddings_trainable: true\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>embedding_size</code> (default: <code>50</code>) : The maximum embedding size, the actual size will be min(vocabulary_size, embedding_size) for dense representations and exactly vocabulary_size for the sparse encoding, where vocabulary_size is the number of different strings appearing in the training set in the input column (plus 1 for the unknown token placeholder ). <li><code>output_size</code> (default: <code>10</code>) : If output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>representation</code> (default: <code>dense</code>): The representation of the embedding. Either dense or sparse. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>force_embedding_size</code> (default: <code>false</code>): Force the embedding size to be equal to the vocabulary size. This parameter has effect only if representation is dense. Options: <code>true</code>, <code>false</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If true embeddings are trained during the training process, if false embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding fine tuning them. This parameter has effect only when representation is dense as sparse one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>): This is the number of stacked fully connected layers that the input to the feature passes through. Their output is projected in the feature's output space.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): By default dense embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if representation is dense.</p> </li>"},{"location":"configuration/features/bag_features/#output-features","title":"Output Features","text":"<p>Bag types are not supported as output features at this time.</p>"},{"location":"configuration/features/binary_features/","title":"\u21c5 Binary Features","text":""},{"location":"configuration/features/binary_features/#preprocessing","title":"Preprocessing","text":"<p>Binary features are directly transformed into a binary valued vector of length <code>n</code> (where <code>n</code> is the size of the dataset) and added to the HDF5 with a key that reflects the name of column in the dataset.</p> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_false\nfallback_true_label: null\nfill_value: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>fill_with_false</code>) : What strategy to follow when there's a missing value in a binary column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>, <code>fill_with_false</code>. See Missing Value Strategy for details.</li> <li><code>fallback_true_label</code> (default: <code>null</code>): The label to interpret as 1 (True) when the binary feature doesn't have a conventional boolean value</li> <li><code>fill_value</code> (default: <code>null</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all binary input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/binary_features/#input-features","title":"Input Features","text":"<p>Binary features have two encoders, <code>passthrough</code> and <code>dense</code>. The available encoder can be specified using the <code>type</code> parameter:</p> <ul> <li><code>type</code> (default <code>passthrough</code>): the possible values are <code>passthrough</code> and <code>dense</code>. <code>passthrough</code> outputs the raw integer values unaltered. <code>dense</code> randomly initializes a trainable embedding matrix.</li> </ul> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example binary feature entry in the input features list:</p> <pre><code>name: binary_column_name\ntype: binary\ntied: null\nencoder: type: dense\n</code></pre> <p>Encoder type and encoder parameters can also be defined once and applied to all binary input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/binary_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/binary_features/#passthrough-encoder","title":"Passthrough Encoder","text":"<p>The <code>passthrough</code> encoder passes through raw binary values without any transformations. Inputs of size <code>b</code> are transformed to outputs of size <code>b x 1</code> where <code>b</code> is the batch size.</p> <pre><code>encoder:\ntype: passthrough\n</code></pre> <p>There are no additional parameters for the <code>passthrough</code> encoder.</p>"},{"location":"configuration/features/binary_features/#dense-encoder","title":"Dense Encoder","text":"<p>The <code>dense</code> encoder passes the raw binary values through a fully connected layer. Inputs of size <code>b</code> are transformed to size <code>b x h</code>.</p> <pre><code>encoder:\ntype: dense\ndropout: 0.0\noutput_size: 256\nnorm: null\nnum_layers: 1\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>output_size</code> (default: <code>256</code>) : Size of the output of the feature.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : Number of stacked fully connected layers to apply. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> </ul>"},{"location":"configuration/features/binary_features/#output-features","title":"Output Features","text":"<p>Binary output features can be used when a binary classification needs to be performed or when the output is a single probability. There is only one decoder available: <code>regressor</code>.</p> <p>Example binary output feature using default parameters:</p> <pre><code>name: binary_column_name\ntype: binary\nreduce_input: sum\ndependencies: []\ncalibration: false\nreduce_dependencies: sum\nthreshold: 0.5\ndecoder:\ntype: regressor\nfc_layers: null\nnum_fc_layers: 0\nfc_output_size: 256\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm: null\nfc_norm_params: null\nfc_activation: relu\nfc_dropout: 0.0\ninput_size: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nloss:\ntype: binary_weighted_cross_entropy\nweight: 1.0\npositive_class_weight: null\nrobust_lambda: 0\nconfidence_penalty: 0\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Features Dependencies.</li> <li><code>calibration</code> (default <code>false</code>): if true, performs calibration by temperature scaling after training is complete. Calibration uses the validation set to find a scale factor (temperature) which is multiplied with the logits to shift output probabilities closer to true likelihoods.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>threshold</code> (defaults <code>0.5</code>): The threshold above (greater or equal) which the predicted output of the sigmoid   function will be mapped to 1.</li> <li><code>loss</code> (default <code>{\"type\": \"binary_weighted_cross_entropy\"}</code>): is a dictionary containing a loss <code>type</code>. <code>binary_weighted_cross_entropy</code> is the only supported loss type for binary output features. See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"regressor\"}</code>): Decoder for the desired task. Options: <code>regressor</code>. See Decoder for details.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all binary output features using the Type-Global Decoder section.</p>"},{"location":"configuration/features/binary_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/binary_features/#regressor","title":"Regressor","text":"<p>The regressor decoder is a (potentially empty) stack of fully connected layers, followed by a projection into a single number followed by a sigmoid function.</p> <pre><code>decoder:\ntype: regressor\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> </ul>"},{"location":"configuration/features/binary_features/#loss","title":"Loss","text":""},{"location":"configuration/features/binary_features/#binary-weighted-cross-entropy","title":"Binary Weighted Cross Entropy","text":"<pre><code>loss:\ntype: binary_weighted_cross_entropy\npositive_class_weight: null\nrobust_lambda: 0\nconfidence_penalty: 0\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>positive_class_weight</code> (default: <code>null</code>) : Weight of the positive class.</li> <li><code>robust_lambda</code> (default: <code>0</code>): Replaces the loss with <code>(1 - robust_lambda) * loss + robust_lambda / c</code> where <code>c</code> is the number of classes. Useful in case of noisy labels.</li> <li><code>confidence_penalty</code> (default: <code>0</code>): Penalizes overconfident predictions (low entropy) by adding an additional term that penalizes too confident predictions by adding a <code>a * (max_entropy - entropy) / max_entropy</code> term to the loss, where a is the value of this parameter. Useful in case of noisy labels.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul> <p>Loss and loss related parameters can also be defined once and applied to all binary output features using the Type-Global Loss section.</p>"},{"location":"configuration/features/binary_features/#metrics","title":"Metrics","text":"<p>The metrics that are calculated every epoch and are available for binary features are the <code>accuracy</code>, <code>loss</code>, <code>precision</code>, <code>recall</code>, <code>roc_auc</code> and <code>specificity</code>.</p> <p>You can set any of these to be the <code>validation_metric</code> in the <code>training</code> section of the configuration if the <code>validation_field</code> is set as the name of a binary feature.</p>"},{"location":"configuration/features/category_features/","title":"\u21c5 Category Features","text":""},{"location":"configuration/features/category_features/#preprocessing","title":"Preprocessing","text":"<p>Category features are transformed into integer valued vectors of size <code>n</code> (where <code>n</code> is the size of the dataset) and added to the HDF5 with a key that reflects the name of column in the dataset. Categories are mapped to integers by first collecting a dictionary of all unique category strings present in the column of the dataset, ranking them descending by frequency and assigning a sequential integer ID from the most frequent to the most rare (with 0 assigned to the special unknown placeholder token <code>&lt;UNK&gt;</code>). The column name is added to the JSON file, with an associated dictionary containing</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the size of the set of all tokens (<code>vocab_size</code>)</li> <li>additional preprocessing information (by default how to fill missing values and what token to use to fill missing values)</li> </ol> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_const\nlowercase: false\nmost_common: 10000\nfill_value: &lt;UNK&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a category column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>lowercase</code> (default: <code>false</code>): Whether the string has to be lowercased before being handled by the tokenizer. Options: <code>true</code>, <code>false</code>.</li> <li><code>most_common</code> (default: <code>10000</code>): The maximum number of most common tokens to be considered. if the data contains more than this amount, the most infrequent tokens will be treated as unknown.</li> <li><code>fill_value</code> (default: <code>&lt;UNK&gt;</code>): The value to replace missing values with in case the <code>missing_value_strategy</code> is <code>fill_with_const</code></li> </ul> <p>Preprocessing parameters can also be defined once and applied to all category input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/category_features/#input-features","title":"Input Features","text":"<p>Category features have three encoders. The <code>passthrough</code> encoder passes the raw integer values coming from the input placeholders to outputs of size <code>b x 1</code>. The other two encoders map to either <code>dense</code> or <code>sparse</code> embeddings (one-hot encodings) and returned as outputs of size <code>b x h</code>, where <code>b</code> is the batch size and <code>h</code> is the dimensionality of the embeddings.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example category feature entry in the input features list:</p> <pre><code>name: category_column_name\ntype: category\ntied: null\nencoder: type: dense\n</code></pre> <p>The available encoder parameters are:</p> <ul> <li><code>type</code> (default <code>dense</code>): the possible values are <code>passthrough</code>, <code>dense</code> and <code>sparse</code>. <code>passthrough</code> outputs the raw integer values unaltered. <code>dense</code> randomly initializes a trainable embedding matrix, <code>sparse</code> uses one-hot encoding.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all category input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/category_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/category_features/#dense-encoder","title":"Dense Encoder","text":"<pre><code>encoder:\ntype: dense\ndropout: 0.0\nembedding_size: 50\nembedding_initializer: null\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>50</code>) : The maximum embedding size, the actual size will be min(vocabulary_size, embedding_size) for dense representations and exactly vocabulary_size for the sparse encoding, where vocabulary_size is the number of different strings appearing in the training set in the column the feature is named after (plus 1 for ). <li><code>embedding_initializer</code> (default: <code>null</code>): Initializer for the embedding matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>, <code>null</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li>"},{"location":"configuration/features/category_features/#sparse-encoder","title":"Sparse Encoder","text":"<pre><code>encoder:\ntype: sparse\ndropout: 0.0\nembedding_initializer: null\nembeddings_on_cpu: false\nembeddings_trainable: false\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_initializer</code> (default: <code>null</code>): Initializer for the embedding matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>, <code>null</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>embeddings_trainable</code> (default: <code>false</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/category_features/#output-features","title":"Output Features","text":"<p>Category features can be used when a multi-class classification needs to be performed. There is only one decoder available for category features: a (potentially empty) stack of fully connected layers, followed by a projection into a vector of size of the number of available classes, followed by a softmax.</p> <p>Example category output feature using default parameters:</p> <pre><code>name: category_column_name\ntype: category\nreduce_input: sum\ndependencies: []\ncalibration: false\nreduce_dependencies: sum\nloss:\ntype: softmax_cross_entropy\nconfidence_penalty: 0\nrobust_lambda: 0\nclass_weights: null\nclass_similarities: null\nclass_similarities_temperature: 0\ndecoder:\ntype: classifier\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>calibration</code> (default <code>false</code>): if true, performs calibration by temperature scaling after training is complete. Calibration uses the validation set to find a scale factor (temperature) which is multiplied with the logits to shift output probabilities closer to true likelihoods.</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Features Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>loss</code> (default <code>{type: softmax_cross_entropy}</code>): is a dictionary containing a loss <code>type</code>. <code>softmax_cross_entropy</code> is the only supported loss type for category output features. See Loss for details.</li> <li><code>top_k</code> (default <code>3</code>): determines the parameter <code>k</code>, the number of categories to consider when computing the <code>top_k</code> measure. It computes accuracy but considering as a match if the true category appears in the first <code>k</code> predicted categories ranked by decoder's confidence.</li> <li><code>decoder</code> (default: <code>{\"type\": \"classifier\"}</code>): Decoder for the desired task. Options: <code>classifier</code>. See Decoder for details.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all category output features using the Type-Global Decoder section.</p>"},{"location":"configuration/features/category_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/category_features/#classifier","title":"Classifier","text":"<pre><code>decoder:\ntype: classifier\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> </ul>"},{"location":"configuration/features/category_features/#loss","title":"Loss","text":""},{"location":"configuration/features/category_features/#softmax-cross-entropy","title":"Softmax Cross Entropy","text":"<pre><code>loss:\ntype: softmax_cross_entropy\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>class_weights</code> (default: <code>null</code>) : Weights to apply to each class in the loss. If not specified, all classes are weighted equally. The value can be a vector of weights, one for each class, that is multiplied to the loss of the datapoints that have that class as ground truth. It is an alternative to oversampling in case of unbalanced class distribution. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too). Alternatively, the value can be a dictionary with class strings as keys and weights as values, like <code>{class_a: 0.5, class_b: 0.7, ...}</code>.</li> <li><code>robust_lambda</code> (default: <code>0</code>): Replaces the loss with <code>(1 - robust_lambda) * loss + robust_lambda / c</code> where <code>c</code> is the number of classes. Useful in case of noisy labels.</li> <li><code>confidence_penalty</code> (default: <code>0</code>): Penalizes overconfident predictions (low entropy) by adding an additional term that penalizes too confident predictions by adding a <code>a * (max_entropy - entropy) / max_entropy</code> term to the loss, where a is the value of this parameter. Useful in case of noisy labels.</li> <li><code>class_similarities</code> (default: <code>null</code>): If not <code>null</code> it is a <code>c x c</code> matrix in the form of a list of lists that contains the mutual similarity of classes. It is used if <code>class_similarities_temperature</code> is greater than 0. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too).</li> <li><code>class_similarities_temperature</code> (default: <code>0</code>): The temperature parameter of the softmax that is performed on each row of <code>class_similarities</code>. The output of that softmax is used to determine the supervision vector to provide instead of the one hot vector that would be provided otherwise for each datapoint. The intuition behind it is that errors between similar classes are more tolerable than errors between really different classes.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul> <p>Loss and loss related parameters can also be defined once and applied to all category output features using the Type-Global Loss section.</p>"},{"location":"configuration/features/category_features/#metrics","title":"Metrics","text":"<p>The measures that are calculated every epoch and are available for category features are <code>accuracy</code>, <code>hits_at_k</code> (computes accuracy considering as a match if the true category appears in the first <code>k</code> predicted categories ranked by decoder's confidence) and the <code>loss</code> itself. You can set either of them as <code>validation_metric</code> in the <code>training</code> section of the configuration if you set the <code>validation_field</code> to be the name of a category feature.</p>"},{"location":"configuration/features/date_features/","title":"\u2191 Date Features","text":"<p>Date features are like <code>2023-06-25 15:00:00</code>, <code>2023-06-25</code>, <code>6-25-2023</code>, or <code>6/25/2023</code>.</p>"},{"location":"configuration/features/date_features/#preprocessing","title":"Preprocessing","text":"<p>Ludwig will try to infer the date format automatically, but a specific format can be provided. The date string spec is the same as the one described in python's datetime.</p> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_const\ndatetime_format: null\nfill_value: ''\n</code></pre> <pre><code>name: date_feature_name\ntype: date\npreprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: ''\ndatetime_format: \"%d %b %Y\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a date column Options: <code>fill_with_const</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>datetime_format</code> (default: <code>null</code>): This parameter can either be a datetime format string, or null, in which case the datetime format will be inferred automatically.</li> <li><code>fill_value</code> (default: ``): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all date input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/date_features/#input-features","title":"Input Features","text":"<p>Input date features are transformed into a int tensors of size <code>N x 9</code> (where <code>N</code> is the size of the dataset and the 9 dimensions contain year, month, day, weekday, yearday, hour, minute, second, and second of day).</p> <p>For example, the date <code>2022-06-25 09:30:59</code> would be deconstructed into:</p> <pre><code>[\n  2022,   # Year\n  6,      # June\n  25,     # 25th day of the month\n  5,      # Weekday: Saturday\n  176,    # 176th day of the year\n  9,      # Hour\n  30,     # Minute\n  59,     # Seconds\n  34259,  # 34259th second of the day\n]\n</code></pre> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Currently there are two encoders supported for dates: <code>DateEmbed</code> (default) and <code>DateWave</code>. The encoder can be set by specifying <code>embed</code> or <code>wave</code> in the feature's <code>encoder</code> parameter in the input feature's configuration.</p> <p>Example date feature entry in the input features list:</p> <pre><code>name: date_feature_name\ntype: date\nencoder: type: embed\n</code></pre> <p>Encoder type and encoder parameters can also be defined once and applied to all date input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/date_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/date_features/#embed-encoder","title":"Embed Encoder","text":"<p>This encoder passes the year through a fully connected layer of one neuron and embeds all other elements for the date, concatenates them and passes the concatenated representation through fully connected layers.</p> <pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 10\noutput_size: 10\nactivation: relu\nnorm: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>embedding_size</code> (default: <code>10</code>) : The maximum embedding size adopted.</li> <li><code>output_size</code> (default: <code>10</code>) : If an output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. Options: <code>true</code>, <code>false</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>. See Normalization for details.</li> <li><code>num_fc_layers</code> (default: <code>0</code>): The number of stacked fully connected layers.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</li> </ul>"},{"location":"configuration/features/date_features/#wave-encoder","title":"Wave Encoder","text":"<p>This encoder passes the year through a fully connected layer of one neuron and represents all other elements for the date by taking the cosine of their value with a different period (12 for months, 31 for days, etc.), concatenates them and passes the concatenated representation through fully connected layers.</p> <pre><code>encoder:\ntype: wave\ndropout: 0.0\noutput_size: 10\nactivation: relu\nnorm: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nnum_fc_layers: 1\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>output_size</code> (default: <code>10</code>) : If an output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>. See Normalization for details.</li> <li><code>num_fc_layers</code> (default: <code>1</code>): The number of stacked fully connected layers.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</li> </ul>"},{"location":"configuration/features/date_features/#output-features","title":"Output Features","text":"<p>There is currently no support for date as an output feature. Consider using the <code>TEXT</code> type.</p>"},{"location":"configuration/features/h3_features/","title":"\u2191 H3 Features","text":"<p>H3 is a indexing system for representing geospatial data. For more details about it refer to https://eng.uber.com/h3.</p>"},{"location":"configuration/features/h3_features/#preprocessing","title":"Preprocessing","text":"<p>Ludwig will parse the H3 64bit encoded format automatically.</p> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_const\nfill_value: 576495936675512319\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in an h3 column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>fill_value</code> (default: <code>576495936675512319</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all H3 input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/h3_features/#input-features","title":"Input Features","text":"<p>Input H3 features are transformed into a int valued tensors of size <code>N x 19</code> (where <code>N</code> is the size of the dataset and the 19 dimensions represent 4 H3 resolution parameters (4) - mode, edge, resolution, base cell - and 15 cell coordinate values.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example H3 feature entry in the input features list:</p> <pre><code>name: h3_feature_name\ntype: h3\ntied: null\nencoder: type: embed\n</code></pre> <p>The available encoder parameters are:</p> <ul> <li><code>type</code> (default <code>embed</code>): the possible values are <code>embed</code>, <code>weighted_sum</code>,  and <code>rnn</code>.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all H3 input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/h3_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/h3_features/#embed-encoder","title":"Embed Encoder","text":"<p>This encoder encodes each component of the H3 representation (mode, edge, resolution, base cell and children cells) with embeddings. Children cells with value <code>0</code> will be masked out. After the embedding, all embeddings are summed and optionally passed through a stack of fully connected layers.</p> <pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 10\noutput_size: 10\nactivation: relu\nnorm: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nreduce_output: sum\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>embedding_size</code> (default: <code>10</code>) : The maximum embedding size adopted.</li> <li><code>output_size</code> (default: <code>10</code>) : If an output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>): The number of stacked fully connected layers.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</li> </ul>"},{"location":"configuration/features/h3_features/#weighted-sum-embed-encoder","title":"Weighted Sum Embed Encoder","text":"<p>This encoder encodes each component of the H3 representation (mode, edge, resolution, base cell and children cells) with embeddings. Children cells with value <code>0</code> will be masked out. After the embedding, all embeddings are summed with a weighted sum (with learned weights) and optionally passed through a stack of fully connected layers.</p> <pre><code>encoder:\ntype: weighted_sum\ndropout: 0.0\nembedding_size: 10\noutput_size: 10\nactivation: relu\nnorm: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nshould_softmax: false\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>embedding_size</code> (default: <code>10</code>) : The maximum embedding size adopted.</li> <li><code>output_size</code> (default: <code>10</code>) : If an output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. Options: <code>true</code>, <code>false</code>.</li> <li><code>should_softmax</code> (default: <code>false</code>): Determines if the weights of the weighted sum should be passed though a softmax layer before being used. Options: <code>true</code>, <code>false</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>): The number of stacked fully connected layers.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</li> </ul>"},{"location":"configuration/features/h3_features/#rnn-encoder","title":"RNN Encoder","text":"<p>This encoder encodes each component of the H3 representation (mode, edge, resolution, base cell and children cells) with embeddings. Children cells with value <code>0</code> will be masked out. After the embedding, all embeddings are passed through an RNN encoder.</p> <p>The intuition behind this is that, starting from the base cell, the sequence of children cells can be seen as a sequence encoding the path in the tree of all H3 hexes.</p> <pre><code>encoder:\ntype: rnn\ndropout: 0.0\ncell_type: rnn\nnum_layers: 1\nembedding_size: 10\nrecurrent_dropout: 0.0\nhidden_size: 10\nbias_initializer: zeros\nactivation: tanh\nrecurrent_activation: sigmoid\nunit_forget_bias: true\nweights_initializer: xavier_uniform\nrecurrent_initializer: orthogonal\nreduce_output: last\nembeddings_on_cpu: false\nuse_bias: true\nbidirectional: false\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : The dropout rate</li> <li><code>cell_type</code> (default: <code>rnn</code>) : The type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>lstm_block</code>, <code>lstm</code>, <code>ln</code>, <code>lstm_cudnn</code>, <code>gru</code>, <code>gru_block</code>, <code>gru_cudnn</code>. For reference about the differences between the cells please refer to PyTorch's documentation. We suggest to use the <code>block</code> variants on CPU and the <code>cudnn</code> variants on GPU because of their increased speed.  Options: <code>rnn</code>, <code>lstm</code>, <code>lstm_block</code>, <code>ln</code>, <code>lstm_cudnn</code>, <code>gru</code>, <code>gru_block</code>, <code>gru_cudnn</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of stacked recurrent layers.</li> <li><code>embedding_size</code> (default: <code>10</code>) : The maximum embedding size adopted.</li> <li><code>recurrent_dropout</code> (default: <code>0.0</code>): The dropout rate for the recurrent state</li> <li><code>hidden_size</code> (default: <code>10</code>): The size of the hidden representation within the transformer block. It is usually the same as the embedding_size, but if the two values are different, a projection layer will be added before the first transformer block.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>activation</code> (default: <code>tanh</code>): The activation function to use Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>recurrent_activation</code> (default: <code>sigmoid</code>): The activation function to use in the recurrent step Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>unit_forget_bias</code> (default: <code>true</code>): If true, add 1 to the bias of the forget gate at initialization Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>recurrent_initializer</code> (default: <code>orthogonal</code>): The initializer for recurrent matrix weights Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>bidirectional</code> (default: <code>false</code>): If true, two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/features/h3_features/#output-features","title":"Output Features","text":"<p>There is currently no support for H3 as an output feature. Consider using the <code>TEXT</code> type.</p>"},{"location":"configuration/features/image_features/","title":"\u2191 Image Features","text":"<p>Input image features are transformed into a float valued tensors of size <code>N x C x H x W</code> (where <code>N</code> is the size of the dataset, <code>C</code> is the number of channels, and <code>H x W</code> is the height and width of the image (can be specified by the user). These tensors are added to HDF5 with a key that reflects the name of column in the dataset.</p> <p>The column name is added to the JSON file, with an associated dictionary containing preprocessing information about the sizes of the resizing.</p>"},{"location":"configuration/features/image_features/#supported-image-formats","title":"Supported Image Formats","text":"<p>The number of channels in the image is determined by the image format. The following table lists the supported image formats and the number of channels.</p> Format Number of channels Grayscale 1 Grayscale with Alpha 2 RGB 3 RGB with Alpha 4"},{"location":"configuration/features/image_features/#preprocessing","title":"Preprocessing","text":"<p>During preprocessing, raw image files are transformed into numpy arrays and saved in the hdf5 format.</p> <p>Note</p> <p>Images passed to an image encoder are expected to have the same size. If images are different sizes, by default they will be resized to the dimensions of the first image in the dataset. Optionally, a <code>resize_method</code> together with a target <code>width</code> and <code>height</code> can be specified in the feature preprocessing parameters, in which case all images will be resized to the specified target size.</p> <pre><code>preprocessing:\nmissing_value_strategy: bfill\nheight: null\nwidth: null\nnum_channels: null\nnum_processes: 1\nfill_value: null\nresize_method: interpolate\ninfer_image_num_channels: true\ninfer_image_dimensions: true\ninfer_image_max_height: 256\ninfer_image_max_width: 256\ninfer_image_sample_size: 100\nstandardize_image: null\nin_memory: true\nrequires_equal_dimensions: false\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>bfill</code>) : What strategy to follow when there's a missing value in an image column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>.</li> <li><code>height</code> (default: <code>null</code>): The image height in pixels. If this parameter is set, images will be resized to the specified height using the resize_method parameter. If None, images will be resized to the size of the first image in the dataset.</li> <li><code>width</code> (default: <code>null</code>): The image width in pixels. If this parameter is set, images will be resized to the specified width using the resize_method parameter. If None, images will be resized to the size of the first image in the dataset.</li> <li><code>num_channels</code> (default: <code>null</code>): Number of channels in the images. If specified, images will be read in the mode specified by the number of channels. If not specified, the number of channels will be inferred from the image format of the first valid image in the dataset.</li> <li><code>num_processes</code> (default: <code>1</code>): Specifies the number of processes to run for preprocessing images.</li> <li><code>fill_value</code> (default: <code>null</code>): The maximum number of most common tokens to be considered. If the data contains more than this amount, the most infrequent tokens will be treated as unknown.</li> <li><code>resize_method</code> (default: <code>interpolate</code>): The method to use for resizing images. Options: <code>crop_or_pad</code>, <code>interpolate</code>.</li> <li><code>infer_image_num_channels</code> (default: <code>true</code>): If true, then the number of channels in the dataset is inferred from a sample of the first image in the dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>infer_image_dimensions</code> (default: <code>true</code>): If true, then the height and width of images in the dataset will be inferred from a sample of the first image in the dataset. Each image that doesn't conform to these dimensions will be resized according to resize_method. If set to false, then the height and width of images in the dataset will be specified by the user. Options: <code>true</code>, <code>false</code>.</li> <li><code>infer_image_max_height</code> (default: <code>256</code>): If infer_image_dimensions is set, this is used as the maximum height of the images in the dataset.</li> <li><code>infer_image_max_width</code> (default: <code>256</code>): If infer_image_dimensions is set, this is used as the maximum width of the images in the dataset.</li> <li><code>infer_image_sample_size</code> (default: <code>100</code>): The sample size used for inferring dimensions of images in infer_image_dimensions.</li> <li><code>standardize_image</code> (default: <code>null</code>): Standardize image by per channel mean centering and standard deviation scaling . Options: <code>imagenet1k</code>, <code>null</code>.</li> <li><code>in_memory</code> (default: <code>true</code>): Defines whether image dataset will reside in memory during the training process or will be dynamically fetched from disk (useful for large datasets). In the latter case a training batch of input images will be fetched from disk each training iteration. Options: <code>true</code>, <code>false</code>.</li> <li><code>requires_equal_dimensions</code> (default: <code>false</code>): If true, then width and height must be equal. Options: <code>true</code>, <code>false</code>.</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all image input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/image_features/#input-features","title":"Input Features","text":"<p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> <li><code>augmentation</code> (default <code>False</code>): specifies image data augmentation operations to generate synthetic training data.  More details on image augmentation can be found here.</li> </ul> <p>Example image feature entry in the input features list:</p> <pre><code>name: image_column_name\ntype: image\ntied: null\nencoder: type: stacked_cnn\n</code></pre> <p>The available encoder parameters are:</p> <ul> <li><code>type</code> (default <code>stacked_cnn</code>): the possible values are <code>stacked_cnn</code>, <code>resnet</code>, <code>mlp_mixer</code>, <code>vit</code>, and TorchVision Pretrained Image Classification models.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all image input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/image_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/image_features/#convolutional-stack-encoder-stacked_cnn","title":"Convolutional Stack Encoder (<code>stacked_cnn</code>)","text":"<p>Stack of 2D convolutional layers with optional normalization, dropout, and down-sampling pooling layers, followed by an optional stack of fully connected layers.</p> <p>Convolutional Stack Encoder takes the following optional parameters:</p> <pre><code>encoder:\ntype: stacked_cnn\nconv_dropout: 0.0\noutput_size: 128\nnum_conv_layers: null\nout_channels: 32\nconv_norm: null\nfc_norm: null\nfc_norm_params: null\nconv_activation: relu\nkernel_size: 3\nstride: 1\npadding_mode: zeros\npadding: valid\ndilation: 1\ngroups: 1\npool_function: max\npool_kernel_size: 2\npool_stride: null\npool_padding: 0\npool_dilation: 1\nconv_norm_params: null\nconv_layers: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_use_bias: true\nfc_bias_initializer: zeros\nfc_weights_initializer: xavier_uniform\nnum_fc_layers: 1\nfc_layers: null\nnum_channels: null\nconv_use_bias: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>conv_dropout</code> (default: <code>0.0</code>) : Dropout rate</li> <li><code>output_size</code> (default: <code>128</code>) : If output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer. </li> <li><code>num_conv_layers</code> (default: <code>null</code>) : Number of convolutional layers to use in the encoder. </li> <li><code>out_channels</code> (default: <code>32</code>): Indicates the number of filters, and by consequence the output channels of the 2d convolution. If out_channels is not already specified in conv_layers this is the default out_channels that will be used for each layer. </li> <li><code>conv_norm</code> (default: <code>null</code>): If a norm is not already specified in conv_layers this is the default norm that will be used for each layer. It indicates the normalization applied to the activations and can be null, batch or layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>fc_norm</code> (default: <code>null</code>): If a norm is not already specified in fc_layers this is the default norm that will be used for each layer. It indicates the norm of the output and can be null, batch or layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Parameters used if norm is either batch or layer. For information on parameters used with batch see Torch's documentation on batch normalization or for layer see Torch's documentation on layer normalization.</li> <li><code>conv_activation</code> (default: <code>relu</code>): If an activation is not already specified in conv_layers this is the default activation that will be used for each layer. It indicates the activation function applied to the output. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>kernel_size</code> (default: <code>3</code>): An integer or pair of integers specifying the kernel size. A single integer specifies a square kernel, while a pair of integers specifies the height and width of the kernel in that order (h, w). If a kernel_size is not specified in conv_layers this kernel_size that will be used for each layer.</li> <li><code>stride</code> (default: <code>1</code>): An integer or pair of integers specifying the stride of the convolution along the height and width. If a stride is not already specified in conv_layers, specifies the default stride of the 2D convolutional kernel that will be used for each layer.</li> <li><code>padding_mode</code> (default: <code>zeros</code>): If padding_mode is not already specified in conv_layers, specifies the default padding_mode of the 2D convolutional kernel that will be used for each layer. Options: <code>zeros</code>, <code>reflect</code>, <code>replicate</code>, <code>circular</code>.</li> <li><code>padding</code> (default: <code>valid</code>): An int, pair of ints (h, w), or one of ['valid', 'same'] specifying the padding used forconvolution kernels.</li> <li><code>dilation</code> (default: <code>1</code>): An int or pair of ints specifying the dilation rate to use for dilated convolution. If dilation is not already specified in conv_layers, specifies the default dilation of the 2D convolutional kernel that will be used for each layer.</li> <li><code>groups</code> (default: <code>1</code>): Groups controls the connectivity between convolution inputs and outputs. When groups = 1, each output channel depends on every input channel. When groups &gt; 1, input and output channels are divided into groups separate groups, where each output channel depends only on the inputs in its respective input channel group. in_channels and out_channels must both be divisible by groups.</li> <li><code>pool_function</code> (default: <code>max</code>): Pooling function to use. Options: <code>max</code>, <code>average</code>, <code>avg</code>, <code>mean</code>.</li> <li><code>pool_kernel_size</code> (default: <code>2</code>): An integer or pair of integers specifying the pooling size. If pool_kernel_size is not specified in conv_layers this is the default value that will be used for each layer.</li> <li><code>pool_stride</code> (default: <code>null</code>): An integer or pair of integers specifying the pooling stride, which is the factor by which the pooling layer downsamples the feature map. Defaults to pool_kernel_size.</li> <li><code>pool_padding</code> (default: <code>0</code>): An integer or pair of ints specifying pooling padding (h, w).</li> <li><code>pool_dilation</code> (default: <code>1</code>): An integer or pair of ints specifying pooling dilation rate (h, w).</li> <li><code>conv_norm_params</code> (default: <code>null</code>): Parameters used if conv_norm is either batch or layer. </li> <li><code>conv_layers</code> (default: <code>null</code>): List of convolutional layers to use in the encoder. </li> <li><code>fc_dropout</code> (default: <code>0.0</code>): Dropout rate</li> <li><code>fc_activation</code> (default: <code>relu</code>): If an activation is not already specified in fc_layers this is the default activation that will be used for each layer. It indicates the activation function applied to the output. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>constant</code>, <code>dirac</code>, <code>eye</code>, <code>identity</code>, <code>kaiming_normal</code>, <code>kaiming_uniform</code>, <code>normal</code>, <code>ones</code>, <code>orthogonal</code>, <code>sparse</code>, <code>uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>zeros</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weights matrix. Options: <code>constant</code>, <code>dirac</code>, <code>eye</code>, <code>identity</code>, <code>kaiming_normal</code>, <code>kaiming_uniform</code>, <code>normal</code>, <code>ones</code>, <code>orthogonal</code>, <code>sparse</code>, <code>uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>zeros</code>.</li> <li><code>num_fc_layers</code> (default: <code>1</code>): The number of stacked fully connected layers.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): A list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: activation, dropout, norm, norm_params, output_size, use_bias, bias_initializer and weights_initializer. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. </p> </li> <li> <p><code>num_channels</code> (default: <code>null</code>): Number of channels to use in the encoder. </p> </li> <li><code>conv_use_bias</code> (default: <code>true</code>): If bias not already specified in conv_layers, specifies if the 2D convolutional kernel should have a bias term. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/features/image_features/#mlp-mixer-encoder","title":"MLP-Mixer Encoder","text":"<p>Encodes images using MLP-Mixer, as described in MLP-Mixer: An all-MLP Architecture for Vision. MLP-Mixer divides the image into equal-sized patches, applying fully connected layers to each patch to compute per-patch representations (tokens) and combining the representations with fully-connected mixer layers.</p> <p>The MLP-Mixer Encoder takes the following optional parameters:</p> <pre><code>encoder:\ntype: mlp_mixer\ndropout: 0.0\nnum_layers: 8\npatch_size: 16\nnum_channels: null\nembed_size: 512\ntoken_size: 2048\nchannel_dim: 256\navg_pool: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate.</li> <li><code>num_layers</code> (default: <code>8</code>) : The depth of the network (the number of Mixer blocks).</li> <li> <p><code>patch_size</code> (default: <code>16</code>): The image patch size. Each patch is patch_size\u00b2 pixels. Must evenly divide the image width and height.</p> </li> <li> <p><code>num_channels</code> (default: <code>null</code>): Number of channels to use in the encoder. </p> </li> <li><code>embed_size</code> (default: <code>512</code>): The patch embedding size, the output size of the mixer if avg_pool is true.</li> <li><code>token_size</code> (default: <code>2048</code>): The per-patch embedding size.</li> <li><code>channel_dim</code> (default: <code>256</code>): Number of channels in hidden layer.</li> <li><code>avg_pool</code> (default: <code>true</code>): If true, pools output over patch dimension, outputs a vector of shape (embed_size). If false, the output tensor is of shape (n_patches, embed_size), where n_patches is img_height x img_width / patch_size\u00b2. Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/features/image_features/#torchvision-pretrained-model-encoders","title":"TorchVision Pretrained Model Encoders","text":"<p>Twenty TorchVision pretrained image classification models are available as Ludwig image encoders.  The available models are:</p> <ul> <li><code>AlexNet</code></li> <li><code>ConvNeXt</code></li> <li><code>DenseNet</code></li> <li><code>EfficientNet</code></li> <li><code>EfficientNetV2</code></li> <li><code>GoogLeNet</code></li> <li><code>Inception V3</code></li> <li><code>MaxVit</code></li> <li><code>MNASNet</code></li> <li><code>MobileNet V2</code></li> <li><code>MobileNet V3</code></li> <li><code>RegNet</code></li> <li><code>ResNet</code></li> <li><code>ResNeXt</code></li> <li><code>ShuffleNet V2</code></li> <li><code>SqueezeNet</code></li> <li><code>SwinTransformer</code></li> <li><code>VGG</code></li> <li><code>VisionTransformer</code></li> <li><code>Wide ResNet</code></li> </ul> <p>See TorchVison documentation for more details.</p> <p>Ludwig encoders parameters for TorchVision pretrained models:</p>"},{"location":"configuration/features/image_features/#alexnet","title":"AlexNet","text":"<pre><code>encoder:\ntype: alexnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: base\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>base</code>): Pretrained model variant to use. Options: <code>base</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#convnext","title":"ConvNeXt","text":"<pre><code>encoder:\ntype: convnext\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: base\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>base</code>): Pretrained model variant to use. Options: <code>tiny</code>, <code>small</code>, <code>base</code>, <code>large</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#densenet","title":"DenseNet","text":"<pre><code>encoder:\ntype: densenet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: 121\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>121</code>): Pretrained model variant to use. Options: <code>121</code>, <code>161</code>, <code>169</code>, <code>201</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#efficientnet","title":"EfficientNet","text":"<pre><code>encoder:\ntype: efficientnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: b0\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>b0</code>): Pretrained model variant to use. Options: <code>b0</code>, <code>b1</code>, <code>b2</code>, <code>b3</code>, <code>b4</code>, <code>b5</code>, <code>b6</code>, <code>b7</code>, <code>v2_</code>, <code>v2_m</code>, <code>v2_l</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#googlenet","title":"GoogLeNet","text":"<pre><code>encoder:\ntype: googlenet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: base\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>base</code>): Pretrained model variant to use. Options: <code>base</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#inception-v3","title":"Inception V3","text":"<pre><code>encoder:\ntype: inceptionv3\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: base\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>base</code>): Pretrained model variant to use. Options: <code>base</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#maxvit","title":"MaxVit","text":"<pre><code>encoder:\ntype: maxvit\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: t\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>t</code>): Pretrained model variant to use. Options: <code>t</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#mnasnet","title":"MNASNet","text":"<pre><code>encoder:\ntype: mnasnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: '0_5'\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>0_5</code>): Pretrained model variant to use. Options: <code>0_5</code>, <code>0_75</code>, <code>1_0</code>, <code>1_3</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#mobilenet-v2","title":"MobileNet V2","text":"<pre><code>encoder:\ntype: mobilenetv2\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: base\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>base</code>): Pretrained model variant to use. Options: <code>base</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#mobilenet-v3","title":"MobileNet V3","text":"<pre><code>encoder:\ntype: mobilenetv3\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: small\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>small</code>): Pretrained model variant to use. Options: <code>small</code>, <code>large</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#regnet","title":"RegNet","text":"<pre><code>encoder:\ntype: regnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: x_1_6gf\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>x_1_6gf</code>): Pretrained model variant to use. Options: <code>x_1_6gf</code>, <code>x_16gf</code>, <code>x_32gf</code>, <code>x_3_2gf</code>, <code>x_400mf</code>, <code>x_800mf</code>, <code>x_8gf</code>, <code>y_128gf</code>, <code>y_16gf</code>, <code>y_1_6gf</code>, <code>y_32gf</code>, <code>y_3_2gf</code>, <code>y_400mf</code>, <code>y_800mf</code>, <code>y_8gf</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#resnet","title":"ResNet","text":"<pre><code>encoder:\ntype: resnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: 50\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>50</code>): Pretrained model variant to use. Options: <code>18</code>, <code>34</code>, <code>50</code>, <code>101</code>, <code>152</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#resnext","title":"ResNeXt","text":"<pre><code>encoder:\ntype: resnext\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: 50_32x4d\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>50_32x4d</code>): Pretrained model variant to use. Options: <code>50_32x4d</code>, <code>101_32x8d</code>, <code>101_64x4d</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#shufflenet-v2","title":"ShuffleNet V2","text":"<pre><code>encoder:\ntype: shufflenet_v2\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: x0_5\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>x0_5</code>): Pretrained model variant to use. Options: <code>x0_5</code>, <code>x1_0</code>, <code>x1_5</code>, <code>x2_0</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#squeezenet","title":"SqueezeNet","text":"<pre><code>encoder:\ntype: squeezenet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: '1_0'\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>1_0</code>): Pretrained model variant to use. Options: <code>1_0</code>, <code>1_1</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#swintransformer","title":"SwinTransformer","text":"<pre><code>encoder:\ntype: swin_transformer\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: t\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>t</code>): Pretrained model variant to use. Options: <code>t</code>, <code>s</code>, <code>b</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#vgg","title":"VGG","text":"<pre><code>encoder:\ntype: vgg\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: 11\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>11</code>): Pretrained model variant to use.</p> </li> </ul>"},{"location":"configuration/features/image_features/#visiontransformer","title":"VisionTransformer","text":"<pre><code>encoder:\ntype: vit\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: b_16\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>b_16</code>): Pretrained model variant to use. Options: <code>b_16</code>, <code>b_32</code>, <code>l_16</code>, <code>l_32</code>, <code>h_14</code>.</p> </li> </ul>"},{"location":"configuration/features/image_features/#wide-resnet","title":"Wide ResNet","text":"<pre><code>encoder:\ntype: wide_resnet\nuse_pretrained: true\ntrainable: true\nmodel_cache_dir: null\nmodel_variant: '50_2'\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Download model weights from pre-trained model. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>model_cache_dir</code> (default: <code>null</code>): Directory path to cache pretrained model weights.</p> </li> <li> <p><code>model_variant</code> (default: <code>50_2</code>): Pretrained model variant to use. Options: <code>50_2</code>, <code>101_2</code>.</p> </li> </ul> <p>Note:</p> <ul> <li>At this time Ludwig supports only the <code>DEFAULT</code> pretrained weights, which are the best available weights for a specific model. More details on <code>DEFAULT</code> weights can be found in this blog post.</li> <li>Some TorchVision pretrained models consume large amounts of memory.  These <code>model_variant</code> required more than 12GB of memory:</li> <li><code>efficientnet_torch</code>: <code>b7</code></li> <li><code>regnet_torch</code>: <code>y_128gf</code></li> <li><code>vit_torch</code>: <code>h_14</code></li> </ul>"},{"location":"configuration/features/image_features/#deprecated-encoders-planned-to-remove-in-v08","title":"Deprecated Encoders (planned to remove in v0.8)","text":""},{"location":"configuration/features/image_features/#legacy-resnet-encoder","title":"Legacy ResNet Encoder","text":"<p>DEPRECATED: This encoder is deprecated and will be removed in a future release. Please use the equivalent TorchVision ResNet encoder instead.</p> <p>Implements ResNet V2 as described in Identity Mappings in Deep Residual Networks.</p> <p>The ResNet encoder takes the following optional parameters:</p> <pre><code>encoder:\ntype: _resnet_legacy\ndropout: 0.0\noutput_size: 128\nactivation: relu\nnorm: null\nfirst_pool_kernel_size: null\nfirst_pool_stride: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nnum_fc_layers: 1\nfc_layers: null\nresnet_size: 50\nnum_channels: null\nout_channels: 32\nkernel_size: 3\nconv_stride: 1\nbatch_norm_momentum: 0.9\nbatch_norm_epsilon: 0.001\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate</li> <li><code>output_size</code> (default: <code>128</code>) : if output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer. </li> <li><code>activation</code> (default: <code>relu</code>): if an activation is not already specified in fc_layers this is the default activation that will be used for each layer. It indicates the activation function applied to the output. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): if a norm is not already specified in fc_layers this is the default norm that will be used for each layer. It indicates the norm of the output and can be null, batch or layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>first_pool_kernel_size</code> (default: <code>null</code>): Pool size to be used for the first pooling layer. If none, the first pooling layer is skipped.</li> <li><code>first_pool_stride</code> (default: <code>null</code>): Stride for first pooling layer. If null, defaults to first_pool_kernel_size.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): initializer for the bias vector. Options: <code>constant</code>, <code>dirac</code>, <code>eye</code>, <code>identity</code>, <code>kaiming_normal</code>, <code>kaiming_uniform</code>, <code>normal</code>, <code>ones</code>, <code>orthogonal</code>, <code>sparse</code>, <code>uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>zeros</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weights matrix. Options: <code>constant</code>, <code>dirac</code>, <code>eye</code>, <code>identity</code>, <code>kaiming_normal</code>, <code>kaiming_uniform</code>, <code>normal</code>, <code>ones</code>, <code>orthogonal</code>, <code>sparse</code>, <code>uniform</code>, <code>xavier_normal</code>, <code>xavier_uniform</code>, <code>zeros</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): parameters used if norm is either batch or layer. For information on parameters used with batch see Torch's documentation on batch normalization or for layer see Torch's documentation on layer normalization.</li> <li><code>num_fc_layers</code> (default: <code>1</code>): The number of stacked fully connected layers.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): A list of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: activation, dropout, norm, norm_params, output_size, use_bias, bias_initializer and weights_initializer. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. </p> </li> <li> <p><code>resnet_size</code> (default: <code>50</code>): The size of the ResNet model to use.</p> </li> <li><code>num_channels</code> (default: <code>null</code>): Number of channels to use in the encoder. </li> <li><code>out_channels</code> (default: <code>32</code>): Indicates the number of filters, and by consequence the output channels of the 2d convolution. If out_channels is not already specified in conv_layers this is the default out_channels that will be used for each layer. </li> <li><code>kernel_size</code> (default: <code>3</code>): An integer or pair of integers specifying the kernel size. A single integer specifies a square kernel, while a pair of integers specifies the height and width of the kernel in that order (h, w). If a kernel_size is not specified in conv_layers this kernel_size that will be used for each layer.</li> <li><code>conv_stride</code> (default: <code>1</code>): An integer or pair of integers specifying the stride of the initial convolutional layer.</li> <li><code>batch_norm_momentum</code> (default: <code>0.9</code>): Momentum of the batch norm running statistics.</li> <li><code>batch_norm_epsilon</code> (default: <code>0.001</code>): Epsilon of the batch norm.</li> </ul>"},{"location":"configuration/features/image_features/#legacy-vision-transformer-encoder","title":"Legacy Vision Transformer Encoder","text":"<p>DEPRECATED: This encoder is deprecated and will be removed in a future release. Please use the equivalent TorchVision VisionTransformer encoder instead.</p> <p>Encodes images using a Vision Transformer as described in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</p> <p>Vision Transformer divides the image into equal-sized patches, uses a linear transformation to encode each flattened patch, then applies a deep transformer architecture to the sequence of encoded patches.</p> <p>The Vision Transformer Encoder takes the following optional parameters:</p> <pre><code>encoder:\ntype: _vit_legacy\nhidden_dropout_prob: 0.1\nattention_probs_dropout_prob: 0.1\ntrainable: true\nuse_pretrained: true\npretrained_model: google/vit-base-patch16-224\nhidden_size: 768\nhidden_act: gelu\npatch_size: 16\ninitializer_range: 0.02\nnum_hidden_layers: 12\nnum_attention_heads: 12\nintermediate_size: 3072\nlayer_norm_eps: 1.0e-12\ngradient_checkpointing: false\n</code></pre> <p>Parameters:</p> <ul> <li><code>hidden_dropout_prob</code> (default: <code>0.1</code>) : The dropout rate for all fully connected layers in the embeddings, encoder, and pooling.</li> <li><code>attention_probs_dropout_prob</code> (default: <code>0.1</code>) : The dropout rate for the attention probabilities.</li> <li><code>trainable</code> (default: <code>true</code>) : Is the encoder trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_pretrained</code> (default: <code>true</code>) : Use pre-trained model weights from Hugging Face. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model</code> (default: <code>google/vit-base-patch16-224</code>) : The name of the pre-trained model to use.</li> <li><code>hidden_size</code> (default: <code>768</code>): Dimensionality of the encoder layers and the pooling layer.</li> <li><code>hidden_act</code> (default: <code>gelu</code>): Hidden layer activation, one of gelu, relu, selu or gelu_new. Options: <code>relu</code>, <code>gelu</code>, <code>selu</code>, <code>gelu_new</code>.</li> <li><code>patch_size</code> (default: <code>16</code>): The image patch size. Each patch is patch_size\u00b2 pixels. Must evenly divide the image width and height.</li> <li> <p><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> </li> <li> <p><code>num_hidden_layers</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</p> </li> <li><code>num_attention_heads</code> (default: <code>12</code>): Number of attention heads in each attention layer.</li> <li><code>intermediate_size</code> (default: <code>3072</code>): Dimensionality of the intermediate (i.e., feed-forward) layer in the Transformer encoder.</li> <li><code>layer_norm_eps</code> (default: <code>1e-12</code>): The epsilon used by the layer normalization layers.</li> <li><code>gradient_checkpointing</code> (default: <code>false</code>):  Options: <code>true</code>, <code>false</code>.</li> </ul>"},{"location":"configuration/features/image_features/#image-augmentation","title":"Image Augmentation","text":"<p>Image augmentation is a technique used to increase the diversity of a training dataset by applying random transformations to the images. The goal is to train a model that is robust to the variations in the training data.</p> <p>Augmentation is specified by the <code>augmentation</code> section in the image feature configuration and can be specified in one of the following ways:</p> <p>Boolean: <code>False</code> (Default) No augmentation is applied to the images.</p> <pre><code>augmentation: False\n</code></pre> <p>Boolean: <code>True</code> The following augmentation methods are applied to the images: <code>random_horizontal_flip</code> and <code>random_rotate</code>.</p> <pre><code>augmentation: True\n</code></pre> <p>List of Augmentation Methods One or more of the following augmentation methods are applied to the images in the order specified by the user: <code>random_horizontal_flip</code>, <code>random_vertical_flip</code>, <code>random_rotate</code>, <code>random_blur</code>, <code>random_brightness</code>, and <code>random_contrast</code>.  The following is an illustrative example.</p> <pre><code>augmentation:\n- type: random_horizontal_flip\n- type: random_vertical_flip\n- type: random_rotate\ndegree: 10\n- type: random_blur\nkernel_size: 3\n- type: random_brightness\nmin: 0.5\nmax: 2.0\n- type: random_contrast\nmin: 0.5\nmax: 2.0\n</code></pre> <p>Augmentation is applied to the batch of images in the training set only.  The validation and test sets are not augmented.</p> <p>Following illustrates how augmentation affects an image:</p> <p></p> <p>Horizontal Flip: Image is randomly flipped horizontally.</p> <pre><code>type: random_horizontal_flip\n</code></pre> <p></p> <p>Vertical Flip:  Image is randomly flipped vertically.</p> <pre><code>type: random_vertical_flip\n</code></pre> <p></p> <p>Rotate: Image is randomly rotated by an amount in the range [-degree, +degree].  <code>degree</code> must be a positive integer.</p> <pre><code>type: random_rotate\ndegree: 15\n</code></pre> <p>Parameters:</p> <ul> <li><code>degree</code> (default: <code>15</code>): Range of angle for random rotation, i.e.,  [-degree, +degree].</li> </ul> <p>Following shows the effect of rotating an image:</p> <p></p> <p>Blur:  Image is randomly blurred using a Gaussian filter with kernel size specified by the user.  The <code>kernel_size</code> must be a positive, odd integer.</p> <pre><code>type: random_blur\nkernel_size: 3\n</code></pre> <p>Parameters:</p> <ul> <li><code>kernel_size</code> (default: <code>3</code>): Kernel size for random blur.</li> </ul> <p>Following shows the effect of blurring an image with various kernel sizes:</p> <p></p> <p>Adjust Brightness: Image brightness is adjusted by a factor randomly selected in the range [min, max].   Both <code>min</code> and <code>max</code> must be a float greater than 0, with <code>min</code> less than <code>max</code>.</p> <pre><code>type: random_brightness\nmin: 0.5\nmax: 2.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>min</code> (default: <code>0.5</code>): Minimum factor for random brightness.</li> <li><code>max</code> (default: <code>2.0</code>): Maximum factor for random brightness.</li> </ul> <p>Following shows the effect of brightness adjustment with various factors:</p> <p></p> <p>Adjust Contrast: Image contrast is adjusted by a factor randomly selected in the range [min, max].  Both <code>min</code> and <code>max</code> must be a float greater than 0, with <code>min</code> less than <code>max</code>.</p> <pre><code>type: random_contrast\nmin: 0.5\nmax: 2.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>min</code> (default: <code>0.5</code>): Minimum factor for random brightness.</li> <li><code>max</code> (default: <code>2.0</code>): Maximum factor for random brightness.</li> </ul> <p>Following shows the effect of contrast adjustment with various factors:</p> <p></p> <p>Illustrative Examples of Image Feature Configuration with Augmentation</p> <pre><code>name: image_column_name\ntype: image\nencoder: type: resnet\nmodel_variant: 18\nuse_pretrained: true\npretrained_cache_dir: None\ntrainable: true\naugmentation: false\n</code></pre> <pre><code>name: image_column_name\ntype: image\nencoder: type: stacked_cnn\naugmentation: true\n</code></pre> <pre><code>name: image_column_name\ntype: image\nencoder: type: alexnet\naugmentation:\n- type: random_horizontal_flip\n- type: random_rotate\ndegree: 10\n- type: random_blur\nkernel_size: 3\n- type: random_brightness\nmin: 0.5\nmax: 2.0\n- type: random_contrast\nmin: 0.5\nmax: 2.0\n- type: random_vertical_flip\n</code></pre>"},{"location":"configuration/features/image_features/#output-features","title":"Output Features","text":"<p>There are no image decoders at the moment (WIP), so images cannot be used as output features.</p>"},{"location":"configuration/features/input_features/","title":"Input Features (\u2191)","text":"<p>The <code>input_features</code> section is list of feature definitions. Each feature definition contains two required fields: <code>name</code> and <code>type</code>.</p> YAMLPython Dict <pre><code>input_features:\n-\nname: Pclass\ntype: category\n</code></pre> <pre><code>{\n    \"input_features\": [{\"name\": \"Pclass\", \"type\": \"category\"}]\n}\n</code></pre> <p><code>name</code> is the name of the feature in the dataset. <code>type</code> is one of the supported data types.</p>"},{"location":"configuration/features/input_features/#preprocessing","title":"Preprocessing","text":"<p>Recall Ludwig's butterfly framework.</p> <p></p> <p>Each input feature can specify its own preprocessing via the <code>preprocessing</code> subsection.</p> YAMLPython Dict <pre><code>input_features:\n-\nname: Fare\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n</code></pre> <pre><code>{\n    \"input_features\": [\n        {\n            \"name\": \"Fare\",\n            \"type\": \"number\",\n            \"preprocessing\": {\n                \"missing_value_strategy\": \"fill_with_mean\"\n            }\n        }\n    ]\n}\n</code></pre> <p>It's also possible to specify preprocessing rules for all features of a certain type. See Type-Global Preprocessing.</p>"},{"location":"configuration/features/input_features/#common-parameters","title":"Common Parameters","text":""},{"location":"configuration/features/input_features/#missing-value-strategy","title":"Missing Value Strategy","text":"<p>Ludwig allows any input feature to be \"missing\" in the dataset (at both training and prediction time). How missing values are handled by default varies depending on the feature type, but this handling strategy can be configured in <code>preprocessing</code> section of the config for each feature:</p> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_mean\n</code></pre> <p>Options:</p> <ul> <li><code>fill_with_const</code>: Replaces the missing value with a specific value specified with the <code>fill_value</code> parameter.</li> <li><code>fill_with_mode</code>: Replaces the missing values with the most frequent value in the column.</li> <li><code>fill_with_mean</code>: Replaces the missing values with the mean of the values in the column (<code>number</code> features only).</li> <li><code>fill_with_false</code> Replace the missing values with the <code>false</code> value in the column (<code>binary</code> features only).</li> <li><code>bfill</code>: Replaces the missing values with the next valid value from the subsequent rows of the input dataset.</li> <li><code>ffill</code>: Replaces the missing values with the previous valid value from the preceding rows of the input dataset.</li> <li><code>drop_row</code>: Removes the entire row from the dataset if this column is missing.</li> </ul> <p>For output features, the default strategy is always <code>drop_row</code>, as otherwise Ludwig would be forced to \"make up\" the ground truth values being predicted. However, this can also be overridden using the same <code>missing_value_stragegy</code> param if so desired.</p>"},{"location":"configuration/features/input_features/#encoders","title":"Encoders","text":"<p>Each input feature can configure a specific <code>encoder</code> to map input feature values into tensors. For instance, a user might want to encode a <code>sequence</code> feature using a <code>transformer</code> or an <code>image</code> feature using a <code>stacked_cnn</code>. Different data types support different encoders. Check the documentation for specific feature types to see what encoders are supported for that type.</p> <p>All the other parameters besides <code>name</code>, <code>type</code>, and <code>preprocessing</code>, will be passed as parameters to the encoder subsection. Note that each encoder can have different parameters, so extensive documentation for each of the encoders that can be used for a certain data type can be found in each data type's documentation. Here is an example of how to specify a specific encoder config for an input feature:</p> YAMLPython Dict <pre><code>input_features:\n-\nname: text\ntype: text\npreprocessing:\ntokenizer: space\nencoder: type: bert\nreduce_output: null\ntrainable: true\n</code></pre> <pre><code>{\n    \"input_features\": [\n        {\n            \"name\": \"text\",\n            \"type\": \"text\",\n            \"level\": \"word\",\n            \"preprocessing\": {\n                \"word_tokenizer\": \"space\"\n            },\n            \"encoder\": {\n                \"type\": \"bert\",\n                \"reduce_output\": None,\n                \"trainable\": True,\n            }\n        }\n    ]\n}\n</code></pre> <p>Encoders map raw feature values into tensors. These are usually vectors in the case of data types without a temporal / sequential aspect, matrices for when there is a temporal / sequential aspect, or higher rank tensors for when there is a spatial or a spatiotemporal aspect to the input data.</p> <p>Different configurations of the same encoder may return a tensor with different rank, for instance a sequential encoder may return a vector of size <code>h</code> that is either the final vector of a sequence or the result of pooling over the sequence length, or it can return a matrix of size <code>l x h</code> where <code>l</code> is the length of the sequence and <code>h</code> is the hidden dimension if you specify the pooling reduce operation (<code>reduce_output</code>) to be <code>None</code>.  For the sake of simplicity you can imagine the output to be a vector in most of the cases, but there is a <code>reduce_output</code> parameter one can specify to change the default behavior.</p> <p>For first-time users, we recommend starting with the defaults.</p>"},{"location":"configuration/features/input_features/#tying-encoder-weights","title":"Tying encoder weights","text":"<p>An additional feature that Ludwig provides is the option to have tied weights between different encoders. For instance, if my model takes two sentences as input and return the probability of their entailment, I may want to encode both sentences with the same encoder.</p> <p>This is done by specifying the <code>tied</code> parameter of one feature to be the name of another output feature. For example:</p> YAMLPython Dict <pre><code>input_features:\n-\nname: sentence1\ntype: text\n-\nname: sentence2\ntype: text\ntied: sentence1\n</code></pre> <pre><code>{\n    \"input_features\": [\n        {\n            \"name\": \"sentence1\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"sentence2\",\n            \"type\": \"text\",\n            \"tied\": \"sentence1\"\n        }\n    ]\n}\n</code></pre> <p>Specifying a name of a non-existent input feature will result in an error. Also, in order to be able to have tied weights, all encoder parameters have to be identical between the two input features.</p> <p>It's also possible to specify encoder type and encoder related parameters for all features of a certain type. See Type-Global Encoder.</p>"},{"location":"configuration/features/number_features/","title":"\u21c5 Number Features","text":""},{"location":"configuration/features/number_features/#preprocessing","title":"Preprocessing","text":"<p>Number features are directly transformed into a float valued vector of length <code>n</code> (where <code>n</code> is the size of the dataset) and added to the HDF5 with a key that reflects the name of column in the dataset. No additional information about them is available in the JSON metadata file.</p> <pre><code>preprocessing:\nmissing_value_strategy: fill_with_const\nnormalization: zscore\noutlier_strategy: null\noutlier_threshold: 3.0\nfill_value: 0.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a number column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>, <code>fill_with_mean</code>. See Missing Value Strategy for details.</li> <li><code>normalization</code> (default: <code>zscore</code>) : Normalization strategy to use for this number feature. If the value is <code>null</code> no normalization is performed. Options: <code>zscore</code>, <code>minmax</code>, <code>log1p</code>, <code>iq</code>, <code>null</code>. See Normalization for details.</li> <li><code>outlier_strategy</code> (default: <code>null</code>) : Determines how outliers will be handled in the dataset. In most cases, replacing outliers with the column mean (<code>fill_with_mean</code>) will be sufficient, but in others the outliers may be damaging enough to merit dropping the entire row of data (<code>drop_row</code>). In some cases, the best way to handle outliers is to leave them in the data, which is the behavior when this parameter is left as <code>null</code>. Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>, <code>fill_with_mean</code>, <code>null</code>.</li> <li><code>outlier_threshold</code> (default: <code>3.0</code>): Standard deviations from the mean past which a value is considered an outlier. The 3-sigma rule in statistics tells us that when data is normally distributed, 95% of the data will lie within 2 standard deviations of the mean, and greater than 99% of the data will lie within 3 standard deviations of the mean (see: 68\u201395\u201399.7 rule). As such anything farther away than that is highly likely to be an outlier, and may distort the learning process by disproportionately affecting the model.</li> <li><code>fill_value</code> (default: <code>0.0</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all number input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/number_features/#normalization","title":"Normalization","text":"<p>Technique to be used when normalizing the number feature types.</p> <p>Options:</p> <ul> <li><code>null</code>: No normalization is performed.</li> <li><code>zscore</code>: The mean and standard deviation are computed so that values are shifted to have zero mean and 1 standard deviation.</li> <li><code>minmax</code>: The minimum is subtracted from values and the result is divided by difference between maximum and minimum.</li> <li><code>log1p</code>: The value returned is the natural log of 1 plus the original value. Note: <code>log1p</code> is defined only for positive values.</li> <li><code>iq</code>: The median is subtracted from values and the result is divided by the interquartile range (IQR), i.e., the 75th percentile value minus the 25th percentile value. The resulting data has a zero mean and median and a standard deviation of 1. This is useful if your feature has large outliers since the normalization won't be skewed by those values.</li> </ul> <p>The best normalization techniqe to use depends on the distribution of your data, but <code>zscore</code> is a good place to start in many cases.</p>"},{"location":"configuration/features/number_features/#input-features","title":"Input Features","text":"<p>Number features have two encoders. One encoder (<code>passthrough</code>) simply returns the raw numerical values coming from the input placeholders as outputs. Inputs are of size <code>b</code> while outputs are of size <code>b x 1</code> where <code>b</code> is the batch size. The other encoder (<code>dense</code>) passes the raw numerical values through fully connected layers. In this case the inputs of size <code>b</code> are transformed to size <code>b x h</code>.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of the input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example number feature entry in the input features list:</p> <pre><code>name: number_column_name\ntype: number\ntied: null\nencoder: type: dense\n</code></pre> <p>The available encoder parameters:</p> <ul> <li><code>type</code> (default <code>passthrough</code>): the possible values are <code>passthrough</code>, <code>dense</code> and <code>sparse</code>. <code>passthrough</code> outputs the raw integer values unaltered. <code>dense</code> randomly initializes a trainable embedding matrix, <code>sparse</code> uses one-hot encoding.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all number input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/number_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/number_features/#passthrough-encoder","title":"Passthrough Encoder","text":"<pre><code>encoder:\ntype: passthrough\n</code></pre> <p>There are no additional parameters for <code>passthrough</code> encoder.</p>"},{"location":"configuration/features/number_features/#dense-encoder","title":"Dense Encoder","text":"<pre><code>encoder:\ntype: dense\ndropout: 0.0\noutput_size: 256\nnorm: null\nnum_layers: 1\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>output_size</code> (default: <code>256</code>) : Size of the output of the feature.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>num_layers</code> (default: <code>1</code>) : Number of stacked fully connected layers to apply. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> </ul>"},{"location":"configuration/features/number_features/#output-features","title":"Output Features","text":"<p>Number features can be used when a regression needs to be performed. There is only one decoder available for number features: a (potentially empty) stack of fully connected layers, followed by a projection to a single number.</p> <p>Example number output feature using default parameters:</p> <pre><code>name: number_column_name\ntype: number\nreduce_input: sum\ndependencies: []\nreduce_dependencies: sum\nloss:\ntype: mean_squared_error\ndecoder:\ntype: regressor\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Feature Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>loss</code> (default <code>{type: mean_squared_error}</code>): is a dictionary containing a loss <code>type</code>. Options: <code>mean_squared_error</code>, <code>mean_absolute_error</code>, <code>root_mean_squared_error</code>, <code>root_mean_squared_percentage_error</code>. See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"regressor\"}</code>): Decoder for the desired task. Options: <code>regressor</code>. See Decoder for details.</li> </ul>"},{"location":"configuration/features/number_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/number_features/#regressor","title":"Regressor","text":"<pre><code>decoder:\ntype: regressor\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all number output features using the Type-Global Decoder section.</p>"},{"location":"configuration/features/number_features/#loss","title":"Loss","text":""},{"location":"configuration/features/number_features/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<pre><code>loss:\ntype: mean_squared_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/number_features/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<pre><code>loss:\ntype: mean_absolute_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/number_features/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<pre><code>loss:\ntype: root_mean_squared_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/number_features/#root-mean-squared-percentage-error-rmspe","title":"Root Mean Squared Percentage Error (RMSPE)","text":"<pre><code>loss:\ntype: root_mean_squared_percentage_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul> <p>Loss and loss related parameters can also be defined once and applied to all number output features using the Type-Global Loss section.</p>"},{"location":"configuration/features/number_features/#metrics","title":"Metrics","text":"<p>The metrics that are calculated every epoch and are available for number features are <code>mean_squared_error</code>, <code>mean_absolute_error</code>, <code>root_mean_squared_error</code>, <code>root_mean_squared_percentage_error</code> and the <code>loss</code> itself. You can set either of them as <code>validation_metric</code> in the <code>training</code> section of the configuration if you set the <code>validation_field</code> to be the name of a number feature.</p>"},{"location":"configuration/features/output_features/","title":"Output Features (\u2193)","text":"<p>The Ludwig configuration's <code>output_features</code> section has the same structure as <code>input_features</code>, which is a list of feature definitions, each of which contains a <code>name</code> and a <code>type</code>, with optional <code>preprocessing</code>, that users want their model to predict.</p> YAMLPython Dict <pre><code>output_features:\n-\nname: french\ntype: text\n</code></pre> <pre><code>{\n    \"output_features\": [\n        {\n            \"name\": \"french\",\n            \"type\": \"text\",\n        }\n    ]\n}\n</code></pre>"},{"location":"configuration/features/output_features/#decoders","title":"Decoders","text":"<p>Recall Ludwig's butterfly framework:</p> <p></p> <p>Instead of having <code>encoders</code>, output features have <code>decoders</code>. All the other parameters besides <code>name</code> and <code>type</code> will be passed as parameters to the decoder subsection which dictates how to build the output feature's decoder. Like encoders, each decoder can also have different parameters, so we have extensive documentation for each decoder that can be used for a certain data type. This can be found in each data type's documentation.</p> YAMLPython Dict <pre><code>output_features:\n-\nname: french\ntype: text\ndecoder: type: generator\ncell_type: lstm\nnum_layers: 2\nmax_sequence_length: 256\n</code></pre> <pre><code>{\n    \"output_features\": [\n        {\n            \"name\": \"french\",\n            \"type\": \"text\",\n            \"decoder\": {\n                \"type\": \"generator\",\n                \"cell_type\": \"lstm\",\n                \"num_layers\": 2,\n                \"max_sequence_length\": 256\n            }\n        }\n    ]\n}\n</code></pre> <p>Decoders take the output of the combiner as input, process it further, for instance passing it through fully connected layers, and predict values, which are subsequently used to compute loss and evaluation metrics.</p> <p>Decoders have additional parameters, in particular <code>loss</code> that allows you to specify a different loss to optimize for this specific decoder. For instance, number features support both <code>mean_squared_error</code> and <code>mean_absolute_error</code> as losses.</p> <p>Details about the available decoders and losses alongside with the description of all parameters is provided in datatype-specific documentation.</p> <p>It's also possible to specify decoder type and decoder related parameters for all features of a certain type. See Type-Global Decoder.</p>"},{"location":"configuration/features/output_features/#multi-task-learning","title":"Multi-task Learning","text":"<p>In most machine learning tasks you want to predict only one target variable, but in Ludwig users are empowered to specify multiple output features. During training, output features are optimized in a multi-task fashion, using a weighted sum of their losses as a combined loss. Ludwig natively supports multi-task learning.</p> <p>When multiple output features are specified, the loss that is optimized is a weighted sum of the losses of each individual output feature.</p> <p>By default each loss weight is <code>1</code>, but this can be changed by specifying a value for the <code>weight</code> parameter in the <code>loss</code> section of each output feature definition.</p> <p>For example, given a <code>category</code> feature <code>A</code> and <code>number</code> feature <code>B</code>, in order to optimize the loss <code>loss_total = 1.5 * loss_A + 0.8 + loss_B</code> the <code>output_feature</code> section of the configuration should look like:</p> YAMLPython Dict <pre><code>output_features:\n-\nname: A\ntype: category\nloss:\nweight: 1.5\n-\nname: A\ntype: number\nloss:\nweight: 0.8\n</code></pre> <pre><code>{\n    \"output_features\": [\n        {\n            \"name\": \"A\",\n            \"type\": \"category\",\n            \"loss\": {\n                \"weight\": 1.5\n            }\n        },\n        {\n            \"name\": \"A\",\n            \"type\": \"number\",\n            \"loss\": {\n                \"weight\": 0.8\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"configuration/features/output_features/#output-feature-dependencies","title":"Output Feature Dependencies","text":"<p>An additional feature that Ludwig provides is the concept of dependencies between <code>output_features</code>.</p> <p>Sometimes output features have strong causal relationships, and knowing which prediction has been made for one can improve the prediction for the other. For example, if there are two output features: 1) coarse grained category and 2) fine-grained category, knowing the prediction made for coarse grained can productively clarify the possible choices for the fine-grained category.</p> <p>Output feature dependencies are declared in the feature definition. For example:</p> YAMLPython Dict <pre><code>output_features:\n-\nname: coarse_class\ntype: category\ndecoder:\nnum_fc_layers: 2\noutput_size: 64\n-\nname: fine_class\ntype: category\ndependencies:\n- coarse_class\ndecoder:\nnum_fc_layers: 1\noutput_size: 64\n</code></pre> <pre><code>{\n    \"output_features\": [\n        {\n            \"name\": \"coarse_class\",\n            \"type\": \"category\",\n            \"decoder\": {\n                \"num_fc_layers\": 2,\n                \"output_size\": 64\n            }\n        },\n        {\n            \"name\": \"fine_class\",\n            \"type\": \"category\",\n            \"dependencies\": [\n                \"coarse_class\"\n            ],\n            \"decoder\": {\n                \"num_fc_layers\": 1,\n                \"output_size\": 64\n            }\n        }\n    ]\n}\n</code></pre> <p>At model building time Ludwig checks that no cyclic dependency exists.</p> <p>For the downstream feature, Ludwig will concatenate all the final representations before the prediction of any dependent output features to feed as input to the downstream feature's decoder1</p> <ol> <li> <p>Assuming the input coming from the combiner has hidden dimension <code>h</code> 128, there are two fully connected layers that return a vector with hidden size 64 at the end of the <code>coarse_class</code> decoder (that vector will be used for the final layer before projecting in the output <code>coarse_class</code> space). In the decoder of <code>fine_class</code>, the 64 dimensional vector of <code>coarse_class</code> will be concatenated with the combiner output vector, making a vector of hidden size 192 that will be passed through a fully connected layer and the 64 dimensional output will be used for the final layer before projecting in the output class space of the <code>fine_class</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"configuration/features/sequence_features/","title":"\u21c5 Sequence Features","text":""},{"location":"configuration/features/sequence_features/#preprocessing","title":"Preprocessing","text":"<p>Sequence features are transformed into an integer valued matrix of size <code>n x l</code> (where <code>n</code> is the number of rows and <code>l</code> is the minimum of the length of the longest sequence and a <code>max_sequence_length</code> parameter) and added to HDF5 with a key that reflects the name of column in the dataset. Each sequence in mapped to a list of integers internally. First, a tokenizer converts each sequence to a list of tokens (default tokenization is done by splitting on spaces). Next, a dictionary is constructed which maps each unique token to its frequency in the dataset column. Tokens are ranked by frequency and a sequential integer ID is assigned from the most frequent to the most rare. Ludwig uses <code>&lt;PAD&gt;</code>, <code>&lt;UNK&gt;</code>, <code>&lt;SOS&gt;</code>, and <code>&lt;EOS&gt;</code> special symbols for padding, unknown, start, and end, consistent with common NLP deep learning practices. Special symbols can also be set manually in the preprocessing config. The column name is added to the JSON file, with an associated dictionary containing</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the maximum length of all sequences (<code>max_sequence_length</code>)</li> <li>additional preprocessing information (how to fill missing values and what token to use to fill missing values)</li> </ol> <pre><code>preprocessing:\ntokenizer: space\nmax_sequence_length: 256\nmissing_value_strategy: fill_with_const\nmost_common: 20000\nlowercase: false\nngram_size: 2\npadding_symbol: &lt;PAD&gt;\nunknown_symbol: &lt;UNK&gt;\npadding: right\nfill_value: &lt;UNK&gt;\ncache_encoder_embeddings: false\nvocab_file: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>tokenizer</code> (default: <code>space</code>) : Defines how to map from the raw string content of the dataset column to a sequence of elements.</li> <li><code>max_sequence_length</code> (default: <code>256</code>) : The maximum length (number of tokens) of the text. Texts that are longer than this value will be truncated, while texts that are shorter will be padded.</li> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a text column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>most_common</code> (default: <code>20000</code>): The maximum number of most common tokens in the vocabulary. If the data contains more than this amount, the most infrequent symbols will be treated as unknown.</li> <li><code>lowercase</code> (default: <code>false</code>): If true, converts the string to lowercase before tokenizing. Options: <code>true</code>, <code>false</code>.</li> <li><code>ngram_size</code> (default: <code>2</code>): The size of the ngram when using the <code>ngram</code> tokenizer (e.g, 2 = bigram, 3 = trigram, etc.).</li> <li><code>padding_symbol</code> (default: <code>&lt;PAD&gt;</code>): The string used as a padding symbol. This special token is mapped to the integer ID 0 in the vocabulary.</li> <li><code>unknown_symbol</code> (default: <code>&lt;UNK&gt;</code>): The string used as an unknown placeholder. This special token is mapped to the integer ID 1 in the vocabulary.</li> <li><code>padding</code> (default: <code>right</code>): The direction of the padding. Options: <code>left</code>, <code>right</code>.</li> <li><code>fill_value</code> (default: <code>&lt;UNK&gt;</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> <li><code>cache_encoder_embeddings</code> (default: <code>false</code>): Compute encoder embeddings in preprocessing, speeding up training time considerably. Options: <code>true</code>, <code>false</code>.</li> <li><code>vocab_file</code> (default: <code>null</code>): Filepath string to a UTF-8 encoded file containing the sequence's vocabulary. On each line the first string until      or   is considered a word.</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all sequence input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/sequence_features/#input-features","title":"Input Features","text":"<p>Sequence features have several encoders and each of them has its own parameters. Inputs are of size <code>b</code> while outputs are of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the dimensionality of the output of the encoder. In case a representation for each element of the sequence is needed (for example for tagging them, or for using an attention mechanism), one can specify the parameter <code>reduce_output</code> to be <code>null</code> and the output will be a <code>b x s x h</code> tensor where <code>s</code> is the length of the sequence. Some encoders, because of their inner workings, may require additional parameters to be specified in order to obtain one representation for each element of the sequence. For instance the <code>parallel_cnn</code> encoder by default pools and flattens the sequence dimension and then passes the flattened vector through fully connected layers, so in order to obtain the full sequence tensor one has to specify <code>reduce_output: null</code>.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example sequence feature entry in the input features list:</p> <pre><code>name: sequence_column_name\ntype: sequence\ntied: null\nencoder: type: stacked_cnn\n</code></pre> <p>The available encoder parameters:</p> <ul> <li><code>type</code> (default <code>parallel_cnn</code>): the name of the encoder to use to encode the sequence, one of <code>embed</code>, <code>parallel_cnn</code>, <code>stacked_cnn</code>, <code>stacked_parallel_cnn</code>, <code>rnn</code>, <code>cnnrnn</code>, <code>transformer</code> and <code>passthrough</code> (equivalent to <code>null</code> or <code>None</code>).</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all sequence input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/sequence_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/sequence_features/#embed-encoder","title":"Embed Encoder","text":"<p>The embed encoder simply maps each token in the input sequence to an embedding, creating a <code>b x s x h</code> tensor where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>h</code> is the embedding size. The tensor is reduced along the <code>s</code> dimension to obtain a single vector of size <code>h</code> for each element of the batch. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 256\nrepresentation: dense\nweights_initializer: uniform\nreduce_output: sum\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>weights_initializer</code> (default: <code>uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#parallel-cnn-encoder","title":"Parallel CNN Encoder","text":"<p>The parallel cnn encoder is inspired by Yoon Kim's Convolutional Neural Network for Sentence Classification. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a number of parallel 1d convolutional layers with different filter size (by default 4 layers with filter size 2, 3, 4 and 5), followed by max pooling and concatenation. This single vector concatenating the outputs of the parallel convolutional layers is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: parallel_cnn\ndropout: 0.0\nembedding_size: 256\nnum_conv_layers: null\noutput_size: 256\nactivation: relu\nfilter_size: 3\nnorm: null\nrepresentation: dense\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nconv_layers: null\npool_function: max\npool_size: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\npretrained_embeddings: null\nnum_filters: 256\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> <li><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</li> </ul>"},{"location":"configuration/features/sequence_features/#stacked-cnn-encoder","title":"Stacked CNN Encoder","text":"<p>The stacked cnn encoder is inspired by Xiang Zhang at all's Character-level Convolutional Networks for Text Classification. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of 1d convolutional layers with different filter size (by default 6 layers with filter size 7, 7, 3, 3, 3 and 3), followed by an optional final pool and by a flatten operation. This single flatten vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify the <code>pool_size</code> of all your <code>conv_layers</code> to be <code>null</code>  and <code>reduce_output: null</code>, while if <code>pool_size</code> has a value different from <code>null</code> and <code>reduce_output: null</code> the returned tensor will be of shape <code>b x s' x h</code>, where <code>s'</code> is width of the output of the last convolutional layer.</p> <pre><code>encoder:\ntype: stacked_cnn\ndropout: 0.0\nnum_conv_layers: null\nembedding_size: 256\noutput_size: 256\nactivation: relu\nfilter_size: 3\nstrides: 1\nnorm: null\nrepresentation: dense\nconv_layers: null\npool_function: max\npool_size: null\ndilation_rate: 1\npool_strides: null\npool_padding: same\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nnum_filters: 256\npadding: same\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>strides</code> (default: <code>1</code>): Stride length of the convolution.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>dilation_rate</code> (default: <code>1</code>): Dilation rate to use for dilated convolution.</li> <li><code>pool_strides</code> (default: <code>null</code>): Factor to scale down.</li> <li><code>pool_padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li> <p><code>padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#stacked-parallel-cnn-encoder","title":"Stacked Parallel CNN Encoder","text":"<p>The stacked parallel cnn encoder is a combination of the Parallel CNN and the Stacked CNN encoders where each layer of the stack is composed of parallel convolutional layers. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of several parallel 1d convolutional layers with different filter size, followed by an optional final pool and by a flatten operation. This single flattened vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: stacked_parallel_cnn\ndropout: 0.0\nembedding_size: 256\noutput_size: 256\nactivation: relu\nfilter_size: 3\nnorm: null\nrepresentation: dense\nnum_stacked_layers: null\npool_function: max\npool_size: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nstacked_layers: null\nnum_filters: 256\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>num_stacked_layers</code> (default: <code>null</code>): If stacked_layers is null, this is the number of elements in the stack of parallel convolutional layers. </li> <li><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>stacked_layers</code> (default: <code>null</code>): a nested list of lists of dictionaries containing the parameters of the stack of parallel convolutional layers. The length of the list determines the number of stacked parallel convolutional layers, length of the sub-lists determines the number of parallel conv layers and the content of each dictionary determines the parameters for a specific layer. </p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#rnn-encoder","title":"RNN Encoder","text":"<p>The rnn encoder works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of recurrent layers (by default 1 layer), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions. If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: rnn\ndropout: 0.0\ncell_type: rnn\nnum_layers: 1\nstate_size: 256\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nrecurrent_dropout: 0.0\nactivation: tanh\nfc_activation: relu\nrecurrent_activation: sigmoid\nrepresentation: dense\nunit_forget_bias: true\nrecurrent_initializer: orthogonal\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nbidirectional: false\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>cell_type</code> (default: <code>rnn</code>) : The type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>gru</code>. For reference about the differences between the cells please refer to torch.nn Recurrent Layers. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of stacked recurrent layers.</li> <li><code>state_size</code> (default: <code>256</code>) : The size of the state of the rnn.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>recurrent_dropout</code> (default: <code>0.0</code>): The dropout rate for the recurrent state</li> <li><code>activation</code> (default: <code>tanh</code>): The default activation function. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>recurrent_activation</code> (default: <code>sigmoid</code>): The activation function to use in the recurrent step Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>unit_forget_bias</code> (default: <code>true</code>): If true, add 1 to the bias of the forget gate at initialization Options: <code>true</code>, <code>false</code>.</li> <li><code>recurrent_initializer</code> (default: <code>orthogonal</code>): The initializer for recurrent matrix weights Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>bidirectional</code> (default: <code>false</code>): If true, two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#cnn-rnn-encoder","title":"CNN RNN Encoder","text":"<p>The <code>cnnrnn</code> encoder works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of convolutional layers (by default 2), that is followed by a stack of recurrent layers (by default 1), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions. If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: cnnrnn\ndropout: 0.0\nconv_dropout: 0.0\ncell_type: rnn\nnum_conv_layers: null\nstate_size: 256\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nrecurrent_dropout: 0.0\nactivation: tanh\nfilter_size: 5\nstrides: 1\nfc_activation: relu\nrecurrent_activation: sigmoid\nconv_activation: relu\nrepresentation: dense\nconv_layers: null\npool_function: max\npool_size: null\ndilation_rate: 1\npool_strides: null\npool_padding: same\nunit_forget_bias: true\nrecurrent_initializer: orthogonal\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nnum_filters: 256\npadding: same\nnum_rec_layers: 1\nbidirectional: false\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>conv_dropout</code> (default: <code>0.0</code>) : The dropout rate for the convolutional layers</li> <li><code>cell_type</code> (default: <code>rnn</code>) : The type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>gru</code>. For reference about the differences between the cells please refer to torch.nn Recurrent Layers. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>state_size</code> (default: <code>256</code>) : The size of the state of the rnn.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>recurrent_dropout</code> (default: <code>0.0</code>): The dropout rate for the recurrent state</li> <li><code>activation</code> (default: <code>tanh</code>): The default activation function to use. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>5</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>strides</code> (default: <code>1</code>): Stride length of the convolution.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>recurrent_activation</code> (default: <code>sigmoid</code>): The activation function to use in the recurrent step Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>conv_activation</code> (default: <code>relu</code>): The default activation function that will be used for each convolutional layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>dilation_rate</code> (default: <code>1</code>): Dilation rate to use for dilated convolution.</li> <li><code>pool_strides</code> (default: <code>null</code>): Factor to scale down.</li> <li><code>pool_padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>unit_forget_bias</code> (default: <code>true</code>): If true, add 1 to the bias of the forget gate at initialization Options: <code>true</code>, <code>false</code>.</li> <li><code>recurrent_initializer</code> (default: <code>orthogonal</code>): The initializer for recurrent matrix weights Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li><code>padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>num_rec_layers</code> (default: <code>1</code>): The number of stacked recurrent layers.</li> <li> <p><code>bidirectional</code> (default: <code>false</code>): If true, two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#transformer-encoder","title":"Transformer Encoder","text":"<p>The <code>transformer</code> encoder implements a stack of transformer blocks, replicating the architecture introduced in the Attention is all you need paper, and adds am optional stack of fully connected layers at the end.</p> <pre><code>encoder:\ntype: transformer\ndropout: 0.1\nnum_layers: 1\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nhidden_size: 256\ntransformer_output_size: 256\nfc_activation: relu\nrepresentation: dense\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nnum_heads: 8\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.1</code>) : The dropout rate for the transformer block. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of transformer layers.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>hidden_size</code> (default: <code>256</code>): The size of the hidden representation within the transformer block. It is usually the same as the embedding_size, but if the two values are different, a projection layer will be added before the first transformer block.</li> <li><code>transformer_output_size</code> (default: <code>256</code>): Size of the fully connected layer after self attention in the transformer block. This is usually the same as hidden_size and embedding_size.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>num_heads</code> (default: <code>8</code>): Number of attention heads in each transformer block.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/sequence_features/#output-features","title":"Output Features","text":"<p>Sequence output features can be used for either tagging (classifying each element of an input sequence) or generation (generating a sequence by sampling from the model). Ludwig provides two sequence decoders named <code>tagger</code> and <code>generator</code> respectively.</p> <p>Example sequence output feature using default parameters:</p> <pre><code>name: seq_column_name\ntype: sequence\nreduce_input: null\ndependencies: []\nreduce_dependencies: sum\nloss:\ntype: softmax_cross_entropy\nconfidence_penalty: 0\nrobust_lambda: 0\nclass_weights: 1\nclass_similarities_temperature: 0\ndecoder: type: generator\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the sequence dimension), <code>last</code> (returns the last vector of the sequence dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Feature Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the sequence dimension), <code>last</code> (returns the last vector of the sequence dimension).</li> <li><code>loss</code> (default <code>{type: softmax_cross_entropy, class_similarities_temperature: 0, class_weights: 1, confidence_penalty: 0, robust_lambda: 0}</code>): is a dictionary containing a loss <code>type</code>. The only available loss <code>type</code> for sequences is <code>softmax_cross_entropy</code>. See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"generator\"}</code>): Decoder for the desired task. Options: <code>generator</code>, <code>tagger</code>. See Decoder for details.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all sequence output features using the Type-Global Decoder section. Loss and loss related parameters can also be defined once in the same way.</p>"},{"location":"configuration/features/sequence_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/sequence_features/#generator","title":"Generator","text":"<p>In the case of <code>generator</code> the decoder is a (potentially empty) stack of fully connected layers, followed by an rnn that generates outputs feeding on its own previous predictions and generates a tensor of size <code>b x s' x c</code>, where <code>b</code> is the batch size, <code>s'</code> is the length of the generated sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy. During training teacher forcing is adopted, meaning the list of targets is provided as both inputs and outputs (shifted by 1), while at evaluation time greedy decoding (generating one token at a time and feeding it as input for the next step) is performed by beam search, using a beam of 1 by default. In general a generator expects a <code>b x h</code> shaped input tensor, where <code>h</code> is a hidden dimension. The <code>h</code> vectors are (after an optional stack of fully connected layers) fed into the rnn generator. One exception is when the generator uses attention, as in that case the expected size of the input tensor is <code>b x s x h</code>, which is the output of a sequence, text or time series input feature without reduced outputs or the output of a sequence-based combiner. If a <code>b x h</code> input is provided to a generator decoder using an rnn with attention instead, an error will be raised during model building.</p> <pre><code>decoder:\ntype: generator\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\ncell_type: gru\nnum_layers: 1\nfc_activation: relu\nreduce_input: sum\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>cell_type</code> (default: <code>gru</code>) : Type of recurrent cell to use. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of stacked recurrent layers.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>reduce_input</code> (default: <code>sum</code>): How to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension) Options: <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>last</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> </ul>"},{"location":"configuration/features/sequence_features/#tagger","title":"Tagger","text":"<p>In the case of <code>tagger</code> the decoder is a (potentially empty) stack of fully connected layers, followed by a projection into a tensor of size <code>b x s x c</code>, where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy. This decoder requires its input to be shaped as <code>b x s x h</code>, where <code>h</code> is a hidden dimension, which is the output of a sequence, text or time series input feature without reduced outputs or the output of a sequence-based combiner. If a <code>b x h</code> input is provided instead, an error will be raised during model building.</p> <pre><code>decoder:\ntype: tagger\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nattention_embedding_size: 256\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_attention: false\nuse_bias: true\nattention_num_heads: 8\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>attention_embedding_size</code> (default: <code>256</code>): The embedding size of the multi-head self attention layer.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_attention</code> (default: <code>false</code>): Whether to apply a multi-head self attention layer before prediction. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>attention_num_heads</code> (default: <code>8</code>): The number of attention heads in the multi-head self attention layer.</li> </ul>"},{"location":"configuration/features/sequence_features/#loss","title":"Loss","text":""},{"location":"configuration/features/sequence_features/#softmax-cross-entropy","title":"Softmax Cross Entropy","text":"<pre><code>loss:\ntype: softmax_cross_entropy\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>class_weights</code> (default: <code>null</code>) : Weights to apply to each class in the loss. If not specified, all classes are weighted equally. The value can be a vector of weights, one for each class, that is multiplied to the loss of the datapoints that have that class as ground truth. It is an alternative to oversampling in case of unbalanced class distribution. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too). Alternatively, the value can be a dictionary with class strings as keys and weights as values, like <code>{class_a: 0.5, class_b: 0.7, ...}</code>.</li> <li><code>robust_lambda</code> (default: <code>0</code>): Replaces the loss with <code>(1 - robust_lambda) * loss + robust_lambda / c</code> where <code>c</code> is the number of classes. Useful in case of noisy labels.</li> <li><code>confidence_penalty</code> (default: <code>0</code>): Penalizes overconfident predictions (low entropy) by adding an additional term that penalizes too confident predictions by adding a <code>a * (max_entropy - entropy) / max_entropy</code> term to the loss, where a is the value of this parameter. Useful in case of noisy labels.</li> <li><code>class_similarities</code> (default: <code>null</code>): If not <code>null</code> it is a <code>c x c</code> matrix in the form of a list of lists that contains the mutual similarity of classes. It is used if <code>class_similarities_temperature</code> is greater than 0. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too).</li> <li><code>class_similarities_temperature</code> (default: <code>0</code>): The temperature parameter of the softmax that is performed on each row of <code>class_similarities</code>. The output of that softmax is used to determine the supervision vector to provide instead of the one hot vector that would be provided otherwise for each datapoint. The intuition behind it is that errors between similar classes are more tolerable than errors between really different classes.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/sequence_features/#metrics","title":"Metrics","text":"<p>The metrics that are calculated every epoch and are available for sequence features are:</p> <ul> <li><code>sequence_accuracy</code> The rate at which the model predicted the correct sequence.</li> <li><code>token_accuracy</code> The number of tokens correctly predicted divided by the total number of tokens in all sequences.</li> <li><code>last_accuracy</code> Accuracy considering only the last element of the sequence. Useful to ensure special end-of-sequence tokens are generated or tagged.</li> <li><code>edit_distance</code> Levenshtein distance: the minimum number of single-token edits (insertions, deletions or substitutions) required to change predicted sequence to ground truth.</li> <li><code>perplexity</code> Perplexity is the inverse of the predicted probability of the ground truth sequence, normalized by the number of tokens. The lower the perplexity, the higher the probability of predicting the true sequence.</li> <li><code>loss</code> The value of the loss function.</li> </ul> <p>You can set any of the above as <code>validation_metric</code> in the <code>training</code> section of the configuration if <code>validation_field</code> names a sequence feature.</p>"},{"location":"configuration/features/set_features/","title":"\u21c5 Set Features","text":""},{"location":"configuration/features/set_features/#preprocessing","title":"Preprocessing","text":"<p>Set features are expected to be provided as a string of elements separated by whitespace, e.g. \"elem5 elem9 elem6\". The string values are transformed into a binary (int8 actually) valued matrix of size <code>n x l</code> (where <code>n</code> is the number of rows in the dataset and <code>l</code> is the minimum of the size of the biggest set and a <code>max_size</code> parameter) and added to HDF5 with a key that reflects the name of column in the dataset. The way sets are mapped into integers consists in first using a tokenizer to map each input string to a sequence of set elements (by default this is done by splitting on spaces). Next a dictionary is constructed which maps each unique element to its frequency in the dataset column. Elements are ranked by frequency and a sequential integer ID is assigned in ascending order from the most frequent to the most rare. The column name is added to the JSON file, with an associated dictionary containing</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the maximum size of all sets (<code>max_set_size</code>)</li> <li>additional preprocessing information (by default how to fill missing values and what token to use to fill missing values)</li> </ol> <pre><code>preprocessing:\ntokenizer: space\nmissing_value_strategy: fill_with_const\nlowercase: false\nmost_common: 10000\nfill_value: &lt;UNK&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>tokenizer</code> (default: <code>space</code>) : Defines how to transform the raw text content of the dataset column to a set of elements. The default value space splits the string on spaces. Common options include: underscore (splits on underscore), comma (splits on comma), json (decodes the string into a set or a list through a JSON parser).</li> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a set column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>lowercase</code> (default: <code>false</code>): If true, converts the string to lowercase before tokenizing. Options: <code>true</code>, <code>false</code>.</li> <li><code>most_common</code> (default: <code>10000</code>): The maximum number of most common tokens to be considered. If the data contains more than this amount, the most infrequent tokens will be treated as unknown.</li> <li><code>fill_value</code> (default: <code>&lt;UNK&gt;</code>): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all set input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/set_features/#input-features","title":"Input Features","text":"<p>Set features have one encoder: <code>embed</code>, the raw binary values coming from the input placeholders are first transformed to sparse integer lists, then they are mapped to either dense or sparse embeddings (one-hot encodings), finally they are reduced on the sequence dimension and returned as an aggregated embedding vector. Inputs are of size <code>b</code> while outputs are of size <code>b x h</code> where <code>b</code> is the batch size and <code>h</code> is the dimensionality of the embeddings.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <pre><code>name: set_column_name\ntype: set\ntied: null\nencoder: type: embed\n</code></pre> <p>Encoder type and encoder parameters can also be defined once and applied to all set input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/set_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/set_features/#embed-encoder","title":"Embed Encoder","text":"<pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 50\noutput_size: 10\nactivation: relu\nnorm: null\nrepresentation: dense\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nnorm_params: null\nnum_fc_layers: 0\nfc_layers: null\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout probability for the embedding.</li> <li><code>embedding_size</code> (default: <code>50</code>) : The maximum embedding size, the actual size will be min(vocabulary_size, embedding_size) for dense representations and exactly vocabulary_size for the sparse encoding, where vocabulary_size is the number of different strings appearing in the training set in the input column (plus 1 for the unknown token placeholder ). <li><code>output_size</code> (default: <code>10</code>) : If output_size is not already specified in fc_layers this is the default output_size that will be used for each layer. It indicates the size of the output of a fully connected layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>. See Normalization for details.</li> <li><code>representation</code> (default: <code>dense</code>): The representation of the embedding. Either dense or sparse. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer to use for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer to use for the weights matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If true embeddings are trained during the training process, if false embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when representation is dense as sparse one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>): This is the number of stacked fully connected layers that the input to the feature passes through. Their output is projected in the feature's output space.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): By default dense embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if representation is dense.</p> </li>"},{"location":"configuration/features/set_features/#output-features","title":"Output Features","text":"<p>Set features can be used when multi-label classification needs to be performed. There is only one decoder available for set features: a (potentially empty) stack of fully connected layers, followed by a projection into a vector of size of the number of available classes, followed by a sigmoid.</p> <pre><code>name: set_column_name\ntype: set\nreduce_input: sum\ndependencies: []\nreduce_dependencies: sum\nloss:\ntype: sigmoid_cross_entropy\ndecoder:\ntype: classifier\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Feature Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>loss</code> (default <code>{type: sigmoid_cross_entropy}</code>): is a dictionary containing a loss <code>type</code>. The only supported loss <code>type</code> for set features is <code>sigmoid_cross_entropy</code>. See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"classifier\"}</code>): Decoder for the desired task. Options: <code>classifier</code>. See Decoder for details.</li> </ul>"},{"location":"configuration/features/set_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/set_features/#classifier","title":"Classifier","text":"<pre><code>decoder:\ntype: classifier\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all set output features using the Type-Global Decoder section.</p>"},{"location":"configuration/features/set_features/#loss","title":"Loss","text":""},{"location":"configuration/features/set_features/#sigmoid-cross-entropy","title":"Sigmoid Cross Entropy","text":"<pre><code>loss:\ntype: sigmoid_cross_entropy\nclass_weights: null\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>class_weights</code> (default: <code>null</code>) : Weights to apply to each class in the loss. If not specified, all classes are weighted equally. The value can be a vector of weights, one for each class, that is multiplied to the loss of the datapoints that have that class as ground truth. It is an alternative to oversampling in case of unbalanced class distribution. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too). Alternatively, the value can be a dictionary with class strings as keys and weights as values, like <code>{class_a: 0.5, class_b: 0.7, ...}</code>.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul> <p>Loss type and loss related parameters can also be defined once and applied to all set output features using the Type-Global Loss section.</p>"},{"location":"configuration/features/set_features/#metrics","title":"Metrics","text":"<p>The metrics that are calculated every epoch and are available for set features are <code>jaccard</code> (counts the number of elements in the intersection of prediction and label divided by number of elements in the union) and the <code>loss</code> itself. You can set either of them as <code>validation_metric</code> in the <code>training</code> section of the configuration if you set the <code>validation_field</code> to be the name of a set feature.</p>"},{"location":"configuration/features/supported_data_types/","title":"Supported Data Types","text":"<p>Ludwig supports a variety of data types. A subset can be used as output features.</p> Type Supported Input Feature Supported Output Feature binary \u2705 \u2705 number \u2705 \u2705 category \u2705 \u2705 bag \u2705 \u2705 set \u2705 \u2705 sequence \u2705 \u2705 text \u2705 \u2705 vector \u2705 \u2705 audio \u2705 date \u2705 h3 \u2705 image \u2705 timeseries \u2705"},{"location":"configuration/features/text_features/","title":"\u21c5 Text Features","text":""},{"location":"configuration/features/text_features/#preprocessing","title":"Preprocessing","text":"<p>Text features are an extension of sequence features. Text inputs are processed by a tokenizer which maps the raw text input into a sequence of tokens. An integer id is assigned to each unique token. Using this mapping, each text string is converted first to a sequence of tokens, and next to a sequence of integers.</p> <p>The list of tokens and their integer representations (vocabulary) is stored in the metadata of the model. In the case of a text output feature, this same mapping is used to post-process predictions to text.</p> <pre><code>preprocessing:\ntokenizer: space_punct\nmax_sequence_length: 256\nmissing_value_strategy: fill_with_const\nmost_common: 20000\nlowercase: true\nngram_size: 2\npadding_symbol: &lt;PAD&gt;\nunknown_symbol: &lt;UNK&gt;\npadding: right\nfill_value: &lt;UNK&gt;\ncache_encoder_embeddings: false\nvocab_file: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>tokenizer</code> (default: <code>space_punct</code>) : Defines how to map from the raw string content of the dataset column to a sequence of elements. Options: <code>space</code>, <code>space_punct</code>, <code>ngram</code>, <code>characters</code>, <code>underscore</code>, <code>comma</code>, <code>untokenized</code>, <code>stripped</code>, <code>english_tokenize</code>, <code>english_tokenize_filter</code>, <code>english_tokenize_remove_stopwords</code>, <code>english_lemmatize</code>, <code>english_lemmatize_filter</code>, <code>english_lemmatize_remove_stopwords</code>, <code>italian_tokenize</code>, <code>italian_tokenize_filter</code>, <code>italian_tokenize_remove_stopwords</code>, <code>italian_lemmatize</code>, <code>italian_lemmatize_filter</code>, <code>italian_lemmatize_remove_stopwords</code>, <code>spanish_tokenize</code>, <code>spanish_tokenize_filter</code>, <code>spanish_tokenize_remove_stopwords</code>, <code>spanish_lemmatize</code>, <code>spanish_lemmatize_filter</code>, <code>spanish_lemmatize_remove_stopwords</code>, <code>german_tokenize</code>, <code>german_tokenize_filter</code>, <code>german_tokenize_remove_stopwords</code>, <code>german_lemmatize</code>, <code>german_lemmatize_filter</code>, <code>german_lemmatize_remove_stopwords</code>, <code>french_tokenize</code>, <code>french_tokenize_filter</code>, <code>french_tokenize_remove_stopwords</code>, <code>french_lemmatize</code>, <code>french_lemmatize_filter</code>, <code>french_lemmatize_remove_stopwords</code>, <code>portuguese_tokenize</code>, <code>portuguese_tokenize_filter</code>, <code>portuguese_tokenize_remove_stopwords</code>, <code>portuguese_lemmatize</code>, <code>portuguese_lemmatize_filter</code>, <code>portuguese_lemmatize_remove_stopwords</code>, <code>dutch_tokenize</code>, <code>dutch_tokenize_filter</code>, <code>dutch_tokenize_remove_stopwords</code>, <code>dutch_lemmatize</code>, <code>dutch_lemmatize_filter</code>, <code>dutch_lemmatize_remove_stopwords</code>, <code>greek_tokenize</code>, <code>greek_tokenize_filter</code>, <code>greek_tokenize_remove_stopwords</code>, <code>greek_lemmatize</code>, <code>greek_lemmatize_filter</code>, <code>greek_lemmatize_remove_stopwords</code>, <code>norwegian_tokenize</code>, <code>norwegian_tokenize_filter</code>, <code>norwegian_tokenize_remove_stopwords</code>, <code>norwegian_lemmatize</code>, <code>norwegian_lemmatize_filter</code>, <code>norwegian_lemmatize_remove_stopwords</code>, <code>lithuanian_tokenize</code>, <code>lithuanian_tokenize_filter</code>, <code>lithuanian_tokenize_remove_stopwords</code>, <code>lithuanian_lemmatize</code>, <code>lithuanian_lemmatize_filter</code>, <code>lithuanian_lemmatize_remove_stopwords</code>, <code>danish_tokenize</code>, <code>danish_tokenize_filter</code>, <code>danish_tokenize_remove_stopwords</code>, <code>danish_lemmatize</code>, <code>danish_lemmatize_filter</code>, <code>danish_lemmatize_remove_stopwords</code>, <code>polish_tokenize</code>, <code>polish_tokenize_filter</code>, <code>polish_tokenize_remove_stopwords</code>, <code>polish_lemmatize</code>, <code>polish_lemmatize_filter</code>, <code>polish_lemmatize_remove_stopwords</code>, <code>romanian_tokenize</code>, <code>romanian_tokenize_filter</code>, <code>romanian_tokenize_remove_stopwords</code>, <code>romanian_lemmatize</code>, <code>romanian_lemmatize_filter</code>, <code>romanian_lemmatize_remove_stopwords</code>, <code>japanese_tokenize</code>, <code>japanese_tokenize_filter</code>, <code>japanese_tokenize_remove_stopwords</code>, <code>japanese_lemmatize</code>, <code>japanese_lemmatize_filter</code>, <code>japanese_lemmatize_remove_stopwords</code>, <code>chinese_tokenize</code>, <code>chinese_tokenize_filter</code>, <code>chinese_tokenize_remove_stopwords</code>, <code>chinese_lemmatize</code>, <code>chinese_lemmatize_filter</code>, <code>chinese_lemmatize_remove_stopwords</code>, <code>multi_tokenize</code>, <code>multi_tokenize_filter</code>, <code>multi_tokenize_remove_stopwords</code>, <code>multi_lemmatize</code>, <code>multi_lemmatize_filter</code>, <code>multi_lemmatize_remove_stopwords</code>, <code>sentencepiece</code>, <code>clip</code>, <code>gpt2bpe</code>, <code>bert</code>, <code>hf_tokenizer</code>.</li> <li><code>max_sequence_length</code> (default: <code>256</code>) : The maximum length (number of tokens) of the text. Texts that are longer than this value will be truncated, while texts that are shorter will be padded.</li> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a text column. Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>most_common</code> (default: <code>20000</code>): The maximum number of most common tokens in the vocabulary. If the data contains more than this amount, the most infrequent symbols will be treated as unknown.</li> <li><code>lowercase</code> (default: <code>true</code>): If true, converts the string to lowercase before tokenizing. Options: <code>true</code>, <code>false</code>.</li> <li><code>ngram_size</code> (default: <code>2</code>): The size of the ngram when using the <code>ngram</code> tokenizer (e.g, 2 = bigram, 3 = trigram, etc.).</li> <li><code>padding_symbol</code> (default: <code>&lt;PAD&gt;</code>): The string used as the padding symbol for sequence features. Ignored for features using huggingface encoders, which have their own vocabulary.</li> <li><code>unknown_symbol</code> (default: <code>&lt;UNK&gt;</code>): The string used as the unknown symbol for sequence features. Ignored for features using huggingface encoders, which have their own vocabulary.</li> <li><code>padding</code> (default: <code>right</code>): The direction of the padding. Options: <code>left</code>, <code>right</code>.</li> <li><code>fill_value</code> (default: <code>&lt;UNK&gt;</code>): The value to replace missing values with in case the <code>missing_value_strategy</code> is <code>fill_with_const</code>.</li> <li> <p><code>cache_encoder_embeddings</code> (default: <code>false</code>): For pretrained encoders, compute encoder embeddings in preprocessing, speeding up training time considerably. Only supported when <code>encoder.trainable=false</code>. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>vocab_file</code> (default: <code>null</code>): Filepath string to a UTF-8 encoded file containing the sequence's vocabulary. On each line the first string until <code>\\t</code> or <code>\\n</code> is considered a word.</p> </li> </ul> <p>Preprocessing parameters can also be defined once and applied to all text input features using the Type-Global Preprocessing section.</p> <p>Note</p> <p>If a text feature's encoder specifies a huggingface model, then the tokenizer for that model will be used automatically.</p>"},{"location":"configuration/features/text_features/#input-features","title":"Input Features","text":"<p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example text feature entry in the input features list:</p> <pre><code>name: text_column_name\ntype: text\ntied: null\nencoder: type: bert\ntrainable: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>type</code> (default <code>parallel_cnn</code>): encoder to use for the input text feature. The available encoders include encoders used for Sequence Features as well as pre-trained text encoders from the face transformers library: <code>albert</code>, <code>auto_transformer</code>, <code>bert</code>, <code>camembert</code>, <code>ctrl</code>, <code>distilbert</code>, <code>electra</code>, <code>flaubert</code>, <code>gpt</code>, <code>gpt2</code>, <code>longformer</code>, <code>roberta</code>, <code>t5</code>, <code>mt5</code>, <code>transformer_xl</code>, <code>xlm</code>, <code>xlmroberta</code>, <code>xlnet</code>.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all text input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/text_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/text_features/#embed-encoder","title":"Embed Encoder","text":"<p>The embed encoder simply maps each token in the input sequence to an embedding, creating a <code>b x s x h</code> tensor where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>h</code> is the embedding size. The tensor is reduced along the <code>s</code> dimension to obtain a single vector of size <code>h</code> for each element of the batch. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: embed\ndropout: 0.0\nembedding_size: 256\nrepresentation: dense\nweights_initializer: uniform\nreduce_output: sum\nembeddings_on_cpu: false\nembeddings_trainable: true\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>weights_initializer</code> (default: <code>uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#parallel-cnn-encoder","title":"Parallel CNN Encoder","text":"<p>The parallel cnn encoder is inspired by Yoon Kim's Convolutional Neural Network for Sentence Classification. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a number of parallel 1d convolutional layers with different filter size (by default 4 layers with filter size 2, 3, 4 and 5), followed by max pooling and concatenation. This single vector concatenating the outputs of the parallel convolutional layers is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: parallel_cnn\ndropout: 0.0\nembedding_size: 256\nnum_conv_layers: null\noutput_size: 256\nactivation: relu\nfilter_size: 3\nnorm: null\nrepresentation: dense\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nconv_layers: null\npool_function: max\npool_size: null\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\npretrained_embeddings: null\nnum_filters: 256\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> <li><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</li> </ul>"},{"location":"configuration/features/text_features/#stacked-cnn-encoder","title":"Stacked CNN Encoder","text":"<p>The stacked cnn encoder is inspired by Xiang Zhang at all's Character-level Convolutional Networks for Text Classification. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of 1d convolutional layers with different filter size (by default 6 layers with filter size 7, 7, 3, 3, 3 and 3), followed by an optional final pool and by a flatten operation. This single flatten vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify the <code>pool_size</code> of all your <code>conv_layers</code> to be <code>null</code>  and <code>reduce_output: null</code>, while if <code>pool_size</code> has a value different from <code>null</code> and <code>reduce_output: null</code> the returned tensor will be of shape <code>b x s' x h</code>, where <code>s'</code> is width of the output of the last convolutional layer.</p> <pre><code>encoder:\ntype: stacked_cnn\ndropout: 0.0\nnum_conv_layers: null\nembedding_size: 256\noutput_size: 256\nactivation: relu\nfilter_size: 3\nstrides: 1\nnorm: null\nrepresentation: dense\nconv_layers: null\npool_function: max\npool_size: null\ndilation_rate: 1\npool_strides: null\npool_padding: same\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nnum_filters: 256\npadding: same\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>strides</code> (default: <code>1</code>): Stride length of the convolution.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>dilation_rate</code> (default: <code>1</code>): Dilation rate to use for dilated convolution.</li> <li><code>pool_strides</code> (default: <code>null</code>): Factor to scale down.</li> <li><code>pool_padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li> <p><code>padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#stacked-parallel-cnn-encoder","title":"Stacked Parallel CNN Encoder","text":"<p>The stacked parallel cnn encoder is a combination of the Parallel CNN and the Stacked CNN encoders where each layer of the stack is composed of parallel convolutional layers. It works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of several parallel 1d convolutional layers with different filter size, followed by an optional final pool and by a flatten operation. This single flattened vector is then passed through a stack of fully connected layers and returned as a <code>b x h</code> tensor where <code>h</code> is the output size of the last fully connected layer. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: stacked_parallel_cnn\ndropout: 0.0\nembedding_size: 256\noutput_size: 256\nactivation: relu\nfilter_size: 3\nnorm: null\nrepresentation: dense\nnum_stacked_layers: null\npool_function: max\npool_size: null\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: sum\nnorm_params: null\nnum_fc_layers: null\nfc_layers: null\nstacked_layers: null\nnum_filters: 256\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate applied to the embedding. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>activation</code> (default: <code>relu</code>): The default activation function that will be used for each layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>3</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>norm</code> (default: <code>null</code>): The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>num_stacked_layers</code> (default: <code>null</code>): If stacked_layers is null, this is the number of elements in the stack of parallel convolutional layers. </li> <li><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Parameters used if norm is either <code>batch</code> or <code>layer</code>.</li> <li><code>num_fc_layers</code> (default: <code>null</code>): Number of parallel fully connected layers to use.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters for each fully connected layer.</p> </li> <li> <p><code>stacked_layers</code> (default: <code>null</code>): a nested list of lists of dictionaries containing the parameters of the stack of parallel convolutional layers. The length of the list determines the number of stacked parallel convolutional layers, length of the sub-lists determines the number of parallel conv layers and the content of each dictionary determines the parameters for a specific layer. </p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#rnn-encoder","title":"RNN Encoder","text":"<p>The rnn encoder works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of recurrent layers (by default 1 layer), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions. If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: rnn\ndropout: 0.0\ncell_type: rnn\nnum_layers: 1\nstate_size: 256\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nrecurrent_dropout: 0.0\nactivation: tanh\nfc_activation: relu\nrecurrent_activation: sigmoid\nrepresentation: dense\nunit_forget_bias: true\nrecurrent_initializer: orthogonal\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nbidirectional: false\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>cell_type</code> (default: <code>rnn</code>) : The type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>gru</code>. For reference about the differences between the cells please refer to torch.nn Recurrent Layers. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of stacked recurrent layers.</li> <li><code>state_size</code> (default: <code>256</code>) : The size of the state of the rnn.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>recurrent_dropout</code> (default: <code>0.0</code>): The dropout rate for the recurrent state</li> <li><code>activation</code> (default: <code>tanh</code>): The default activation function. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>recurrent_activation</code> (default: <code>sigmoid</code>): The activation function to use in the recurrent step Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>unit_forget_bias</code> (default: <code>true</code>): If true, add 1 to the bias of the forget gate at initialization Options: <code>true</code>, <code>false</code>.</li> <li><code>recurrent_initializer</code> (default: <code>orthogonal</code>): The initializer for recurrent matrix weights Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>bidirectional</code> (default: <code>false</code>): If true, two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#cnn-rnn-encoder","title":"CNN RNN Encoder","text":"<p>The <code>cnnrnn</code> encoder works by first mapping the input token sequence <code>b x s</code> (where <code>b</code> is the batch size and <code>s</code> is the length of the sequence) into a sequence of embeddings, then it passes the embedding through a stack of convolutional layers (by default 2), that is followed by a stack of recurrent layers (by default 1), followed by a reduce operation that by default only returns the last output, but can perform other reduce functions. If you want to output the full <code>b x s x h</code> where <code>h</code> is the size of the output of the last rnn layer, you can specify <code>reduce_output: null</code>.</p> <pre><code>encoder:\ntype: cnnrnn\ndropout: 0.0\nconv_dropout: 0.0\ncell_type: rnn\nnum_conv_layers: null\nstate_size: 256\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nrecurrent_dropout: 0.0\nactivation: tanh\nfilter_size: 5\nstrides: 1\nfc_activation: relu\nrecurrent_activation: sigmoid\nconv_activation: relu\nrepresentation: dense\nconv_layers: null\npool_function: max\npool_size: null\ndilation_rate: 1\npool_strides: null\npool_padding: same\nunit_forget_bias: true\nrecurrent_initializer: orthogonal\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nnum_filters: 256\npadding: same\nnum_rec_layers: 1\nbidirectional: false\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Dropout rate. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>conv_dropout</code> (default: <code>0.0</code>) : The dropout rate for the convolutional layers</li> <li><code>cell_type</code> (default: <code>rnn</code>) : The type of recurrent cell to use. Available values are: <code>rnn</code>, <code>lstm</code>, <code>gru</code>. For reference about the differences between the cells please refer to torch.nn Recurrent Layers. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_conv_layers</code> (default: <code>null</code>) : The number of stacked convolutional layers when <code>conv_layers</code> is <code>null</code>.</li> <li><code>state_size</code> (default: <code>256</code>) : The size of the state of the rnn.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>recurrent_dropout</code> (default: <code>0.0</code>): The dropout rate for the recurrent state</li> <li><code>activation</code> (default: <code>tanh</code>): The default activation function to use. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>filter_size</code> (default: <code>5</code>): Size of the 1d convolutional filter. It indicates how wide the 1d convolutional filter is.</li> <li><code>strides</code> (default: <code>1</code>): Stride length of the convolution.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>recurrent_activation</code> (default: <code>sigmoid</code>): The activation function to use in the recurrent step Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>conv_activation</code> (default: <code>relu</code>): The default activation function that will be used for each convolutional layer. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li> <p><code>conv_layers</code> (default: <code>null</code>):  A list of dictionaries containing the parameters of all the convolutional layers. The length of the list determines the number of stacked convolutional layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>num_filters</code>, <code>filter_size</code>, <code>strides</code>, <code>padding</code>, <code>dilation_rate</code>, <code>use_bias</code>, <code>pool_function</code>, <code>pool_padding</code>, <code>pool_size</code>, <code>pool_strides</code>, <code>bias_initializer</code>, <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one specified as a parameter of the encoder will be used instead. If both <code>conv_layers</code> and <code>num_conv_layers</code> are <code>null</code>, a default list will be assigned to <code>conv_layers</code> with the value <code>[{filter_size: 7, pool_size: 3}, {filter_size: 7, pool_size: 3}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: null}, {filter_size: 3, pool_size: 3}]</code>.</p> </li> <li> <p><code>pool_function</code> (default: <code>max</code>): Pooling function to use. <code>max</code> will select the maximum value. Any of <code>average</code>, <code>avg</code>, or <code>mean</code> will compute the mean value Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</p> </li> <li><code>pool_size</code> (default: <code>null</code>): The default pool_size that will be used for each layer. If a pool_size is not already specified in conv_layers this is the default pool_size that will be used for each layer. It indicates the size of the max pooling that will be performed along the <code>s</code> sequence dimension after the convolution operation.</li> <li><code>dilation_rate</code> (default: <code>1</code>): Dilation rate to use for dilated convolution.</li> <li><code>pool_strides</code> (default: <code>null</code>): Factor to scale down.</li> <li><code>pool_padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>unit_forget_bias</code> (default: <code>true</code>): If true, add 1 to the bias of the forget gate at initialization Options: <code>true</code>, <code>false</code>.</li> <li><code>recurrent_initializer</code> (default: <code>orthogonal</code>): The initializer for recurrent matrix weights Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>num_filters</code> (default: <code>256</code>): Number of filters, and by consequence number of output channels of the 1d convolution.</p> </li> <li><code>padding</code> (default: <code>same</code>): Padding to use. Options: <code>valid</code>, <code>same</code>.</li> <li><code>num_rec_layers</code> (default: <code>1</code>): The number of stacked recurrent layers.</li> <li> <p><code>bidirectional</code> (default: <code>false</code>): If true, two recurrent networks will perform encoding in the forward and backward direction and their outputs will be concatenated. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#transformer-encoder","title":"Transformer Encoder","text":"<p>The <code>transformer</code> encoder implements a stack of transformer blocks, replicating the architecture introduced in the Attention is all you need paper, and adds am optional stack of fully connected layers at the end.</p> <pre><code>encoder:\ntype: transformer\ndropout: 0.1\nnum_layers: 1\nembedding_size: 256\noutput_size: 256\nnorm: null\nnum_fc_layers: 0\nfc_dropout: 0.0\nhidden_size: 256\ntransformer_output_size: 256\nfc_activation: relu\nrepresentation: dense\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nembeddings_on_cpu: false\nembeddings_trainable: true\nreduce_output: last\nnorm_params: null\nfc_layers: null\nnum_heads: 8\npretrained_embeddings: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.1</code>) : The dropout rate for the transformer block. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of transformer layers.</li> <li><code>embedding_size</code> (default: <code>256</code>) : The maximum embedding size. The actual size will be <code>min(vocabulary_size, embedding_size)</code> for <code>dense</code> representations and exactly <code>vocabulary_size</code> for the <code>sparse</code> encoding, where <code>vocabulary_size</code> is the number of unique strings appearing in the training set input column plus the number of special tokens (<code>&lt;UNK&gt;</code>, <code>&lt;PAD&gt;</code>, <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code>).</li> <li><code>output_size</code> (default: <code>256</code>) : The default output_size that will be used for each layer.</li> <li><code>norm</code> (default: <code>null</code>) : The default norm that will be used for each layer. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of parallel fully connected layers to use. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>hidden_size</code> (default: <code>256</code>): The size of the hidden representation within the transformer block. It is usually the same as the embedding_size, but if the two values are different, a projection layer will be added before the first transformer block.</li> <li><code>transformer_output_size</code> (default: <code>256</code>): Size of the fully connected layer after self attention in the transformer block. This is usually the same as hidden_size and embedding_size.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>representation</code> (default: <code>dense</code>): Representation of the embedding. <code>dense</code> means the embeddings are initialized randomly, <code>sparse</code> means they are initialized to be one-hot encodings. Options: <code>dense</code>, <code>sparse</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether to use a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>embeddings_on_cpu</code> (default: <code>false</code>): Whether to force the placement of the embedding matrix in regular memory and have the CPU resolve them. By default embedding matrices are stored on GPU memory if a GPU is used, as it allows for faster access, but in some cases the embedding matrix may be too large. This parameter forces the placement of the embedding matrix in regular memory and the CPU is used for embedding lookup, slightly slowing down the process as a result of data transfer between CPU and GPU memory. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>embeddings_trainable</code> (default: <code>true</code>): If <code>true</code> embeddings are trained during the training process, if <code>false</code> embeddings are fixed. It may be useful when loading pretrained embeddings for avoiding finetuning them. This parameter has effect only when <code>representation</code> is <code>dense</code>; <code>sparse</code> one-hot encodings are not trainable. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>last</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li> <p><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</p> </li> <li> <p><code>num_heads</code> (default: <code>8</code>): Number of attention heads in each transformer block.</p> </li> <li> <p><code>pretrained_embeddings</code> (default: <code>null</code>): Path to a file containing pretrained embeddings. By default <code>dense</code> embeddings are initialized randomly, but this parameter allows to specify a path to a file containing embeddings in the GloVe format. When the file containing the embeddings is loaded, only the embeddings with labels present in the vocabulary are kept, the others are discarded. If the vocabulary contains strings that have no match in the embeddings file, their embeddings are initialized with the average of all other embedding plus some random noise to make them different from each other. This parameter has effect only if <code>representation</code> is <code>dense</code>.</p> </li> </ul>"},{"location":"configuration/features/text_features/#huggingface-encoders","title":"Huggingface encoders","text":"<p>All huggingface-based text encoders are configured with the following parameters:</p> <ul> <li><code>pretrained_model_name_or_path</code> (default is the huggingface default model path for the specified encoder, i.e. <code>bert-base-uncased</code> for BERT). This can be either the name of a model or a path where it was downloaded. For details on the variants available refer to the Hugging Face documentation.</li> <li><code>reduce_output</code> (default <code>cls_pooled</code>): defines how to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Available values are: <code>cls_pooled</code>, <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension) and  <code>null</code> (which does not reduce and returns the full tensor).</li> <li><code>trainable</code> (default <code>false</code>): if <code>true</code> the weights of the encoder will be trained, otherwise they will be kept frozen.</li> </ul> <p>Note</p> <p>Any hyperparameter of any huggingface encoder can be overridden. Check the huggingface documentation for which parameters are used for which models.</p> <pre><code>name: text_column_name\ntype: text\nencoder: bert\ntrainable: true\nnum_attention_heads: 16 # Instead of 12\n</code></pre>"},{"location":"configuration/features/text_features/#autotransformer","title":"AutoTransformer","text":"<p>The <code>auto_transformer</code> encoder automatically instantiates the model architecture for the specified <code>pretrained_model_name_or_path</code>. Unlike the other HF encoders, <code>auto_transformer</code> does not provide a default value for <code>pretrained_model_name_or_path</code>, this is its only mandatory parameter. See the Hugging Face AutoModels documentation for more details.</p> <pre><code>encoder:\ntype: auto_transformer\npretrained_model_name_or_path: bert-base-uncased\ntrainable: false\nreduce_output: sum\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>pretrained_model_name_or_path</code> (default: <code>bert-base-uncased</code>) : Name or path of the pretrained model.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#albert","title":"ALBERT","text":"<p>The <code>albert</code> encoder loads a pretrained ALBERT (default <code>albert-base-v2</code>) model using the Hugging Face transformers package. Albert is similar to BERT, with significantly lower memory usage and somewhat faster training time:.</p> <pre><code>encoder:\ntype: albert\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: albert-base-v2\nreduce_output: cls_pooled\nembedding_size: 128\nhidden_size: 768\nnum_hidden_layers: 12\nnum_hidden_groups: 1\nnum_attention_heads: 12\nintermediate_size: 3072\ninner_group_num: 1\nhidden_act: gelu_new\nhidden_dropout_prob: 0.0\nattention_probs_dropout_prob: 0.0\nmax_position_embeddings: 512\ntype_vocab_size: 2\ninitializer_range: 0.02\nlayer_norm_eps: 1.0e-12\nclassifier_dropout_prob: 0.1\nposition_embedding_type: absolute\npad_token_id: 0\nbos_token_id: 2\neos_token_id: 3\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>albert-base-v2</code>): Name or path of the pretrained model.</li> <li><code>reduce_output</code> (default: <code>cls_pooled</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>embedding_size</code> (default: <code>128</code>): Dimensionality of vocabulary embeddings.</li> <li><code>hidden_size</code> (default: <code>768</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>num_hidden_layers</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>num_hidden_groups</code> (default: <code>1</code>): Number of groups for the hidden layers, parameters in the same group are shared.</li> <li><code>num_attention_heads</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>intermediate_size</code> (default: <code>3072</code>): The dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</li> <li><code>inner_group_num</code> (default: <code>1</code>): The number of inner repetition of attention and ffn.</li> <li><code>hidden_act</code> (default: <code>gelu_new</code>): The non-linear activation function (function or string) in the encoder and pooler. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li><code>hidden_dropout_prob</code> (default: <code>0.0</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>attention_probs_dropout_prob</code> (default: <code>0.0</code>): The dropout ratio for the attention probabilities.</li> <li><code>max_position_embeddings</code> (default: <code>512</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large (e.g., 512 or 1024 or 2048).</li> <li><code>type_vocab_size</code> (default: <code>2</code>): The vocabulary size of the token_type_ids passed when calling AlbertModel or TFAlbertModel.</li> <li><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</li> <li><code>layer_norm_eps</code> (default: <code>1e-12</code>): The epsilon used by the layer normalization layers.</li> <li><code>classifier_dropout_prob</code> (default: <code>0.1</code>): The dropout ratio for attached classifiers.</li> <li><code>position_embedding_type</code> (default: <code>absolute</code>):  Options: <code>absolute</code>, <code>relative_key</code>, <code>relative_key_query</code>.</li> <li><code>pad_token_id</code> (default: <code>0</code>): The ID of the token to use as padding.</li> <li><code>bos_token_id</code> (default: <code>2</code>): The beginning of sequence token ID.</li> <li><code>eos_token_id</code> (default: <code>3</code>): The end of sequence token ID.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#bert","title":"BERT","text":"<p>The bert encoder loads a pretrained BERT (default bert-base-uncased) model using the Hugging Face transformers package. BERT is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.</p> <pre><code>encoder:\ntype: bert\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: bert-base-uncased\nhidden_dropout_prob: 0.1\nattention_probs_dropout_prob: 0.1\nmax_position_embeddings: 512\nclassifier_dropout: null\nreduce_output: cls_pooled\nhidden_size: 768\nnum_hidden_layers: 12\nnum_attention_heads: 12\nintermediate_size: 3072\nhidden_act: gelu\ntype_vocab_size: 2\ninitializer_range: 0.02\nlayer_norm_eps: 1.0e-12\npad_token_id: 0\ngradient_checkpointing: false\nposition_embedding_type: absolute\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>bert-base-uncased</code>): Name or path of the pretrained model.</li> <li><code>hidden_dropout_prob</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>attention_probs_dropout_prob</code> (default: <code>0.1</code>): The dropout ratio for the attention probabilities.</li> <li><code>max_position_embeddings</code> (default: <code>512</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li><code>classifier_dropout</code> (default: <code>null</code>): The dropout ratio for the classification head.</li> <li><code>reduce_output</code> (default: <code>cls_pooled</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>hidden_size</code> (default: <code>768</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>num_hidden_layers</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>num_attention_heads</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>intermediate_size</code> (default: <code>3072</code>): Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</li> <li><code>hidden_act</code> (default: <code>gelu</code>): The non-linear activation function (function or string) in the encoder and pooler. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li><code>type_vocab_size</code> (default: <code>2</code>): The vocabulary size of the token_type_ids passed when calling BertModel or TFBertModel.</li> <li><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</li> <li><code>layer_norm_eps</code> (default: <code>1e-12</code>): The epsilon used by the layer normalization layers.</li> <li><code>pad_token_id</code> (default: <code>0</code>): The ID of the token to use as padding.</li> <li><code>gradient_checkpointing</code> (default: <code>false</code>): Whether to use gradient checkpointing. Options: <code>true</code>, <code>false</code>.</li> <li><code>position_embedding_type</code> (default: <code>absolute</code>): Type of position embedding. Options: <code>absolute</code>, <code>relative_key</code>, <code>relative_key_query</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#camembert","title":"CamemBERT","text":"<p>The <code>camembert</code> encoder loads a pretrained CamemBERT (default <code>jplu/tf-camembert-base</code>) model using the Hugging Face transformers package. CamemBERT is pre-trained on a large French language web-crawled text corpus.</p> <pre><code>encoder:\ntype: camembert\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: camembert-base\nhidden_dropout_prob: 0.1\nattention_probs_dropout_prob: 0.1\nmax_position_embeddings: 514\nclassifier_dropout: null\nreduce_output: sum\nhidden_size: 768\nhidden_act: gelu\ninitializer_range: 0.02\nnum_hidden_layers: 12\nnum_attention_heads: 12\nintermediate_size: 3072\ntype_vocab_size: 1\nlayer_norm_eps: 1.0e-05\npad_token_id: 1\ngradient_checkpointing: false\nposition_embedding_type: absolute\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>camembert-base</code>): Name or path of the pretrained model.</li> <li><code>hidden_dropout_prob</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>attention_probs_dropout_prob</code> (default: <code>0.1</code>): The dropout ratio for the attention probabilities.</li> <li><code>max_position_embeddings</code> (default: <code>514</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li><code>classifier_dropout</code> (default: <code>null</code>): The dropout ratio for the classification head.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>hidden_size</code> (default: <code>768</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>hidden_act</code> (default: <code>gelu</code>): The non-linear activation function (function or string) in the encoder and pooler. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li> <p><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> </li> <li> <p><code>num_hidden_layers</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</p> </li> <li><code>num_attention_heads</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>intermediate_size</code> (default: <code>3072</code>): Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</li> <li><code>type_vocab_size</code> (default: <code>1</code>): The vocabulary size of the token_type_ids passed when calling BertModel or TFBertModel.</li> <li><code>layer_norm_eps</code> (default: <code>1e-05</code>): The epsilon used by the layer normalization layers.</li> <li><code>pad_token_id</code> (default: <code>1</code>): The ID of the token to use as padding.</li> <li><code>gradient_checkpointing</code> (default: <code>false</code>): Whether to use gradient checkpointing. Options: <code>true</code>, <code>false</code>.</li> <li><code>position_embedding_type</code> (default: <code>absolute</code>): Type of position embedding. Options: <code>absolute</code>, <code>relative_key</code>, <code>relative_key_query</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#distilbert","title":"DistilBERT","text":"<p>The <code>distilbert</code> encoder loads a pretrained DistilBERT (default <code>distilbert-base-uncased</code>) model using the Hugging Face transformers package. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language understanding benchmark.</p> <pre><code>encoder:\ntype: distilbert\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: distilbert-base-uncased\ndropout: 0.1\nmax_position_embeddings: 512\nattention_dropout: 0.1\nactivation: gelu\nreduce_output: sum\ninitializer_range: 0.02\nqa_dropout: 0.1\nseq_classif_dropout: 0.2\nsinusoidal_pos_embds: false\nn_layers: 6\nn_heads: 12\ndim: 768\nhidden_dim: 3072\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>distilbert-base-uncased</code>): Name or path of the pretrained model.</li> <li><code>dropout</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>max_position_embeddings</code> (default: <code>512</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li><code>attention_dropout</code> (default: <code>0.1</code>): The dropout ratio for the attention probabilities.</li> <li><code>activation</code> (default: <code>gelu</code>): The non-linear activation function (function or string) in the encoder and pooler. If string, 'gelu', 'relu', 'silu' and 'gelu_new' are supported. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</li> <li><code>qa_dropout</code> (default: <code>0.1</code>): The dropout probabilities used in the question answering model DistilBertForQuestionAnswering.</li> <li> <p><code>seq_classif_dropout</code> (default: <code>0.2</code>): The dropout probabilities used in the sequence classification and the multiple choice model DistilBertForSequenceClassification.</p> </li> <li> <p><code>sinusoidal_pos_embds</code> (default: <code>false</code>): Whether to use sinusoidal positional embeddings. Options: <code>true</code>, <code>false</code>.</p> </li> <li><code>n_layers</code> (default: <code>6</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>n_heads</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>dim</code> (default: <code>768</code>):  Dimensionality of the encoder layers and the pooler layer.</li> <li><code>hidden_dim</code> (default: <code>3072</code>): The size of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#electra","title":"ELECTRA","text":"<p>The `electra`` encoder loads a pretrained ELECTRA model using the Hugging Face transformers package. ELECTRA is a new pretraining approach which trains two transformer models the generator and the discriminator. The generator\u2019s role is to replace tokens in a sequence, and is therefore trained as a masked language model. The discriminator, which is the model we\u2019re interested in, tries to identify which tokens were replaced by the generator in the sequence.</p> <pre><code>encoder:\ntype: electra\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: google/electra-small-discriminator\nhidden_dropout_prob: 0.1\nattention_probs_dropout_prob: 0.1\nmax_position_embeddings: 512\nclassifier_dropout: null\nreduce_output: sum\nembedding_size: 128\nhidden_size: 256\nhidden_act: gelu\ninitializer_range: 0.02\nnum_hidden_layers: 12\nnum_attention_heads: 4\nintermediate_size: 1024\ntype_vocab_size: 2\nlayer_norm_eps: 1.0e-12\nposition_embedding_type: absolute\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>google/electra-small-discriminator</code>): Name or path of the pretrained model.</li> <li><code>hidden_dropout_prob</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>attention_probs_dropout_prob</code> (default: <code>0.1</code>): The dropout ratio for the attention probabilities.</li> <li><code>max_position_embeddings</code> (default: <code>512</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li><code>classifier_dropout</code> (default: <code>null</code>): The dropout ratio for the classification head.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>embedding_size</code> (default: <code>128</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>hidden_size</code> (default: <code>256</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>hidden_act</code> (default: <code>gelu</code>): The non-linear activation function (function or string) in the encoder and pooler. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li> <p><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> </li> <li> <p><code>num_hidden_layers</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</p> </li> <li><code>num_attention_heads</code> (default: <code>4</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>intermediate_size</code> (default: <code>1024</code>): Dimensionality of the \u201cintermediate\u201d (i.e., feed-forward) layer in the Transformer encoder.</li> <li><code>type_vocab_size</code> (default: <code>2</code>): The vocabulary size of the token_type_ids passed when calling ElectraModel or TFElectraModel.</li> <li><code>layer_norm_eps</code> (default: <code>1e-12</code>): The epsilon used by the layer normalization layers.</li> <li><code>position_embedding_type</code> (default: <code>absolute</code>): Type of position embedding. Options: <code>absolute</code>, <code>relative_key</code>, <code>relative_key_query</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#flaubert","title":"FlauBERT","text":"<p>The <code>flaubert`` encoder loads a pretrained [FlauBERT](https://arxiv.org/abs/1912.05372) (default</code>jplu/tf-flaubert-base-uncased``) model using the Hugging Face transformers package. FlauBERT has an architecture similar to BERT and is pre-trained on a large French language corpus.</p> <pre><code>encoder:\ntype: flaubert\nuse_pretrained: false\ntrainable: false\npretrained_model_name_or_path: flaubert/flaubert_small_cased\ndropout: 0.1\nreduce_output: sum\npre_norm: true\nlayerdrop: 0.2\nemb_dim: 512\nn_layers: 6\nn_heads: 8\nattention_dropout: 0.1\ngelu_activation: true\nsinusoidal_embeddings: false\ncausal: false\nasm: false\nn_langs: 1\nuse_lang_emb: true\nmax_position_embeddings: 512\nembed_init_std: 0.02209708691207961\ninit_std: 0.02\nlayer_norm_eps: 1.0e-06\nbos_index: 0\neos_index: 1\npad_index: 2\nunk_index: 3\nmask_index: 5\nis_encoder: true\nmask_token_id: 0\nlang_id: 0\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>false</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>flaubert/flaubert_small_cased</code>): Name of path of the pretrained model.</li> <li><code>dropout</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>pre_norm</code> (default: <code>true</code>): Whether to apply the layer normalization before or after the feed forward layer following the attention in each layer (Vaswani et al., Tensor2Tensor for Neural Machine Translation. 2018) Options: <code>true</code>, <code>false</code>.</li> <li><code>layerdrop</code> (default: <code>0.2</code>): Probability to drop layers during training (Fan et al., Reducing Transformer Depth on Demand with Structured Dropout. ICLR 2020)</li> <li><code>emb_dim</code> (default: <code>512</code>): Dimensionality of the encoder layers and the pooler layer.</li> <li><code>n_layers</code> (default: <code>6</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>n_heads</code> (default: <code>8</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>attention_dropout</code> (default: <code>0.1</code>): The dropout probability for the attention mechanism</li> <li><code>gelu_activation</code> (default: <code>true</code>): Whether or not to use a gelu activation instead of relu. Options: <code>true</code>, <code>false</code>.</li> <li><code>sinusoidal_embeddings</code> (default: <code>false</code>): Whether or not to use sinusoidal positional embeddings instead of absolute positional embeddings. Options: <code>true</code>, <code>false</code>.</li> <li><code>causal</code> (default: <code>false</code>): Whether or not the model should behave in a causal manner. Causal models use a triangular attention mask in order to only attend to the left-side context instead if a bidirectional context. Options: <code>true</code>, <code>false</code>.</li> <li><code>asm</code> (default: <code>false</code>): Whether or not to use an adaptive log softmax projection layer instead of a linear layer for the prediction layer. Options: <code>true</code>, <code>false</code>.</li> <li><code>n_langs</code> (default: <code>1</code>): The number of languages the model handles. Set to 1 for monolingual models.</li> <li><code>use_lang_emb</code> (default: <code>true</code>): Whether to use language embeddings. Some models use additional language embeddings, see the multilingual models page for information on how to use them. Options: <code>true</code>, <code>false</code>.</li> <li><code>max_position_embeddings</code> (default: <code>512</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li><code>embed_init_std</code> (default: <code>0.02209708691207961</code>): The standard deviation of the truncated_normal_initializer for initializing the embedding matrices.</li> <li><code>init_std</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices except the embedding matrices.</li> <li><code>layer_norm_eps</code> (default: <code>1e-06</code>): The epsilon used by the layer normalization layers.</li> <li><code>bos_index</code> (default: <code>0</code>): The index of the beginning of sentence token in the vocabulary.</li> <li><code>eos_index</code> (default: <code>1</code>): The index of the end of sentence token in the vocabulary.</li> <li><code>pad_index</code> (default: <code>2</code>): The index of the padding token in the vocabulary.</li> <li><code>unk_index</code> (default: <code>3</code>): The index of the unknown token in the vocabulary.</li> <li><code>mask_index</code> (default: <code>5</code>): The index of the masking token in the vocabulary.</li> <li><code>is_encoder</code> (default: <code>true</code>): Whether or not the initialized model should be a transformer encoder or decoder as seen in Vaswani et al. Options: <code>true</code>, <code>false</code>.</li> <li><code>mask_token_id</code> (default: <code>0</code>): Model agnostic parameter to identify masked tokens when generating text in an MLM context.</li> <li><code>lang_id</code> (default: <code>0</code>): The ID of the language used by the model. This parameter is used when generating text in a given language.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#gpt","title":"GPT","text":"<p>The <code>gpt</code> encoder loads a pretrained GPT (default <code>openai-gpt</code>) model using the Hugging Face transformers package. GPT is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies, the Toronto Book Corpus.</p> <pre><code>encoder:\ntype: gpt\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: openai-gpt\nreduce_output: sum\ninitializer_range: 0.02\nn_positions: 40478\nn_ctx: 512\nn_embd: 768\nn_layer: 12\nn_head: 12\nafn: gelu_new\nresid_pdrop: 0.1\nembd_pdrop: 0.1\nattn_pdrop: 0.1\nlayer_norm_epsilon: 1.0e-05\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>openai-gpt</code>): Name or path of the pretrained model.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li> <p><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> </li> <li> <p><code>n_positions</code> (default: <code>40478</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> </li> <li><code>n_ctx</code> (default: <code>512</code>): Dimensionality of the causal mask (usually same as n_positions)</li> <li><code>n_embd</code> (default: <code>768</code>): Dimensionality of the embeddings and hidden states.</li> <li><code>n_layer</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>n_head</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>afn</code> (default: <code>gelu_new</code>): The non-linear activation function (function or string) in the encoder and pooler. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li><code>resid_pdrop</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>embd_pdrop</code> (default: <code>0.1</code>): The dropout ratio for the embeddings.</li> <li><code>attn_pdrop</code> (default: <code>0.1</code>): The dropout ratio for the attention.</li> <li><code>layer_norm_epsilon</code> (default: <code>1e-05</code>): The epsilon to use in the layer normalization layers</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#gpt2","title":"GPT2","text":"<p>The <code>gpt2</code> encoder loads a pretrained GPT-2 (default <code>gpt2</code>) model using the Hugging Face transformers package. GPT-2 is a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.</p> <pre><code>encoder:\ntype: gpt2\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: gpt2\nreduce_output: sum\ninitializer_range: 0.02\nn_positions: 1024\nn_ctx: 1024\nn_embd: 768\nn_layer: 12\nn_head: 12\nn_inner: null\nactivation_function: gelu_new\nresid_pdrop: 0.1\nembd_pdrop: 0.1\nattn_pdrop: 0.1\nlayer_norm_epsilon: 1.0e-05\nscale_attn_weights: true\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>gpt2</code>): Name or path of the pretrained model.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li> <p><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> </li> <li> <p><code>n_positions</code> (default: <code>1024</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> </li> <li><code>n_ctx</code> (default: <code>1024</code>): Dimensionality of the causal mask (usually same as n_positions)</li> <li><code>n_embd</code> (default: <code>768</code>): Dimensionality of the embeddings and hidden states.</li> <li><code>n_layer</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>n_head</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>n_inner</code> (default: <code>null</code>): Dimensionality of the inner feed-forward layers. None will set it to 4 times n_embd</li> <li><code>activation_function</code> (default: <code>gelu_new</code>): Activation function, to be selected in the list ['relu', 'silu', 'gelu', 'tanh', 'gelu_new']. Options: <code>relu</code>, <code>silu</code>, <code>gelu</code>, <code>tanh</code>, <code>gelu_new</code>.</li> <li><code>resid_pdrop</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>embd_pdrop</code> (default: <code>0.1</code>): The dropout ratio for the embeddings.</li> <li><code>attn_pdrop</code> (default: <code>0.1</code>): The dropout ratio for the attention.</li> <li><code>layer_norm_epsilon</code> (default: <code>1e-05</code>): The epsilon to use in the layer normalization layers.</li> <li><code>scale_attn_weights</code> (default: <code>true</code>): Scale attention weights by dividing by sqrt(hidden_size). Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#longformer","title":"Longformer","text":"<p>The <code>longformer</code> encoder loads a pretrained Longformer (default <code>allenai/longformer-base-4096</code>) model using the Hugging Face transformers package. Longformer is a good choice for longer text, as it supports sequences up to 4096 tokens long.</p> <pre><code>encoder:\ntype: longformer\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: allenai/longformer-base-4096\nmax_position_embeddings: 4098\nreduce_output: cls_pooled\nattention_window: 512\nsep_token_id: 2\ntype_vocab_size: 1\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>allenai/longformer-base-4096</code>): Name or path of the pretrained model.</li> <li><code>max_position_embeddings</code> (default: <code>4098</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li> <p><code>reduce_output</code> (default: <code>cls_pooled</code>): The method used to reduce a sequence of tensors down to a single tensor.</p> </li> <li> <p><code>attention_window</code> (default: <code>512</code>): Size of an attention window around each token. If an int, use the same size for all layers. To specify a different window size for each layer, use a List[int] where len(attention_window) == num_hidden_layers.</p> </li> <li> <p><code>sep_token_id</code> (default: <code>2</code>): ID of the separator token, which is used when building a sequence from multiple sequences</p> </li> <li> <p><code>type_vocab_size</code> (default: <code>1</code>): The vocabulary size of the token_type_ids passed when calling LongformerEncoder</p> </li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#roberta","title":"RoBERTa","text":"<p>The <code>roberta</code> encoder loads a pretrained RoBERTa (default <code>roberta-base</code>) model using the Hugging Face transformers package. Replication of BERT pretraining which may match or exceed the performance of BERT. RoBERTa builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.</p> <pre><code>encoder:\ntype: roberta\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: roberta-base\nreduce_output: cls_pooled\neos_token_id: 2\npad_token_id: 1\nbos_token_id: 0\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>roberta-base</code>): Name or path of the pretrained model.</li> <li><code>reduce_output</code> (default: <code>cls_pooled</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li> <p><code>eos_token_id</code> (default: <code>2</code>): The end of sequence token ID.</p> </li> <li> <p><code>pad_token_id</code> (default: <code>1</code>): The ID of the token to use as padding.</p> </li> <li><code>bos_token_id</code> (default: <code>0</code>): The beginning of sequence token ID.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#t5","title":"T5","text":"<p>The <code>t5</code> encoder loads a pretrained T5 (default <code>t5-small</code>) model using the Hugging Face transformers package. T5 (Text-to-Text Transfer Transformer) is pre-trained on a huge text dataset crawled from the web and shows good transfer performance on multiple tasks.</p> <pre><code>encoder:\ntype: t5\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: t5-small\nnum_layers: 6\ndropout_rate: 0.1\nreduce_output: sum\nd_ff: 2048\nd_model: 512\nd_kv: 64\nnum_decoder_layers: 6\nnum_heads: 8\nrelative_attention_num_buckets: 32\nlayer_norm_eps: 1.0e-06\ninitializer_factor: 1\nfeed_forward_proj: relu\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>t5-small</code>): Name or path of the pretrained model.</li> <li><code>num_layers</code> (default: <code>6</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>dropout_rate</code> (default: <code>0.1</code>): The ratio for all dropout layers.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li> <p><code>d_ff</code> (default: <code>2048</code>): Size of the intermediate feed forward layer in each T5Block.</p> </li> <li> <p><code>d_model</code> (default: <code>512</code>): Size of the encoder layers and the pooler layer.</p> </li> <li><code>d_kv</code> (default: <code>64</code>): Size of the key, query, value projections per attention head. d_kv has to be equal to d_model // num_heads.</li> <li><code>num_decoder_layers</code> (default: <code>6</code>): Number of hidden layers in the Transformer decoder. Will use the same value as num_layers if not set.</li> <li><code>num_heads</code> (default: <code>8</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>relative_attention_num_buckets</code> (default: <code>32</code>): The number of buckets to use for each attention layer.</li> <li><code>layer_norm_eps</code> (default: <code>1e-06</code>): The epsilon used by the layer normalization layers.</li> <li><code>initializer_factor</code> (default: <code>1</code>): A factor for initializing all weight matrices (should be kept to 1, used internally for initialization testing).</li> <li><code>feed_forward_proj</code> (default: <code>relu</code>): Type of feed forward layer to be used. Should be one of 'relu' or 'gated-gelu'. T5v1.1 uses the 'gated-gelu' feed forward projection. Original T5 uses 'relu'. Options: <code>relu</code>, <code>gated-gelu</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#transformerxl","title":"TransformerXL","text":"<p>The <code>transformer_xl</code> encoder loads a pretrained Transformer-XL (default <code>transfo-xl-wt103</code>) model using the Hugging Face transformers package. Adds novel positional encoding scheme which improves understanding and generation of long-form text up to thousands of tokens. Transformer-XL is a causal (uni-directional) transformer with relative positioning (sinuso\u00efdal) embeddings which can reuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax inputs and outputs (tied).</p> <pre><code>encoder:\ntype: transformer_xl\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: transfo-xl-wt103\ndropout: 0.1\nreduce_output: sum\nadaptive: true\ncutoffs:\n- 20000\n- 40000\n- 200000\nd_model: 1024\nd_embed: 1024\nn_head: 16\nd_head: 64\nd_inner: 4096\ndiv_val: 4\npre_lnorm: false\nn_layer: 18\nmem_len: 1600\nclamp_len: 1000\nsame_length: true\nproj_share_all_but_first: true\nattn_type: 0\nsample_softmax: -1\ndropatt: 0.0\nuntie_r: true\ninit: normal\ninit_range: 0.01\nproj_init_std: 0.01\ninit_std: 0.02\nlayer_norm_epsilon: 1.0e-05\neos_token_id: 0\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>transfo-xl-wt103</code>): Name or path of the pretrained model.</li> <li><code>dropout</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li> <p><code>adaptive</code> (default: <code>true</code>): Whether or not to use adaptive softmax. Options: <code>true</code>, <code>false</code>.</p> </li> <li> <p><code>cutoffs</code> (default: <code>[20000, 40000, 200000]</code>): Cutoffs for the adaptive softmax.</p> </li> <li><code>d_model</code> (default: <code>1024</code>): Dimensionality of the model\u2019s hidden states.</li> <li><code>d_embed</code> (default: <code>1024</code>): Dimensionality of the embeddings</li> <li><code>n_head</code> (default: <code>16</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>d_head</code> (default: <code>64</code>): Dimensionality of the model\u2019s heads.</li> <li><code>d_inner</code> (default: <code>4096</code>):  Inner dimension in FF</li> <li><code>div_val</code> (default: <code>4</code>): Divident value for adapative input and softmax.</li> <li><code>pre_lnorm</code> (default: <code>false</code>): Whether or not to apply LayerNorm to the input instead of the output in the blocks. Options: <code>true</code>, <code>false</code>.</li> <li><code>n_layer</code> (default: <code>18</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>mem_len</code> (default: <code>1600</code>): Length of the retained previous heads.</li> <li><code>clamp_len</code> (default: <code>1000</code>): Use the same pos embeddings after clamp_len.</li> <li><code>same_length</code> (default: <code>true</code>): Whether or not to use the same attn length for all tokens Options: <code>true</code>, <code>false</code>.</li> <li><code>proj_share_all_but_first</code> (default: <code>true</code>): True to share all but first projs, False not to share. Options: <code>true</code>, <code>false</code>.</li> <li><code>attn_type</code> (default: <code>0</code>): Attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.</li> <li><code>sample_softmax</code> (default: <code>-1</code>): Number of samples in the sampled softmax.</li> <li><code>dropatt</code> (default: <code>0.0</code>): The dropout ratio for the attention probabilities.</li> <li><code>untie_r</code> (default: <code>true</code>): Whether ot not to untie relative position biases. Options: <code>true</code>, <code>false</code>.</li> <li><code>init</code> (default: <code>normal</code>): Parameter initializer to use.</li> <li><code>init_range</code> (default: <code>0.01</code>): Parameters initialized by U(-init_range, init_range).</li> <li><code>proj_init_std</code> (default: <code>0.01</code>): Parameters initialized by N(0, init_std)</li> <li><code>init_std</code> (default: <code>0.02</code>): Parameters initialized by N(0, init_std)</li> <li><code>layer_norm_epsilon</code> (default: <code>1e-05</code>): The epsilon to use in the layer normalization layers</li> <li><code>eos_token_id</code> (default: <code>0</code>): The end of sequence token ID.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#xlmroberta","title":"XLMRoBERTa","text":"<p>The <code>xlmroberta</code> encoder loads a pretrained XLM-RoBERTa (default <code>jplu/tf-xlm-reoberta-base</code>) model using the Hugging Face transformers package. XLM-RoBERTa is a multi-language model similar to BERT, trained on 100 languages. XLM-RoBERTa is based on Facebook\u2019s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.</p> <pre><code>encoder:\ntype: xlmroberta\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: xlm-roberta-base\nreduce_output: cls_pooled\nmax_position_embeddings: 514\ntype_vocab_size: 1\npad_token_id: 1\nbos_token_id: 0\neos_token_id: 2\nadd_pooling_layer: true\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>xlm-roberta-base</code>): Name or path of the pretrained model.</li> <li><code>reduce_output</code> (default: <code>cls_pooled</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>max_position_embeddings</code> (default: <code>514</code>): The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li> <li> <p><code>type_vocab_size</code> (default: <code>1</code>): The vocabulary size of the token_type_ids passed in.</p> </li> <li> <p><code>pad_token_id</code> (default: <code>1</code>): The ID of the token to use as padding.</p> </li> <li><code>bos_token_id</code> (default: <code>0</code>): The beginning of sequence token ID.</li> <li><code>eos_token_id</code> (default: <code>2</code>): The end of sequence token ID.</li> <li><code>add_pooling_layer</code> (default: <code>true</code>): Whether to add a pooling layer to the encoder. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#xlnet","title":"XLNet","text":"<p>The <code>xlnet</code> encoder loads a pretrained XLNet (default <code>xlnet-base-cased</code>) model using the Hugging Face transformers package. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order. XLNet outperforms BERT on a variety of benchmarks.</p> <pre><code>encoder:\ntype: xlnet\nuse_pretrained: true\ntrainable: false\npretrained_model_name_or_path: xlnet-base-cased\ndropout: 0.1\nreduce_output: sum\nff_activation: gelu\ninitializer_range: 0.02\nsummary_activation: tanh\nsummary_last_dropout: 0.1\nd_model: 768\nn_layer: 12\nn_head: 12\nd_inner: 3072\nuntie_r: true\nattn_type: bi\nlayer_norm_eps: 1.0e-12\nmem_len: null\nreuse_len: null\nuse_mems_eval: true\nuse_mems_train: false\nbi_data: false\nclamp_len: -1\nsame_length: false\nsummary_type: last\nsummary_use_proj: true\nstart_n_top: 5\nend_n_top: 5\npad_token_id: 5\nbos_token_id: 1\neos_token_id: 2\npretrained_kwargs: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_pretrained</code> (default: <code>true</code>) : Whether to use the pretrained weights for the model. If false, the model will train from scratch which is very computationally expensive. Options: <code>true</code>, <code>false</code>.</li> <li><code>trainable</code> (default: <code>false</code>) : Whether to finetune the model on your dataset. Options: <code>true</code>, <code>false</code>.</li> <li><code>pretrained_model_name_or_path</code> (default: <code>xlnet-base-cased</code>): Name or path of the pretrained model.</li> <li><code>dropout</code> (default: <code>0.1</code>): The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</li> <li><code>reduce_output</code> (default: <code>sum</code>): The method used to reduce a sequence of tensors down to a single tensor.</li> <li><code>ff_activation</code> (default: <code>gelu</code>): The non-linear activation function (function or string) in the encoder and pooler. If string, 'gelu', 'relu', 'silu' and 'gelu_new' are supported. Options: <code>gelu</code>, <code>relu</code>, <code>silu</code>, <code>gelu_new</code>.</li> <li><code>initializer_range</code> (default: <code>0.02</code>): The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</li> <li><code>summary_activation</code> (default: <code>tanh</code>): Argument used when doing sequence summary. Used in the sequence classification and multiple choice models.</li> <li> <p><code>summary_last_dropout</code> (default: <code>0.1</code>): Used in the sequence classification and multiple choice models.</p> </li> <li> <p><code>d_model</code> (default: <code>768</code>): Dimensionality of the encoder layers and the pooler layer.</p> </li> <li><code>n_layer</code> (default: <code>12</code>): Number of hidden layers in the Transformer encoder.</li> <li><code>n_head</code> (default: <code>12</code>): Number of attention heads for each attention layer in the Transformer encoder.</li> <li><code>d_inner</code> (default: <code>3072</code>): Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</li> <li><code>untie_r</code> (default: <code>true</code>): Whether or not to untie relative position biases Options: <code>true</code>, <code>false</code>.</li> <li><code>attn_type</code> (default: <code>bi</code>): The attention type used by the model. Currently only 'bi' is supported. Options: <code>bi</code>.</li> <li><code>layer_norm_eps</code> (default: <code>1e-12</code>): The epsilon used by the layer normalization layers.</li> <li><code>mem_len</code> (default: <code>null</code>): The number of tokens to cache. The key/value pairs that have already been pre-computed in a previous forward pass won\u2019t be re-computed. </li> <li><code>reuse_len</code> (default: <code>null</code>): The number of tokens in the current batch to be cached and reused in the future.</li> <li><code>use_mems_eval</code> (default: <code>true</code>): Whether or not the model should make use of the recurrent memory mechanism in evaluation mode. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_mems_train</code> (default: <code>false</code>): Whether or not the model should make use of the recurrent memory mechanism in train mode. Options: <code>true</code>, <code>false</code>.</li> <li><code>bi_data</code> (default: <code>false</code>): Whether or not to use bidirectional input pipeline. Usually set to True during pretraining and False during finetuning. Options: <code>true</code>, <code>false</code>.</li> <li><code>clamp_len</code> (default: <code>-1</code>): Clamp all relative distances larger than clamp_len. Setting this attribute to -1 means no clamping.</li> <li><code>same_length</code> (default: <code>false</code>): Whether or not to use the same attention length for each token. Options: <code>true</code>, <code>false</code>.</li> <li><code>summary_type</code> (default: <code>last</code>): Argument used when doing sequence summary. Used in the sequence classification and multiple choice models. Options: <code>last</code>, <code>first</code>, <code>mean</code>, <code>cls_index</code>, <code>attn</code>.</li> <li><code>summary_use_proj</code> (default: <code>true</code>):  Options: <code>true</code>, <code>false</code>.</li> <li><code>start_n_top</code> (default: <code>5</code>): Used in the SQuAD evaluation script.</li> <li><code>end_n_top</code> (default: <code>5</code>):  Used in the SQuAD evaluation script.</li> <li><code>pad_token_id</code> (default: <code>5</code>): The ID of the token to use as padding.</li> <li><code>bos_token_id</code> (default: <code>1</code>): The beginning of sequence token ID.</li> <li><code>eos_token_id</code> (default: <code>2</code>): The end of sequence token ID.</li> <li><code>pretrained_kwargs</code> (default: <code>null</code>): Additional kwargs to pass to the pretrained model.</li> </ul>"},{"location":"configuration/features/text_features/#output-features","title":"Output Features","text":"<p>Text output features are a special case of Sequence Features, so all options of sequence features are available for text features as well.</p> <p>Text output features can be used for either tagging (classifying each token of an input sequence) or text generation (generating text by repeatedly sampling from the model). There are two decoders available for these tasks named <code>tagger</code> and <code>generator</code> respectively.</p> <p>Example text output feature using default parameters:</p> <pre><code>name: text_column_name\ntype: text\nreduce_input: null\ndependencies: []\nreduce_dependencies: sum\nloss:\ntype: softmax_cross_entropy\nconfidence_penalty: 0\nrobust_lambda: 0\nclass_weights: 1\nclass_similarities_temperature: 0\ndecoder: type: generator\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the sequence dimension), <code>last</code> (returns the last vector of the sequence dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Feature Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the sequence dimension), <code>last</code> (returns the last vector of the sequence dimension).</li> <li><code>loss</code> (default <code>{type: softmax_cross_entropy, class_similarities_temperature: 0, class_weights: 1, confidence_penalty: 0, robust_lambda: 0}</code>): is a dictionary containing a loss <code>type</code>. The only available loss <code>type</code> for text features is <code>softmax_cross_entropy</code>. See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"generator\"}</code>): Decoder for the desired task. Options: <code>generator</code>, <code>tagger</code>. See Decoder for details.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all text output features using the Type-Global Decoder section. Loss and loss related parameters can also be defined once in the same way.</p>"},{"location":"configuration/features/text_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/text_features/#generator","title":"Generator","text":"<p>In the case of <code>generator</code> the decoder is a (potentially empty) stack of fully connected layers, followed by an RNN that generates outputs feeding on its own previous predictions and generates a tensor of size <code>b x s' x c</code>, where <code>b</code> is the batch size, <code>s'</code> is the length of the generated sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy. During training teacher forcing is adopted, meaning the list of targets is provided as both inputs and outputs (shifted by 1), while at evaluation time greedy decoding (generating one token at a time and feeding it as input for the next step) is performed by beam search, using a beam of 1 by default. In general a generator expects a <code>b x h</code> shaped input tensor, where <code>h</code> is a hidden dimension. The <code>h</code> vectors are (after an optional stack of fully connected layers) fed into the rnn generator. One exception is when the generator uses attention, as in that case the expected size of the input tensor is <code>b x s x h</code>, which is the output of a sequence, text or time series input feature without reduced outputs or the output of a sequence-based combiner. If a <code>b x h</code> input is provided to a generator decoder using an RNN with attention instead, an error will be raised during model building.</p> <pre><code>decoder:\ntype: generator\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\ncell_type: gru\nnum_layers: 1\nfc_activation: relu\nreduce_input: sum\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>cell_type</code> (default: <code>gru</code>) : Type of recurrent cell to use. Options: <code>rnn</code>, <code>lstm</code>, <code>gru</code>.</li> <li><code>num_layers</code> (default: <code>1</code>) : The number of stacked recurrent layers.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>reduce_input</code> (default: <code>sum</code>): How to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension) Options: <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>last</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> </ul>"},{"location":"configuration/features/text_features/#tagger","title":"Tagger","text":"<p>In the case of <code>tagger</code> the decoder is a (potentially empty) stack of fully connected layers, followed by a projection into a tensor of size <code>b x s x c</code>, where <code>b</code> is the batch size, <code>s</code> is the length of the sequence and <code>c</code> is the number of classes, followed by a softmax_cross_entropy. This decoder requires its input to be shaped as <code>b x s x h</code>, where <code>h</code> is a hidden dimension, which is the output of a sequence, text or time series input feature without reduced outputs or the output of a sequence-based combiner. If a <code>b x h</code> input is provided instead, an error will be raised during model building.</p> <pre><code>decoder:\ntype: tagger\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\nfc_activation: relu\nattention_embedding_size: 256\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_attention: false\nuse_bias: true\nattention_num_heads: 8\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>attention_embedding_size</code> (default: <code>256</code>): The embedding size of the multi-head self attention layer.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_attention</code> (default: <code>false</code>): Whether to apply a multi-head self attention layer before prediction. Options: <code>true</code>, <code>false</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>attention_num_heads</code> (default: <code>8</code>): The number of attention heads in the multi-head self attention layer.</li> </ul>"},{"location":"configuration/features/text_features/#loss","title":"Loss","text":""},{"location":"configuration/features/text_features/#softmax-cross-entropy","title":"Softmax Cross Entropy","text":"<pre><code>loss:\ntype: softmax_cross_entropy\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>class_weights</code> (default: <code>null</code>) : Weights to apply to each class in the loss. If not specified, all classes are weighted equally. The value can be a vector of weights, one for each class, that is multiplied to the loss of the datapoints that have that class as ground truth. It is an alternative to oversampling in case of unbalanced class distribution. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too). Alternatively, the value can be a dictionary with class strings as keys and weights as values, like <code>{class_a: 0.5, class_b: 0.7, ...}</code>.</li> <li><code>robust_lambda</code> (default: <code>0</code>): Replaces the loss with <code>(1 - robust_lambda) * loss + robust_lambda / c</code> where <code>c</code> is the number of classes. Useful in case of noisy labels.</li> <li><code>confidence_penalty</code> (default: <code>0</code>): Penalizes overconfident predictions (low entropy) by adding an additional term that penalizes too confident predictions by adding a <code>a * (max_entropy - entropy) / max_entropy</code> term to the loss, where a is the value of this parameter. Useful in case of noisy labels.</li> <li><code>class_similarities</code> (default: <code>null</code>): If not <code>null</code> it is a <code>c x c</code> matrix in the form of a list of lists that contains the mutual similarity of classes. It is used if <code>class_similarities_temperature</code> is greater than 0. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too).</li> <li><code>class_similarities_temperature</code> (default: <code>0</code>): The temperature parameter of the softmax that is performed on each row of <code>class_similarities</code>. The output of that softmax is used to determine the supervision vector to provide instead of the one hot vector that would be provided otherwise for each datapoint. The intuition behind it is that errors between similar classes are more tolerable than errors between really different classes.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/text_features/#metrics","title":"Metrics","text":"<p>The metrics available for text features are the same as for Sequence Features:</p> <ul> <li><code>sequence_accuracy</code> The rate at which the model predicted the correct sequence.</li> <li><code>token_accuracy</code> The number of tokens correctly predicted divided by the total number of tokens in all sequences.</li> <li><code>last_accuracy</code> Accuracy considering only the last element of the sequence. Useful to ensure special end-of-sequence tokens are generated or tagged.</li> <li><code>edit_distance</code> Levenshtein distance: the minimum number of single-token edits (insertions, deletions or substitutions) required to change predicted sequence to ground truth.</li> <li><code>perplexity</code> Perplexity is the inverse of the predicted probability of the ground truth sequence, normalized by the number of tokens. The lower the perplexity, the higher the probability of predicting the true sequence.</li> <li><code>loss</code> The value of the loss function.</li> </ul> <p>You can set any of the above as <code>validation_metric</code> in the <code>training</code> section of the configuration if <code>validation_field</code> names a sequence feature.</p>"},{"location":"configuration/features/time_series_features/","title":"\u2191 Time Series Features","text":""},{"location":"configuration/features/time_series_features/#preprocessing","title":"Preprocessing","text":"<p>Timeseries features are handled as sequence features, with the only difference being that the matrix in the HDF5 preprocessing file uses floats instead of integers.</p> <p>Since data is continuous, the JSON file, which typically stores vocabulary mappings, isn't needed.</p>"},{"location":"configuration/features/time_series_features/#input-features","title":"Input Features","text":""},{"location":"configuration/features/time_series_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/time_series_features/#sequence-encoders","title":"Sequence Encoders","text":"<p>Time series encoders are the same as for Sequence Features, with one exception:</p> <p>Time series features don't have an embedding layer at the beginning, so the <code>b x s</code> placeholders (where <code>b</code> is the batch size and <code>s</code> is the sequence length) are directly mapped to a <code>b x s x 1</code> tensor and then passed to the different sequential encoders.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of another input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example category feature entry in the input features list:</p> <pre><code>name: timeseries_column_name\ntype: timeseries\ntied: null\nencoder: type: parallel_cnn\n</code></pre>"},{"location":"configuration/features/time_series_features/#passthrough-encoder","title":"Passthrough Encoder","text":"<p>The passthrough encoder simply transforms each input value into a float value and adds a dimension to the input tensor, creating a <code>b x s x 1</code> tensor where <code>b</code> is the batch size and <code>s</code> is the length of the sequence. The tensor is reduced along the <code>s</code> dimension to obtain a single vector of size <code>h</code> for each element of the batch. If you want to output the full <code>b x s x h</code> tensor, you can specify <code>reduce_output: null</code>. This encoder is not really useful for <code>sequence</code> or <code>text</code> features, but may be useful for <code>timeseries</code> features, as it allows for using them without any processing in later stages of the model, like in a sequence combiner for instance.</p> <pre><code>encoder:\ntype: passthrough\nencoding_size: null\nreduce_output: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>encoding_size</code> (default: <code>null</code>): The size of the encoding vector, or None if sequence elements are scalars.</li> <li><code>reduce_output</code> (default: <code>null</code>): How to reduce the output tensor along the <code>s</code> sequence length dimension if the rank of the tensor is greater than 2. Options: <code>last</code>, <code>sum</code>, <code>mean</code>, <code>avg</code>, <code>max</code>, <code>concat</code>, <code>attention</code>, <code>none</code>, <code>None</code>, <code>null</code>.</li> </ul>"},{"location":"configuration/features/time_series_features/#output-features","title":"Output Features","text":"<p>There are no time series decoders at the moment.</p> <p>If this would unlock an interesting use case for your application, please file a GitHub Issue or ping the Ludwig Slack.</p>"},{"location":"configuration/features/vector_features/","title":"\u21c5 Vector Features","text":"<p>Vector features enable providing an ordered set of numerical values within a single feature.</p> <p>This is useful for providing pre-trained representations or activations obtained from other models or for providing multivariate inputs and outputs. An interesting use of vector features is the possibility of providing a probability distribution as output for a multiclass classification problem instead of a single correct class like with a category feature. Vector output features can also be useful for distillation and noise-aware losses.</p>"},{"location":"configuration/features/vector_features/#preprocessing","title":"Preprocessing","text":"<p>The data is expected as whitespace separated numerical values. Example: \"1.0 0.0 1.04 10.49\".  All vectors are expected to be of the same size.</p> <pre><code>preprocessing:\nvector_size: null\nmissing_value_strategy: fill_with_const\nfill_value: ''\n</code></pre> <p>Parameters:</p> <ul> <li><code>vector_size</code> (default: <code>null</code>) : The size of the vector. If None, the vector size will be inferred from the data.</li> <li><code>missing_value_strategy</code> (default: <code>fill_with_const</code>) : What strategy to follow when there's a missing value in a vector column Options: <code>fill_with_const</code>, <code>fill_with_mode</code>, <code>bfill</code>, <code>ffill</code>, <code>drop_row</code>. See Missing Value Strategy for details.</li> <li><code>fill_value</code> (default: ``): The value to replace missing values with in case the missing_value_strategy is fill_with_const</li> </ul> <p>Preprocessing parameters can also be defined once and applied to all vector input features using the Type-Global Preprocessing section.</p>"},{"location":"configuration/features/vector_features/#input-features","title":"Input Features","text":"<p>The vector feature supports two encoders: <code>dense</code> and <code>passthrough</code>.</p> <p>The encoder parameters specified at the feature level are:</p> <ul> <li><code>tied</code> (default <code>null</code>): name of the input feature to tie the weights of the encoder with. It needs to be the name of a feature of the same type and with the same encoder parameters.</li> </ul> <p>Example vector feature entry in the input features list:</p> <pre><code>name: vector_column_name\ntype: vector\ntied: null\nencoder: type: dense\n</code></pre> <p>The available encoder parameters are:</p> <ul> <li><code>type</code> (default <code>dense</code>): the possible values are <code>passthrough</code> and <code>dense</code>. <code>passthrough</code> outputs the raw vector values unaltered. <code>dense</code> uses a stack of fully connected layers to create an embedding matrix.</li> </ul> <p>Encoder type and encoder parameters can also be defined once and applied to all vector input features using the Type-Global Encoder section.</p>"},{"location":"configuration/features/vector_features/#encoders","title":"Encoders","text":""},{"location":"configuration/features/vector_features/#passthrough-encoder","title":"Passthrough Encoder","text":"<pre><code>encoder:\ntype: passthrough\n</code></pre> <p>There are no additional parameters for <code>passthrough</code> encoder.</p>"},{"location":"configuration/features/vector_features/#dense-encoder","title":"Dense Encoder","text":"<p>For vector features, a dense encoder (stack of fully connected layers) can be used to encode the vector.  </p> <pre><code>encoder:\ntype: dense\ndropout: 0.0\noutput_size: 256\nnorm: null\nnum_layers: 1\nactivation: relu\nuse_bias: true\nbias_initializer: zeros\nweights_initializer: xavier_uniform\nnorm_params: null\nfc_layers: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>output_size</code> (default: <code>256</code>) : Size of the output of the feature.</li> <li><code>norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>num_layers</code> (default: <code>1</code>) : Number of stacked fully connected layers to apply. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li> <p><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.  Alternatively it is possible to specify a dictionary with a key <code>type</code> that identifies the type of initializer and other keys for its parameters, e.g. <code>{type: normal, mean: 0, stddev: 0}</code>. For a description of the parameters of each initializer, see torch.nn.init.</p> </li> <li> <p><code>norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</p> </li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> </ul>"},{"location":"configuration/features/vector_features/#output-features","title":"Output Features","text":"<p>Vector features can be used when multi-class classification needs to be performed with a noise-aware loss or when the task is multivariate regression.</p> <p>There is only one decoder available for vector features: a (potentially empty) stack of fully connected layers, followed by a projection into a tensor of the vector size (optionally followed by a softmax in the case of multi-class classification).</p> <p>Example vector output feature using default parameters:</p> <pre><code>name: vector_column_name\ntype: vector\nreduce_input: sum\ndependencies: []\nreduce_dependencies: sum\nloss:\ntype: sigmoid_cross_entropy\ndecoder:\ntype: projector\n</code></pre> <p>Parameters:</p> <ul> <li><code>reduce_input</code> (default <code>sum</code>): defines how to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>dependencies</code> (default <code>[]</code>): the output features this one is dependent on. For a detailed explanation refer to Output Features Dependencies.</li> <li><code>reduce_dependencies</code> (default <code>sum</code>): defines how to reduce the output of a dependent feature that is not a vector, but a matrix or a higher order tensor, on the first dimension (second if you count the batch dimension). Available values are: <code>sum</code>, <code>mean</code> or <code>avg</code>, <code>max</code>, <code>concat</code> (concatenates along the first dimension), <code>last</code> (returns the last vector of the first dimension).</li> <li><code>softmax</code> (default <code>false</code>): determines if to apply a softmax at the end of the decoder. It is useful for predicting a vector of values that sum up to 1 and can be interpreted as probabilities.</li> <li><code>loss</code> (default <code>{type: mean_squared_error}</code>): is a dictionary containing a loss <code>type</code>. The available loss <code>type</code> are <code>mean_squared_error</code>, <code>mean_absolute_error</code> and <code>softmax_cross_entropy</code> (use it only if <code>softmax</code> is <code>true</code>). See Loss for details.</li> <li><code>decoder</code> (default: <code>{\"type\": \"projector\"}</code>): Decoder for the desired task. Options: <code>projector</code>. See Decoder for details.</li> </ul>"},{"location":"configuration/features/vector_features/#decoders","title":"Decoders","text":""},{"location":"configuration/features/vector_features/#projector","title":"Projector","text":"<pre><code>decoder:\ntype: projector\nnum_fc_layers: 0\nfc_output_size: 256\nfc_norm: null\nfc_dropout: 0.0\noutput_size: null\nfc_activation: relu\nactivation: null\nfc_layers: null\nfc_use_bias: true\nfc_weights_initializer: xavier_uniform\nfc_bias_initializer: zeros\nfc_norm_params: null\nuse_bias: true\nweights_initializer: xavier_uniform\nbias_initializer: zeros\nclip: null\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_fc_layers</code> (default: <code>0</code>) : Number of fully-connected layers if <code>fc_layers</code> not specified. Increasing layers adds capacity to the model, enabling it to learn more complex feature interactions.</li> <li><code>fc_output_size</code> (default: <code>256</code>) : Output size of fully connected stack.</li> <li><code>fc_norm</code> (default: <code>null</code>) : Default normalization applied at the beginnging of fully connected layers. Options: <code>batch</code>, <code>layer</code>, <code>ghost</code>, <code>null</code>. See Normalization for details.</li> <li><code>fc_dropout</code> (default: <code>0.0</code>) : Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout).</li> <li><code>output_size</code> (default: <code>null</code>) : Size of the output of the decoder.</li> <li><code>fc_activation</code> (default: <code>relu</code>): Default activation function applied to the output of the fully connected layers. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>activation</code> (default: <code>null</code>):  Indicates the activation function applied to the output. Options: <code>elu</code>, <code>leakyRelu</code>, <code>logSigmoid</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>softmax</code>, <code>null</code>.</li> <li><code>fc_layers</code> (default: <code>null</code>): List of dictionaries containing the parameters of all the fully connected layers. The length of the list determines the number of stacked fully connected layers and the content of each dictionary determines the parameters for a specific layer. The available parameters for each layer are: <code>activation</code>, <code>dropout</code>, <code>norm</code>, <code>norm_params</code>, <code>output_size</code>, <code>use_bias</code>, <code>bias_initializer</code> and <code>weights_initializer</code>. If any of those values is missing from the dictionary, the default one provided as a standalone parameter will be used instead.</li> <li><code>fc_use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector in the fc_stack. Options: <code>true</code>, <code>false</code>.</li> <li><code>fc_weights_initializer</code> (default: <code>xavier_uniform</code>): The weights initializer to use for the layers in the fc_stack</li> <li><code>fc_bias_initializer</code> (default: <code>zeros</code>): The bias initializer to use for the layers in the fc_stack</li> <li><code>fc_norm_params</code> (default: <code>null</code>): Default parameters passed to the <code>norm</code> module.</li> <li><code>use_bias</code> (default: <code>true</code>): Whether the layer uses a bias vector. Options: <code>true</code>, <code>false</code>.</li> <li><code>weights_initializer</code> (default: <code>xavier_uniform</code>): Initializer for the weight matrix. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>bias_initializer</code> (default: <code>zeros</code>): Initializer for the bias vector. Options: <code>uniform</code>, <code>normal</code>, <code>constant</code>, <code>ones</code>, <code>zeros</code>, <code>eye</code>, <code>dirac</code>, <code>xavier_uniform</code>, <code>xavier_normal</code>, <code>kaiming_uniform</code>, <code>kaiming_normal</code>, <code>orthogonal</code>, <code>sparse</code>, <code>identity</code>.</li> <li><code>clip</code> (default: <code>null</code>): Clip the output of the decoder to be within the given range.</li> </ul> <p>Decoder type and decoder parameters can also be defined once and applied to all vector output features using the Type-Global Decoder section.</p>"},{"location":"configuration/features/vector_features/#loss","title":"Loss","text":""},{"location":"configuration/features/vector_features/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<pre><code>loss:\ntype: mean_squared_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/vector_features/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<pre><code>loss:\ntype: mean_absolute_error\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul>"},{"location":"configuration/features/vector_features/#softmax-cross-entropy","title":"Softmax Cross Entropy","text":"<pre><code>loss:\ntype: softmax_cross_entropy\nclass_weights: null\nrobust_lambda: 0\nconfidence_penalty: 0\nclass_similarities: null\nclass_similarities_temperature: 0\nweight: 1.0\n</code></pre> <p>Parameters:</p> <ul> <li><code>class_weights</code> (default: <code>null</code>) : Weights to apply to each class in the loss. If not specified, all classes are weighted equally. The value can be a vector of weights, one for each class, that is multiplied to the loss of the datapoints that have that class as ground truth. It is an alternative to oversampling in case of unbalanced class distribution. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too). Alternatively, the value can be a dictionary with class strings as keys and weights as values, like <code>{class_a: 0.5, class_b: 0.7, ...}</code>.</li> <li><code>robust_lambda</code> (default: <code>0</code>): Replaces the loss with <code>(1 - robust_lambda) * loss + robust_lambda / c</code> where <code>c</code> is the number of classes. Useful in case of noisy labels.</li> <li><code>confidence_penalty</code> (default: <code>0</code>): Penalizes overconfident predictions (low entropy) by adding an additional term that penalizes too confident predictions by adding a <code>a * (max_entropy - entropy) / max_entropy</code> term to the loss, where a is the value of this parameter. Useful in case of noisy labels.</li> <li><code>class_similarities</code> (default: <code>null</code>): If not <code>null</code> it is a <code>c x c</code> matrix in the form of a list of lists that contains the mutual similarity of classes. It is used if <code>class_similarities_temperature</code> is greater than 0. The ordering of the vector follows the category to integer ID mapping in the JSON metadata file (the <code>&lt;UNK&gt;</code> class needs to be included too).</li> <li><code>class_similarities_temperature</code> (default: <code>0</code>): The temperature parameter of the softmax that is performed on each row of <code>class_similarities</code>. The output of that softmax is used to determine the supervision vector to provide instead of the one hot vector that would be provided otherwise for each datapoint. The intuition behind it is that errors between similar classes are more tolerable than errors between really different classes.</li> <li><code>weight</code> (default: <code>1.0</code>): Weight of the loss.</li> </ul> <p>Loss type and loss related parameters can also be defined once and applied to all vector output features using the Type-Global Loss section.</p>"},{"location":"configuration/features/vector_features/#metrics","title":"Metrics","text":"<p>The metrics that are calculated every epoch and are available for set features are <code>mean_squared_error</code>, <code>mean_absolute_error</code>, <code>r2</code>, and the <code>loss</code> itself.</p> <p>You can set any of them as <code>validation_metric</code> in the <code>training</code> section of the configuration if you set the <code>validation_field</code> to be the name of a vector feature.</p>"},{"location":"developer_guide/","title":"Developer Guide","text":"<p>This is the Ludwig Developer Guide. It helps you understanding the structure of the Ludwig codebase and learn how to add modules and contribute to it.</p>"},{"location":"developer_guide/add_a_combiner/","title":"Add a Combiner","text":"<p>Combiners are responsible for combining the outputs of one or more input features into a single combined representation, which is usually a vector, but may also be a sequence of vectors or some other higher-dimensional tensor. One or more output features will use this combined representation to generate predictions.</p> <p>Users can specify which combiner to use in the <code>combiner</code> section of the configuration, if a combiner is not specified the <code>concat</code> combiner will be used.</p> <p>Recall the ECD (Encoder, Combiner, Decoder) data flow architecture: all input feature outputs flow into the combiner, and the combiner's output flows into all output features.</p> <pre><code>+-----------+                      +-----------+\n|Input      |                      | Output    |\n|Feature 1  +-+                  +-+ Feature 1 + ---&gt; Prediction 1\n+-----------+ |                  | +-----------+\n+-----------+ |   +----------+   | +-----------+\n|...        +---&gt; | Combiner +---&gt; |...        +\n+-----------+ |   +----------+   | +-----------+\n+-----------+ |                  | +-----------+\n|Input      +-+                  +-+ Output    |\n|Feature N  |                      | Feature N + ---&gt; Prediction N\n+-----------+                      +-----------+\n</code></pre> <p>There is an additional complication to keep in mind: input features may either output vectors, or sequences of vectors. Thus, a combiner may have to handle a mix of input features whose outputs are of different dimensionality. <code>SequenceConcatCombiner</code>, for example, resolves this by requiring that all input sequences be of the same length. It will raise a <code>ValueError</code> exception if they are not. <code>SequenceConcatCombiner</code> tiles non-sequence inputs to the sequence length before concatenation, processing all input features as sequences of the same length.</p> <p>New combiners should make it clear in their doc strings if they support sequence inputs, declare any requirements on sequence length, type, or dimension, and validate their input features.</p> <p>In this guide we'll outline how to extend Ludwig by adding a new combiner, using the <code>transformer</code> combiner as a template. To add a new combiner:</p> <ol> <li>Define a dataclass to represent the combiner schema.</li> <li>Create a new combiner class inheriting from <code>ludwig.combiners.Combiner</code> or one of its subclasses.</li> <li>Allocate all layers and state in the <code>__init__</code> method.</li> <li>Implement your combiner's forward pass in <code>def forward(self, inputs: Dict):</code>.</li> <li>Add tests.</li> <li>Add the new combiner to the combiner registry.</li> </ol>"},{"location":"developer_guide/add_a_combiner/#1-define-the-combiners-schema","title":"1. Define the combiner's schema","text":"<p>The combiner schema is a <code>dataclass</code> (overloaded by the <code>marshmallow_dataclass</code> modue) that must extend <code>BaseCombinerConfig</code>. Its attributes are the configuration parameters of the combiner. All fields should have a type and a default value. The <code>ludwig.schema.utils.py</code> module provides convenient methods for specifying the valid types and ranges of a combiner config. For example, the <code>TransformerCombiner</code> has the following schema:</p> <pre><code>from typing import Optional, List, Dict, Any\n\n# Main imports:\nfrom marshmallow_dataclass import dataclass\nfrom ludwig.schema import utils as schema_utils\nfrom ludwig.schema.combiners.base import BaseCombinerConfig\n\n@dataclass\nclass TransformerCombinerConfig(BaseCombinerConfig):\n    num_layers: int = schema.PositiveInteger(default=1)\n    hidden_size: int = schema.NonNegativeInteger(default=256)\n    num_heads: int = schema.NonNegativeInteger(default=8)\n    transformer_output_size: int = schema.NonNegativeInteger(default=256)\n    dropout: float = schema.FloatRange(default=0.1, min=0, max=1)\n    fc_layers: Optional[List[Dict[str, Any]]] = schema.DictList()\n    num_fc_layers: int = schema.NonNegativeInteger(default=0)\n    output_size: int = schema.PositiveInteger(default=256)\n    use_bias: bool = True\n    weights_initializer: Union[str, Dict] = \\\n        schema.InitializerOrDict(default=\"xavier_uniform\")\n    bias_initializer: Union[str, Dict] = \\\n        schema.InitializerOrDict(default=\"zeros\")\n    norm: Optional[str] = schema.StringOptions([\"batch\", \"layer\"])\n    norm_params: Optional[dict] = schema.Dict()\n    fc_activation: str = \"relu\"\n    fc_dropout: float = schema.FloatRange(default=0.0, min=0, max=1)\n    fc_residual: bool = False\n    reduce_output: Optional[str] = schema.ReductionOptions(default=\"mean\")\n</code></pre> <p>This schema should live in its own file inside <code>ludwig/schema/combiners/</code>. So that it is more convenient to import elsewhere in Ludwig, you may also add it as an import to <code>ludwig/schema/combiners/__init__.py</code> like so:</p> <pre><code>from ludwig.schema.combiners.transformer import TransformerCombinerConfig  # noqa: F401\n</code></pre>"},{"location":"developer_guide/add_a_combiner/#2-add-a-new-combiner-class","title":"2. Add a new combiner class","text":"<p>Source code for combiners lives in <code>ludwig/combiners/</code>. Add a new python module which declares a new combiner class. For this example, we'll show how to implement a simplified version of <code>transformer</code> combiner which would be defined in <code>transformer_combiner.py</code>.</p> <p>Note</p> <p>At present, all combiners are defined in <code>ludwig/combiners/combiners.py</code>. However, for new combiners we recommend creating a new python module with a name corresponding to the new combiner class.</p> <pre><code>@register_combiner(name=\"transformer\")\nclass TransformerCombiner(Combiner):\n    def __init__(\n        self,\n        input_features: Dict[str, InputFeature] = None,\n        config: TransformerCombinerConfig = None,\n        **kwargs\n    ):\n        super().__init__(input_features)\n        self.name = \"TransformerCombiner\"\n\n    def forward(\n        self,\n        inputs: Dict,\n    ) -&gt; Dict[str: torch.Tensor]:\n\n    @staticmethod\n    def get_schema_cls():\n        return TransformerCombinerConfig\n</code></pre> <p>Implement <code>@staticmethod def get_schema_cls():</code> and return the class name of your config schema.</p>"},{"location":"developer_guide/add_a_combiner/#3-implement-constructor","title":"3. Implement Constructor","text":"<p>The combiner constructor will be initialized with a dictionary of the input features and the combiner config. The constructor must pass the input features to the superclass constructor, set its <code>name</code> property, then create its own layers and state.</p> <p>The <code>input_features</code> dictionary is passed in to the constructor to make information about the number, size, and type of the inputs accessible. This may determine what resources the combiner needs to allocate. For example, the <code>transformer</code> combiner treats its input features as a sequence, where the sequence length is the number of features. We can determine the sequence length here as <code>self.sequence_size = len(self.input_features)</code>.</p> <pre><code>    def __init__(\n        self,\n        input_features: Dict[str, InputFeature] = None,\n        config: TransformerCombinerConfig = None,\n        **kwargs\n    ):\n        super().__init__(input_features)\n        self.name = \"TransformerCombiner\"\n        # ...\n        self.sequence_size = len(self.input_features)\n\n        self.transformer_stack = TransformerStack(\n            input_size=config.hidden_size,\n            sequence_size=self.sequence_size,\n            hidden_size=config.hidden_size,\n            num_heads=config.num_heads,\n            output_size=config.transformer_output_size,\n            num_layers=config.num_layers,\n            dropout=config.dropout,\n        )\n        # ...\n</code></pre>"},{"location":"developer_guide/add_a_combiner/#4-implement-forward-method","title":"4. Implement <code>forward</code> method","text":"<p>The <code>forward</code> method of the combiner should combine the input feature representations into a single output tensor, which will be passed to output feature decoders. Each key in inputs is an input feature name, and the respective value is a dictionary of the input feature's outputs. Each feature output dictionary is guaranteed to contain an <code>encoder_output</code> key, and may contain other outputs depending on the encoder.</p> <p><code>forward</code> returns a dictionary mapping strings to tensors which must contain a <code>combiner_output</code> key. It may optionally return additional values that might be useful for output feature decoding, loss computation, or explanation. For example, <code>TabNetCombiner</code> returns its sparse attention masks (<code>attention_masks</code>, and <code>aggregated_attention_masks</code>) which are useful to see which input features were attended to in each prediction step.</p> <p>For example, the following is a simplified version of <code>TransformerCombiner</code>'s forward method:</p> <pre><code>    def forward(\n        self, inputs: Dict[str, Dict[str, torch.Tensor]]\n    ) -&gt; Dict[str, torch.Tensor]:\n        encoder_outputs = [inputs[k][\"encoder_output\"] for k in inputs]\n\n        # ================ Flatten ================\n        batch_size = encoder_outputs[0].shape[0]\n        encoder_outputs = [\n            torch.reshape(eo, [batch_size, -1]) for eo in encoder_outputs\n        ]\n\n        # ================ Project &amp; Concat ================\n        projected = [\n            self.projectors[i](eo) for i, eo in enumerate(encoder_outputs)\n        ]\n        hidden = torch.stack(projected)\n        hidden = torch.permute(hidden, (1, 0, 2))\n\n        # ================ Transformer Layers ================\n        hidden = self.transformer_stack(hidden)\n\n        # ================ Sequence Reduction ================\n        if self.reduce_output is not None:\n            hidden = self.reduce_sequence(hidden)\n            hidden = self.fc_stack(hidden)\n\n        return_data = {\"combiner_output\": hidden}\n        return return_data\n</code></pre> <p>Inputs</p> <ul> <li>inputs (<code>Dict[str, Dict[str, torch.Tensor]]</code>): A dictionary of input feature outputs, keyed by the input feature names. Each input feature output dictionary is guaranteed to include <code>encoder_output</code>, and may include other key/value pairs depending on the input feature's encoder.</li> </ul> <p>Return</p> <ul> <li>(<code>Dict[str, torch.Tensor]</code>): A dictionary containing the required key <code>combiner_output</code> whose value is the combiner output tensor, and any other optional output key/value pairs.</li> </ul>"},{"location":"developer_guide/add_a_combiner/#5-add-new-class-to-the-registry","title":"5. Add new class to the registry","text":"<p>Mapping between combiner names in the model config and combiner classes is made by registering the class in the combiner registry. The combiner registry is defined in <code>ludwig/schema/combiners/utils.py</code>. To register your class, add the <code>@register_combiner</code> decorator on the line above its class definition, specifying the name of the combiner:</p> <pre><code>@register_combiner(name=\"transformer\")\nclass TransformerCombiner(Combiner):\n</code></pre>"},{"location":"developer_guide/add_a_combiner/#6-add-tests","title":"6. Add tests","text":"<p>Add a corresponding unit test module to <code>tests/ludwig/combiners</code>, using the name of your combiner module prefixed by <code>test_</code> i.e. <code>test_transformer_combiner.py</code>.</p> <p>At a minimum, the unit test should ensure that:</p> <ol> <li>The combiner's forward pass succeeds for all feature types it supports.</li> <li>The combiner fails in expected ways when given unsupported input. (Skip this if the combiner supports all input feature types.)</li> <li>The combiner produces output of the correct type and dimensionality given a variety of configs.</li> </ol> <p>Use <code>@pytest.mark.parametrize</code> to parameterize your test with different configurations, also test edge cases:</p> <pre><code>@pytest.mark.parametrize(\"output_size\", [8, 16])\n@pytest.mark.parametrize(\"transformer_output_size\", [4, 12])\ndef test_transformer_combiner(\n        encoder_outputs: tuple,\n        transformer_output_size: int,\n        output_size: int) -&gt; None:\n    encoder_outputs_dict, input_feature_dict = encoder_outputs\n</code></pre> <p>For examples of combiner tests, see <code>tests/ludwig/combiners/test_combiners.py</code>.</p> <p>For more detail about unit testing in Ludiwg, see also Unit Test Design Guidelines.</p>"},{"location":"developer_guide/add_a_dataset/","title":"Add a Dataset","text":"<p>The Ludwig Dataset Zoo is a corpus of various datasets from the web conveniently built into Ludwig.</p> <p>Ludwig datasets automate managing credentials for downloading data from sites like Kaggle, merging multiple files into a single dataset, sharing data parsing code, and loading datasets directly into data frames which can be used to train Ludwig models.</p> <ul> <li>The Ludwig Datasets API is contained in <code>ludwig/datasets/</code>.</li> <li>Dataset configs are defined under <code>ludwig/datasets/configs/</code>.</li> <li>Custom loaders for specific datasets are in <code>ludwig/datasets/loaders/</code>.</li> </ul> <p>Datasets are made available in Ludwig by providing a dataset config .yaml file.  For many datasets, creating this YAML file is the only necessary step.</p>"},{"location":"developer_guide/add_a_dataset/#1-create-a-new-dataset-config","title":"1. Create a new dataset config","text":"<p>Create a new <code>.yaml</code> file under <code>ludwig/datasets/configs/</code> with a name matching the name of the dataset. The config file must have the following required keys:</p> <ul> <li><code>version</code>: The version of the dataset</li> <li><code>name</code>: The name of the dataset. This is the name which will be imported or passed into <code>get_datasets(datset_name)</code>.</li> <li><code>description</code>: Human-readable description of the dataset. May contain multi-line text with links.</li> <li>One of <code>download_urls</code>, <code>kaggle_competition</code>, or <code>kaggle_dataset_id</code>.</li> </ul> <p>Supported compressed archive and data file types will be inferred automatically from the file extension.</p> <p>Note</p> <p>If the dataset has a train/validation/test split used as a benchmark in research papers or Kaggle contests, we recommend preserving the original splits so that Ludwig models may be compared against published results.</p> <p>For the full set of options, see <code>ludwig.datasets.dataset_config.DatasetConfig</code>. If the options provided by <code>DatasetConfig</code> are sufficient to integrate your dataset, skip ahead to step 3. Test your Dataset.</p> <p>If, however, the dataset requires other processing not provided by the default dataset loader, continue to step 2.</p>"},{"location":"developer_guide/add_a_dataset/#2-define-a-dataset-loader-if-needed","title":"2. Define a dataset loader if needed","text":"<p>If the options provided by <code>DatasetConfig</code> do not cover the format of your dataset, or if the dataset requires unique processing before training, you can add python code in a dataset loader.</p> <p>The loader class should inherit from <code>ludwig.datasets.loaders.dataset_loader.DatasetLoader</code>, and its module name should match the name of the dataset.  For example, AG News has a dataset loader <code>agnews.AGNewsLoader</code> in <code>ludwig/datasets/loaders/agnews.py</code>.</p> <p>To instruct Ludwig to use your loader, add the <code>loader</code> property to your dataset config:</p> <pre><code>loader: agnews.AGNewsLoader\n</code></pre> <p>Datasets are processed in four phases:</p> <ol> <li>Download       - The dataset files are downloaded to the cache.</li> <li>Verify         - Hashes of downloaded files are verified.</li> <li>Extract        - The dataset files are extracted from an archive (may be a no-op if data is not archived).</li> <li>Transform      - The dataset is transformed into a format usable for training and is ready to load.<ol> <li>Transform Files      (Files -&gt; Files)</li> <li>Load Dataframe       (Files -&gt; DataFrame)</li> <li>Transform Dataframe  (DataFrame -&gt; DataFrame)</li> <li>Save Processed       (DataFrame -&gt; File)</li> </ol> </li> </ol> <p>For each of these phases, there is a corresponding method in <code>ludwig.datasets.loaders.DatasetLoader</code> which may be overridden to provide custom processing.</p>"},{"location":"developer_guide/add_a_dataset/#3-test-your-dataset","title":"3. Test your dataset","text":"<p>Create a simple training script and ludwig config to ensure that the Ludwig training API runs with the new dataset. For example:</p> <pre><code>from ludwig.api import LudwigModel\nfrom ludwig.datasets import titanic\n\ntraining_set, test_set, _, = titanic.load(split=True)\nmodel = LudwigModel(config=\"model_config.yaml\", logging_level=logging.INFO)\ntrain_stats, _, _ = model.train(training_set=training_set, test_set=test_set, model_name=\"titanic_model\")\n</code></pre> <p>If you have added a custom loader, please also a unit test to ensure that your loader works with future versions. Following the examples below, provide a small sample of the data to the unit test so the test will not need to download the dataset.</p> <p>Examples of unit tests:</p> <ul> <li>Titanic unit test</li> <li>MNIST unit test</li> </ul> <p>Note for Kaggle Datasets</p> <p>In order to test downloading datasets hosted on Kaggle, please follow these instructions to obtain the necessary API credentials. If the dataset is part of a competition, you will also need to accept the competition terms in the Kaggle web UI.</p> <p>For testing, the Titanic example also illustrates how to use a mock kaggle client in tests. Unit tests should be runnable without credentials or internet connectivity.</p>"},{"location":"developer_guide/add_a_dataset/#4-add-a-modeling-example","title":"4. Add a modeling example","text":"<p>Consider sharing an example for how users can train models using your dataset, for example:</p> <ul> <li>Titanic training script</li> <li>MNIST training script</li> </ul>"},{"location":"developer_guide/add_a_decoder/","title":"Add a Decoder","text":""},{"location":"developer_guide/add_a_decoder/#1-add-a-new-decoder-class","title":"1. Add a new decoder class","text":"<p>Source code for decoders lives under <code>ludwig/decoders/</code>. Decoders are grouped into modules by their output feature type. For instance, all new sequence decoders should be added to <code>ludwig/decoders/sequence_decoders.py</code>.</p> <p>Note</p> <p>A decoder may support multiple output types, if so it should be defined in the module corresponding to its most generic supported type. If a decoder is generic with respect to output type, add it to <code>ludwig/decoders/generic_decoders.py</code>.</p> <p>To create a new decoder:</p> <ol> <li>Define a new decoder class. Inherit from <code>ludwig.decoders.base.Decoder</code> or one of its subclasses.</li> <li>Create all layers and state in the <code>__init__</code> method, after calling <code>super().__init__()</code>.</li> <li>Implement your decoder's forward pass in <code>def forward(self, combiner_outputs, **kwargs):</code>.</li> <li>Define a schema class.</li> </ol> <p>Note: <code>Decoder</code> inherits from <code>LudwigModule</code>, which is itself a torch.nn.Module, so all the usual concerns of developing Torch modules apply.</p> <p>All decoder parameters should be provided as keyword arguments to the constructor, and must have a default value. For example the <code>SequenceGeneratorDecoder</code> decoder takes the following list of parameters in its constructor:</p> <pre><code>from ludwig.constants import SEQUENCE, TEXT\nfrom ludwig.decoders.base import Decoder\nfrom ludwig.decoders.registry import register_decoder\n\n@register_decoder(\"generator\", [SEQUENCE, TEXT])\nclass SequenceGeneratorDecoder(Decoder):\n    def __init__(\n        self,\n        vocab_size: int,\n        max_sequence_length: int,\n        cell_type: str = \"gru\",\n        input_size: int = 256,\n        reduce_input: str = \"sum\",\n        num_layers: int = 1,\n        **kwargs,\n    ):\n    super().__init__()\n    # Initialize any modules, layers, or variable state\n</code></pre>"},{"location":"developer_guide/add_a_decoder/#2-implement-forward","title":"2. Implement <code>forward</code>","text":"<p>Actual computation of activations takes place inside the <code>forward</code> method of the decoder. All decoders should have the following signature:</p> <pre><code>    def forward(self, combiner_outputs, **kwargs):\n        # perform forward pass\n        # combiner_hidden_output = combiner_outputs[HIDDEN]\n        # ...\n        # logits = result of decoder forward pass\n        return {LOGITS: logits}\n</code></pre> <p>Inputs</p> <ul> <li>combiner_outputs (Dict[str, torch.Tensor]): The input tensor, which is the output of a combiner or the combination of combiner and the activations of any dependent output decoders. The dictionary of combiner outputs includes a tensor of shape <code>b x h</code>, where <code>b</code> is the batch size and <code>h</code> is the embedding size, or a sequence of embeddings <code>b x s x h</code> where <code>s</code> is the sequence length.</li> </ul> <p>Return</p> <ul> <li>(Dict[str, torch.Tensor]): A dictionary of decoder output tensors.  Typical decoders will return values for the keys <code>LOGITS</code>, <code>PREDICTION</code>, or both (defined in <code>ludwig.constants</code>).</li> </ul>"},{"location":"developer_guide/add_a_decoder/#3-add-the-new-decoder-class-to-the-corresponding-decoder-registry","title":"3. Add the new decoder class to the corresponding decoder registry","text":"<p>Mapping between decoder names in the model definition and decoder classes is made by registering the class in a decoder registry. The decoder registry is defined in <code>ludwig/decoders/registry.py</code>. To register your class, add the <code>@register_decoder</code> decorator on the line above its class definition, specifying the name of the decoder and a list of supported output feature types:</p> <pre><code>@register_decoder(\"generator\", [SEQUENCE, TEXT])\nclass SequenceGeneratorDecoder(Decoder):\n</code></pre>"},{"location":"developer_guide/add_a_decoder/#4-define-a-schema-class","title":"4. Define a schema class","text":"<p>In order to ensure that user config validation for your custom defined decoder functions as desired, we need to define a schema class to go along with the newly defined decoder. To do this, we use a marshmallow_dataclass decorator on a class definition that contains all the inputs to your custom decoder as attributes. For each attribute, we use utility functions to validate that input from the <code>ludwig.schema.utils</code> directory. Lastly, we need to put a reference to this schema class on the custom decoder class. For example:</p> <pre><code>from marshmallow_dataclass import dataclass\n\nfrom ludwig.constants import SEQUENCE, TEXT\nfrom ludwig.schema.decoders.base import BaseDecoderConfig\nfrom ludwig.schema.decoders.utils import register_decoder_config\nimport ludwig.schema.utils as schema_utils\n\n@register_decoder_config(\"generator\", [SEQUENCE, TEXT])\n@dataclass\nclass SequenceGeneratorDecoderConfig(BaseDecoderConfig):\n\n    type: str = schema_utils.StringOptions(options=[\"generator\"], default=\"generator\")\n    vocab_size: int = schema_utils.Integer(default=None, description=\"\")\n    max_sequence_length: int = schema_utils.Integer(default=None, description=\"\")\n    cell_type: str = schema_utils.String(default=\"gru\", description=\"\")\n    input_size: int = schema_utils.Integer(default=256, description=\"\")\n    reduce_input: str = schema_utils.ReductionOptions(default=\"sum\")\n    num_layers: int = schema_utils.Integer(default=1, description=\"\")\n</code></pre> <p>And lastly you should add a reference to the schema class on the custom decoder:</p> <pre><code>    @staticmethod\n    def get_schema_cls():\n        return SequenceGeneratorDecoderConfig\n</code></pre>"},{"location":"developer_guide/add_a_feature_type/","title":"Add a Feature Type","text":""},{"location":"developer_guide/add_a_feature_type/#1-define-the-new-feature-type","title":"1. Define the new feature type","text":"<p>Feature types are defined as constants in <code>ludwig/constants.py</code>.</p> <p>Add the name of the new feature type as a constant:</p> <pre><code>BINARY = \"binary\"\nCATEGORY = \"category\"\n...\nNEW_FEATURE_TYPE = \"new_feature_type_name\"\n</code></pre>"},{"location":"developer_guide/add_a_feature_type/#2-add-feature-classes-in-a-new-python-module","title":"2. Add feature classes in a new python module","text":"<p>Source code for feature classes lives under <code>ludwig/features/</code>. Add the implementation of the new feature into a new python module <code>ludwig/feature/&lt;new_name&gt;_feature.py</code>.</p> <p>Input and output feature classes are defined in the same file, for example <code>CategoryInputFeature</code> and <code>CategoryOutputFeature</code> are defined in <code>ludwig/features/category_feature.py</code>.</p> <p>Input features inherit from <code>ludwig.features.base_feature.InputFeature</code> and corresponding mixin feature classes:</p> <pre><code>class CategoryInputFeature(CategoryFeatureMixin, InputFeature):\n</code></pre> <p>Similarly, output features inherit from the <code>ludwig.features.base_feature.OutputFeature</code> and corresponding mixin feature classes:</p> <pre><code>class CategoryOutputFeature(CategoryFeatureMixin, OutputFeature):\n</code></pre> <p>Feature base classes (<code>InputFeature</code>, <code>OutputFeature</code>) inherit from <code>LudwigModule</code> which is itself a torch.nn.Module, so all the usual concerns of developing Torch modules apply.</p> <p>Mixin classes provide shared preprocessing/postprocessing state and logic, such as the mapping from categories to indices, which are shared by input and output feature implementations. Mixin classes are not torch modules, and do not need to provide a forward method.</p> <pre><code>class CategoryFeatureMixin(BaseFeatureMixin):\n</code></pre>"},{"location":"developer_guide/add_a_feature_type/#3-implement-required-methods","title":"3. Implement required methods","text":""},{"location":"developer_guide/add_a_feature_type/#input-features","title":"Input features","text":""},{"location":"developer_guide/add_a_feature_type/#constructor","title":"Constructor","text":"<p>Feature parameters are provided in a dictionary of key-value pairs as an argument to the constructor.  The <code>feature</code> dictionary should usually be passed to the superclass constructor before initialization:</p> <pre><code>def __init__(self, feature: [str, Any], encoder_obj=None):\n    super().__init__(feature)\n    # Initialize any modules, layers, or variable state\n</code></pre> <p>Inputs</p> <ul> <li>feature: (dict) contains all feature config parameters.</li> <li>encoder_obj: (Encoder, default: <code>None</code>) is an encoder object of the supported type (category encoder, binary encoder, etc.). Input features typically create their own encoder, <code>encoder_obj</code> is only specified when two input features share the same encoder.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#forward","title":"forward","text":"<p>All input features must implement the <code>forward</code> method with the following signature:</p> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n    # perform forward pass\n    # ...\n    # inputs_encoded = result of encoder forward pass\n    return inputs_encoded\n</code></pre> <p>Inputs</p> <ul> <li>inputs (torch.Tensor): The input tensor.</li> </ul> <p>Return</p> <ul> <li>(torch.Tensor): Input data encoded by the input feature's encoder.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#input_shape","title":"input_shape","text":"<pre><code>@property\ndef input_shape(self) -&gt; torch.Size:\n</code></pre> <p>Return</p> <ul> <li>(torch.Size): The fully-specified size of the feature's expected input, without batch dimension.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#output-features","title":"Output features","text":""},{"location":"developer_guide/add_a_feature_type/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, feature: Dict[str, Any], output_features: Dict[str, OutputFeature]):\n    super().__init__(feature, output_features)\n    self.overwrite_defaults(feature)\n    # Initialize any decoder modules, layers, metrics, loss objects, etc...\n</code></pre> <p>Inputs</p> <ul> <li>feature (dict): contains all feature parameters.</li> <li>output_features (dict[Str, OutputFeature]): Dictionary of other output features, only used if this output feature depends on other outputs.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#logits","title":"logits","text":"<p>Computes feature logits from the combiner output (and any features this feature depends on).</p> <pre><code>def logits(self, inputs: Dict[str, torch.Tensor],  **kwargs):\n    hidden = inputs[HIDDEN]\n    # logits = results of decoder operation\n    return logits\n</code></pre> <p>Inputs</p> <ul> <li>inputs (dict): input dictionary which contains the <code>HIDDEN</code> key, whose value is the output of the combiner. Will contain other input keys if this feature depends on other output features.</li> </ul> <p>Return</p> <ul> <li>(torch.Tensor): feature logits.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#create_predict_module","title":"create_predict_module","text":"<p>Creates and returns a <code>torch.nn.Module</code> that converts raw model outputs (logits) to predictions. This module is required for exporting models to Torchscript.</p> <pre><code>def create_predict_module(self) -&gt; PredictModule:\n</code></pre> <p>Return</p> <ul> <li>(PredictModule): A module whose forward method convert feature logits to predictions.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#output_shape","title":"output_shape","text":"<pre><code>@property\ndef output_shape(self) -&gt; torch.Size:\n</code></pre> <p>Return</p> <ul> <li>(torch.Size): The fully-specified size of the feature's output, without batch dimension.</li> </ul>"},{"location":"developer_guide/add_a_feature_type/#feature-mixins","title":"Feature Mixins","text":"<p>If your new feature can re-use the preprocessing and postprocessing logic of an existing feature type, you do not need to implement a new mixin class. If your new feature does require unique pre or post-processing, add a new subclass of <code>ludwig.features.base_feature.BaseFeatureMixin</code>. Implement all abstract methods of <code>BaseFeatureMixin</code>.</p>"},{"location":"developer_guide/add_a_feature_type/#4-add-the-new-feature-classes-to-the-corresponding-feature-registries","title":"4. Add the new feature classes to the corresponding feature registries","text":"<p>Input and output feature registries are defined in <code>ludwig/features/feature_registries.py</code>. Import your new feature classes, and add them to the appropriate registry dictionaries:</p> <pre><code>base_type_registry = {\n    CATEGORY: CategoryFeatureMixin,\n...\n}\ninput_type_registry = {\n    CATEGORY: CategoryInputFeature,\n...\n}\noutput_type_registry = {\n    CATEGORY: CategoryOutputFeature,\n...\n}\n</code></pre>"},{"location":"developer_guide/add_a_feature_type/#5-add-schema-class-definitions-for-new-feature-types","title":"5. Add schema class definitions for new feature types","text":"<p>In order to validate user input against the expected inputs and input types for the new feature type you have defined, we need to create schema classes that will autogenerate the json schema required for validation.</p> <p>If the new feature type will just function as an input feature, you only need to define an input feature schema class. Here is an example of how the category feature schema classes are defined:</p>"},{"location":"developer_guide/add_a_feature_type/#input-feature-type","title":"Input Feature Type","text":"<pre><code>from marshmallow_dataclass import dataclass\n\nfrom ludwig.constants import CATEGORY\nfrom ludwig.schema import utils as schema_utils\nfrom ludwig.schema.encoders.base import BaseEncoderConfig\nfrom ludwig.schema.encoders.utils import EncoderDataclassField\nfrom ludwig.schema.features.base import BaseInputFeatureConfig\nfrom ludwig.schema.features.preprocessing.base import BasePreprocessingConfig\nfrom ludwig.schema.features.preprocessing.utils import PreprocessingDataclassField\nfrom ludwig.schema.features.utils import input_config_registry, output_config_registry\n\n\n@input_config_registry.register(CATEGORY)\n@dataclass\nclass CategoryInputFeatureConfig(BaseInputFeatureConfig):\n\"\"\"CategoryInputFeatureConfig is a dataclass that configures the parameters used for a category input\n    feature.\"\"\"\n\n    preprocessing: BasePreprocessingConfig = PreprocessingDataclassField(feature_type=CATEGORY)\n\n    encoder: BaseEncoderConfig = EncoderDataclassField(\n        feature_type=CATEGORY,\n        default=\"dense\",\n    )\n\n    tied: str = schema_utils.String(\n        default=None,\n        allow_none=True,\n        description=\"Name of input feature to tie the weights of the encoder with.  It needs to be the name of a \"\n        \"feature of the same type and with the same encoder parameters.\",\n    )\n</code></pre> <p>If the new feature type can also be an output feature type, you will need to define an output feature schema class as well:</p>"},{"location":"developer_guide/add_a_feature_type/#output-feature-type","title":"Output Feature Type","text":"<pre><code>from marshmallow_dataclass import dataclass\n\nfrom ludwig.schema import utils as schema_utils\nfrom ludwig.constants import CATEGORY, SOFTMAX_CROSS_ENTROPY\nfrom ludwig.schema.decoders.base import BaseDecoderConfig\nfrom ludwig.schema.decoders.utils import DecoderDataclassField\nfrom ludwig.schema.features.base import BaseOutputFeatureConfig\nfrom ludwig.schema.features.loss.loss import BaseLossConfig\nfrom ludwig.schema.features.loss.utils import LossDataclassField\nfrom ludwig.schema.features.preprocessing.base import BasePreprocessingConfig\nfrom ludwig.schema.features.preprocessing.utils import PreprocessingDataclassField\nfrom ludwig.schema.features.utils import output_config_registry\n\n@output_config_registry.register(CATEGORY)\n@dataclass\nclass CategoryOutputFeatureConfig(BaseOutputFeatureConfig):\n\"\"\"CategoryOutputFeatureConfig is a dataclass that configures the parameters used for a category output\n    feature.\"\"\"\n\n    preprocessing: BasePreprocessingConfig = PreprocessingDataclassField(feature_type=\"category_output\")\n\n    loss: BaseLossConfig = LossDataclassField(\n        feature_type=CATEGORY,\n        default=SOFTMAX_CROSS_ENTROPY,\n    )\n\n    decoder: BaseDecoderConfig = DecoderDataclassField(\n        feature_type=CATEGORY,\n        default=\"classifier\",\n    )\n\n    reduce_input: str = schema_utils.ReductionOptions(\n        default=\"sum\",\n        description=\"How to reduce an input that is not a vector, but a matrix or a higher order tensor, on the first \"\n        \"dimension (second if you count the batch dimension)\",\n    )\n\n    dependencies: list = schema_utils.List(\n        default=[],\n        description=\"List of input features that this feature depends on.\",\n    )\n\n    reduce_dependencies: str = schema_utils.ReductionOptions(\n        default=\"sum\",\n        description=\"How to reduce the dependencies of the output feature.\",\n    )\n\n    top_k: int = schema_utils.NonNegativeInteger(\n        default=3,\n        description=\"Determines the parameter k, the number of categories to consider when computing the top_k \"\n        \"measure. It computes accuracy but considering as a match if the true category appears in the \"\n        \"first k predicted categories ranked by decoder's confidence.\",\n    )\n\n    calibration: bool = schema_utils.Boolean(\n        default=False,\n        description=\"Calibrate the model's output probabilities using temperature scaling.\",\n    )\n</code></pre> <p>Lastly, you need to add a reference to the schema class definitions on your input feature type definitions. So for instance, on the <code>CategoryInputFeature</code> class, we need to add a <code>get_schema_cls</code> method:</p> <pre><code>class CategoryInputFeature(CategoryFeatureMixin, InputFeature):\n\n...\n\n    @staticmethod\n    def get_schema_cls():\n        return CategoryInputFeatureConfig\n</code></pre> <p>Likewise for the output feature class:</p> <pre><code>class CategoryOutputFeature(CategoryFeatureMixin, OutputFeature):\n\n...\n\n    @staticmethod\n    def get_schema_cls():\n        return CategoryOutputFeatureConfig\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/","title":"Add a Hyperopt Algorithm","text":"<p>The hyperparameter optimization design in Ludwig is based on two abstract interfaces: <code>HyperoptSampler</code> and <code>HyperoptExecutor</code>.</p> <p>See Hyperopt configuration for examples of how the sampler and executor are configured.</p>"},{"location":"developer_guide/add_a_hyperopt/#hyperoptsampler","title":"HyperoptSampler","text":"<p><code>HyperoptSampler</code> dictates how to sample hyperparameters values.</p> <p>The sampler is configured by <code>sampler</code> section of the <code>hyperopt</code> section of the Ludwig configuration.</p> <p>Each hyperparameter that should be sampled is declared in <code>hyperopt.parameters</code>, which also specifies additional constraints that the <code>Sampler</code> should honor. For example:</p> <pre><code>hyperopt:\ngoal: minimize\noutput_feature: combined\nmetric: loss\nsplit: validation\nparameters:\ntrainer.learning_rate:\nspace: linear\nrange:\nlow: 0.001\nhigh: 0.1\nsteps: 4\ntext.fc_layers:\nspace: choice\ncategories:\n- [{\"output_size\": 512}, {\"output_size\": 256}]\n- [{\"output_size\": 512}]\n- [{\"output_size\": 256}]\n</code></pre> <p>Here, <code>trainer.learning_rate</code> is sampled in continuously while <code>text.fc_layers</code> is sampled discretely.</p> <p>Note</p> <p>Different <code>HyperoptSampler</code>s are described here.</p>"},{"location":"developer_guide/add_a_hyperopt/#hyperoptexecutor","title":"HyperoptExecutor","text":"<p><code>HyperoptExecutor</code> dictates how to execute the hyperparameter optimization, which operates independently of how hyperparameters are actually sampled.</p> <p>A <code>HyperoptExecutor</code> uses a <code>HyperoptSampler</code> to sample hyperparameters values, usually initializes an execution context, like a multithread pool for instance, and executes the hyperparameter optimization according to the sampler.</p> <p>First, a new batch of parameters values is sampled from the <code>HyperoptSampler</code>. Then, sampled parameters values are merged with the seed Ludwig configuration, with the sampled parameters values overriding the seed's.</p> <p>Training is executed and validation losses and metrics are collected. A <code>(sampled_parameters, statistics)</code> pair is provided to the <code>HyperoptSampler.update</code> function to inform the next sample of hyperparameters.</p> <p>The loop is repeated until all the samples are sampled.</p> <p>Finally, <code>HyperoptExecutor.execute</code> returns a list of dictionaries that each contain: the sampled parameters, metric scores, and other training, validation, and test statistics.</p> <p>The returned list is printed and saved to disk, so that it can also be used as input to hyperparameter optimization visualizations.</p> <p>Note</p> <p>Different <code>HyperoptExecutor</code>s are described here</p>"},{"location":"developer_guide/add_a_hyperopt/#adding-a-hyperoptsampler","title":"Adding a HyperoptSampler","text":""},{"location":"developer_guide/add_a_hyperopt/#1-add-a-new-sampler-class","title":"1. Add a new sampler class","text":"<p>The source code for the base <code>HyperoptSampler</code> class is in <code>ludwig/hyperopt/sampling.py</code>.</p> <p>Classes extending the base class should be defined in this file.</p>"},{"location":"developer_guide/add_a_hyperopt/#__init__","title":"<code>__init__</code>","text":"<pre><code>def __init__(self, goal: str, parameters: Dict[str, Any]):\n</code></pre> <p>The parameters of the base <code>HyperoptStrategy</code> class constructor are:</p> <ul> <li><code>goal</code> which indicates if to minimize or maximize a metric or a loss of any of the output features on any of the splits which is defined in the <code>hyperopt</code> section</li> <li><code>parameters</code> which contains all hyperparameters to optimize with their types and ranges / values.</li> </ul> <p>Example:</p> <pre><code>goal = \"minimize\"\nparameters = {\n    \"training.learning_rate\": {\n        \"type\": \"float\",\n        \"low\": 0.001,\n        \"high\": 0.1,\n        \"steps\": 4,\n        \"scale\": \"linear\"\n    },\n    \"combiner.num_fc_layers\": {\n        \"type\": \"int\",\n        \"low\": 2,\n        \"high\": 6,\n        \"steps\": 3\n    }\n}\n\nsampler = GridSampler(goal, parameters)\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#sample","title":"<code>sample</code>","text":"<pre><code>def sample(self) -&gt; Dict[str, Any]:\n</code></pre> <p><code>sample</code> is a method that yields a new sample according to the sampler. It returns a set of parameters names and their values. If <code>finished()</code> returns <code>True</code>, calling <code>sample</code> would return a <code>IndexError</code>.</p> <p>Example returned value:</p> <pre><code>{'training.learning_rate': 0.005, 'combiner.num_fc_layers': 2, 'utterance.cell_type': 'gru'}\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#sample_batch","title":"<code>sample_batch</code>","text":"<pre><code>def sample_batch(self, batch_size: int = 1) -&gt; List[Dict[str, Any]]:\n</code></pre> <p><code>sample_batch</code> method returns a list of sampled parameters of length equal to or less than <code>batch_size</code>. If <code>finished()</code> returns <code>True</code>, calling <code>sample_batch</code> would return a <code>IndexError</code>.</p> <p>Example returned value:</p> <pre><code>[{'training.learning_rate': 0.005, 'combiner.num_fc_layers': 2, 'utterance.cell_type': 'gru'}, {'training.learning_rate': 0.015, 'combiner.num_fc_layers': 3, 'utterance.cell_type': 'lstm'}]\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#update","title":"<code>update</code>","text":"<pre><code>def update(\n    self,\n    sampled_parameters: Dict[str, Any],\n    metric_score: float\n):\n</code></pre> <p><code>update</code> updates the sampler with the results of previous computation.</p> <ul> <li><code>sampled_parameters</code> is a dictionary of sampled parameters.</li> <li><code>metric_score</code> is the value of the optimization metric obtained for the specified sample.</li> </ul> <p>It is not needed for stateless strategies like grid and random, but is needed for stateful strategies like bayesian and evolutionary ones.</p> <p>Example:</p> <pre><code>sampled_parameters = {\n    'training.learning_rate': 0.005,\n    'combiner.num_fc_layers': 2,\n    'utterance.cell_type': 'gru'\n}\nmetric_score = 2.53463\n\nsampler.update(sampled_parameters, metric_score)\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#update_batch","title":"<code>update_batch</code>","text":"<pre><code>def update_batch(\n    self,\n    parameters_metric_tuples: Iterable[Tuple[Dict[str, Any], float]]\n):\n</code></pre> <p><code>update_batch</code> updates the sampler with the results of previous computation in batch.</p> <ul> <li><code>parameters_metric_tuples</code> a list of pairs of sampled parameters and their respective metric value.</li> </ul> <p>It is not needed for stateless strategies like grid and random, but is needed for stateful strategies like bayesian and evolutionary ones.</p> <p>Example:</p> <pre><code>sampled_parameters = [\n    {\n        'training.learning_rate': 0.005,\n        'combiner.num_fc_layers': 2,\n        'utterance.cell_type': 'gru'\n    },\n    {\n        'training.learning_rate': 0.015,\n        'combiner.num_fc_layers': 5,\n        'utterance.cell_type': 'lstm'\n    }\n]\nmetric_scores = [2.53463, 1.63869]\n\nsampler.update_batch(zip(sampled_parameters, metric_scores))\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#finished","title":"<code>finished</code>","text":"<pre><code>def finished(self) -&gt; bool:\n</code></pre> <p>The <code>finished</code> method return <code>True</code> when all samples have been sampled, return <code>False</code> otherwise.</p>"},{"location":"developer_guide/add_a_hyperopt/#2-add-the-new-sampler-class-to-the-corresponding-sampler-registry","title":"2. Add the new sampler class to the corresponding sampler registry","text":"<p>The <code>sampler_registry</code> contains a mapping between <code>sampler</code> names in the <code>hyperopt</code> section of model definition and <code>HyperoptSampler</code> sub-classes.</p> <p>Add the new sampler to the registry:</p> <pre><code>sampler_registry = {\n    \"random\": RandomSampler,\n    \"grid\": GridSampler,\n    ...,\n    \"new_sampler_name\": NewSamplerClass\n}\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#adding-a-hyperoptexecutor","title":"Adding a HyperoptExecutor","text":""},{"location":"developer_guide/add_a_hyperopt/#1-add-a-new-executor-class","title":"1. Add a new executor class","text":"<p>The source code for the base <code>HyperoptExecutor</code> class is in the <code>ludwig/utils/hyperopt_utils.py</code> module. Classes extending the base class should be defined in the module.</p>"},{"location":"developer_guide/add_a_hyperopt/#__init___1","title":"<code>__init__</code>","text":"<pre><code>def __init__(\n    self,\n    hyperopt_sampler: HyperoptSampler,\n    output_feature: str,\n    metric: str,\n    split: str\n)\n</code></pre> <p>The parameters of the base <code>HyperoptExecutor</code> class constructor are</p> <ul> <li><code>hyperopt_sampler</code> is a <code>HyperoptSampler</code> object that will be used to sample hyperparameters values</li> <li><code>output_feature</code> is a <code>str</code> containing the name of the output feature that we want to optimize the metric or loss of. Available values are <code>combined</code> (default) or the name of any output feature provided in the model definition. <code>combined</code> is a special output feature that allows to optimize for the aggregated loss and metrics of all output features.</li> <li><code>metric</code> is the metric that we want to optimize for. The default one is <code>loss</code>, but depending on the tye of the feature defined in <code>output_feature</code>, different metrics and losses are available. Check the metrics section of the specific output feature type to figure out what metrics are available to use.</li> <li><code>split</code> is the split of data that we want to compute our metric on. By default it is the <code>validation</code> split, but you have the flexibility to specify <code>train</code> or <code>test</code> splits.</li> </ul> <p>Example:</p> <pre><code>goal = \"minimize\"\nparameters = {\n    \"training.learning_rate\": {\n        \"type\": \"float\",\n        \"low\": 0.001,\n        \"high\": 0.1,\n        \"steps\": 4,\n        \"scale\": \"linear\"\n    },\n    \"combiner.num_fc_layers\": {\n        \"type\": \"int\",\n        \"low\": 2,\n        \"high\": 6,\n        \"steps\": 3\n    }\n}\noutput_feature = \"combined\"\nmetric = \"loss\"\nsplit = \"validation\"\n\ngrid_sampler = GridSampler(goal, parameters)\nexecutor = SerialExecutor(grid_sampler, output_feature, metric, split)\n</code></pre>"},{"location":"developer_guide/add_a_hyperopt/#execute","title":"<code>execute</code>","text":"<pre><code>def execute(\n    self,\n    config,\n    dataset=None,\n    training_set=None,\n    validation_set=None,\n    test_set=None,\n    training_set_metadata=None,\n    data_format=None,\n    experiment_name=\"hyperopt\",\n    model_name=\"run\",\n    model_load_path=None,\n    model_resume_path=None,\n    skip_save_training_description=False,\n    skip_save_training_statistics=False,\n    skip_save_model=False,\n    skip_save_progress=False,\n    skip_save_log=False,\n    skip_save_processed_input=False,\n    skip_save_unprocessed_output=False,\n    skip_save_predictions=False,\n    skip_save_eval_stats=False,\n    output_directory=\"results\",\n    gpus=None,\n    gpu_memory_limit=None,\n    allow_parallel_threads=True,\n    use_horovod=None,\n    random_seed=default_random_seed,\n    debug=False,\n    **kwargs\n):\n</code></pre> <p>The <code>execute</code> method executes the hyperparameter optimization. It can leverage the <code>run_experiment</code> function to obtain training and eval statistics and the <code>self.get_metric_score</code> function to extract the metric score from the eval results according to <code>self.output_feature</code>, <code>self.metric</code> and <code>self.split</code>.</p>"},{"location":"developer_guide/add_a_hyperopt/#2-add-the-new-executor-class-to-the-corresponding-executor-registry","title":"2. Add the new executor class to the corresponding executor registry","text":"<p>The <code>executor_registry</code> contains a mapping between <code>executor</code> names in the <code>hyperopt</code> section of model definition and <code>HyperoptExecutor</code> sub-classes. To make a new executor available, add it to the registry:</p> <pre><code>executor_registry = {\n    \"serial\": SerialExecutor,\n    \"parallel\": ParallelExecutor,\n    \"fiber\": FiberExecutor,\n    \"new_executor_name\": NewExecutorClass\n}\n</code></pre>"},{"location":"developer_guide/add_a_loss_function/","title":"Add a Loss Function","text":"<p>At a high level, a loss function evaluates how well a model predicts a dataset. Loss functions should always output a scalar. Lower loss corresponds to a better fit, thus the objective of training is to minimize the loss.</p> <p>Ludwig losses conform to the <code>torch.nn.Module</code> interface, and are declared in <code>ludwig/modules/loss_modules.py</code>. Before implementing a new loss from scratch, check the documentation of torch.nn loss functions to see if the desired loss is available. Adding a torch loss to Ludwig is simpler than implementing a loss from scratch.</p>"},{"location":"developer_guide/add_a_loss_function/#add-a-torch-loss-to-ludwig","title":"Add a torch loss to Ludwig","text":"<p>Torch losses whose call signature takes model outputs and targets i.e. <code>loss(model(input), target)</code> can be added to Ludwig easily by declaring a trivial subclass in <code>ludwig/modules/loss_modules.py</code> and registering the loss for one or more output feature types. This example adds <code>MAELoss</code> (mean absolute error loss) to Ludwig:</p> <pre><code>@register_loss(\"mean_absolute_error\", [NUMBER, TIMESERIES, VECTOR])\nclass MAELoss(torch.nn.L1Loss, LogitsInputsMixin):\n    def __init__(self, **kwargs):\n        super().__init__()\n</code></pre> <p>The <code>@register_loss</code> decorator registers the loss under the name <code>mean_absolute_error</code>, and indicates it is supported for <code>NUMBER</code>, <code>TIMESERIES</code>, and <code>VECTOR</code> output features.</p>"},{"location":"developer_guide/add_a_loss_function/#implement-a-loss-from-scratch","title":"Implement a loss from scratch","text":""},{"location":"developer_guide/add_a_loss_function/#implement-loss-function","title":"Implement loss function","text":"<p>To implement a new loss function, we recommend first implementing it as a function of logits and labels, plus any other configuration parameters. For this example, lets suppose we have implemented the tempered softmax from \"Robust Bi-Tempered Logistic Loss Based on Bregman Divergences\". This loss function takes two constant parameters <code>t1</code> and <code>t2</code>, which we'd like to allow users to specify in the config.</p> <p>Assuming we have the following function:</p> <pre><code>def tempered_softmax_cross_entropy_loss(\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        t1: float, t2: float) -&gt; torch.Tensor:\n    # Computes the loss, returns the result as a torch.Tensor.\n</code></pre>"},{"location":"developer_guide/add_a_loss_function/#define-and-register-module","title":"Define and register module","text":"<p>Next, we'll define a module class which computes our loss function, and add it to the loss registry for <code>CATEGORY</code> output features with <code>@register_loss</code>. <code>LogitsInputsMixin</code> tells Ludwig that this loss should be called with the output feature <code>logits</code>, which are the feature decoder outputs before normalization to a probability distribution.</p> <pre><code>@register_loss(\"tempered_softmax_cross_entropy\", [CATEGORY])\nclass TemperedSoftmaxCrossEntropy(torch.nn.Module, LogitsInputsMixin):\n</code></pre> <p>Note</p> <p>It is possible to define losses on other outputs besides <code>logits</code> but this is not used in Ludwig today. For example, loss could be computed over <code>probabilities</code>, but it is usually more numerically stable to compute from <code>logits</code> (rather than backpropagating loss through a softmax function).</p>"},{"location":"developer_guide/add_a_loss_function/#constructor","title":"constructor","text":"<p>The loss constructor will receive any parameters specified in the config as kwargs. It must provide reasonable defaults for all arguments.</p> <pre><code>def __init__(self, t1: float = 1.0, t2: float = 1.0, **kwargs):\n    super().__init__()\n    self.t1 = t1\n    self.t2 = t2\n</code></pre>"},{"location":"developer_guide/add_a_loss_function/#forward","title":"forward","text":"<p>The forward method is responsible for computing the loss. Here we'll call the <code>tempered_softmax_cross_entropy_loss</code> after ensuring its inputs are the correct type, and return its output averaged over the batch.</p> <pre><code>def forward(self, logits: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    labels = target.long()\n    loss = tempered_softmax_cross_entropy_loss(logits, labels, self.t1, self.t2)\n    return torch.mean(loss)\n</code></pre>"},{"location":"developer_guide/add_a_loss_function/#define-a-loss-schema-class","title":"Define a loss schema class","text":"<p>In order to validate user input against the expected inputs and input types for the new loss you have defined, we need to create a schema class that will autogenerate the json schema required for validation. This class should be defined in <code>ludiwg.schema.features.loss.loss.py</code>. This example adds a schema class for the <code>MAELoss</code> class defined above:</p> <pre><code>@dataclass\nclass MAELossConfig(BaseLossConfig):\n\n    type: str = schema_utils.StringOptions(\n        options=[MEAN_ABSOLUTE_ERROR],\n        description=\"Type of loss.\",\n    )\n\n    weight: float = schema_utils.NonNegativeFloat(\n        default=1.0,\n        description=\"Weight of the loss.\",\n    )\n</code></pre> <p>Lastly, we need to add a reference to this schema class on the loss class. For example, on the <code>MAELoss</code> class defined above, we would add:</p> <pre><code>    @staticmethod\n    def get_schema_cls():\n        return MAELossConfig\n</code></pre>"},{"location":"developer_guide/add_a_metric/","title":"Add a Metric","text":"<p>Metrics are used to report model performance during training and evaluation, and also serve as optimization objectives for hyperparameter optimization.</p> <p>Concretely, metrics are modules which compute a function of the model's output for each batch and aggregate the function's result over all batches. A common example of a metric is the <code>LossMetric</code>, which computes the average batch loss. Metrics are defined in <code>ludwig/modules/metric_modules.py</code>. Ludwig's metrics are designed to be consistent with <code>torchmetrics</code> and conform to the interface of <code>torchmetrics.Metric</code>.</p> <p>Note</p> <p>Before implementing a new metric from scratch, check the torchmetrics documentation to see if the desired function is available there. Torch metrics can often be added to Ludwig trivially, see <code>RMSEMetric</code> in <code>ludwig/modules/metric_modules.py</code> for example.</p>"},{"location":"developer_guide/add_a_metric/#1-add-a-new-metric-class","title":"1. Add a new metric class","text":"<p>For the majority of use cases metrics should be averaged over batches, for this Ludwig provides a <code>MeanMetric</code> class which keeps a running average of its values. The following examples will assume averaging is desired and inherit from <code>MeanMetric</code>. If you need different aggregation behavior replace <code>MeanMetric</code> with <code>LudwigMetric</code> and accumulate the metric values as needed.</p> <p>We'll use <code>TokenAccuracyMetric</code> as an example, which treats each token of a sequence as an independent prediction and computes average accuracy over sequences.</p> <p>First, declare the new metric class in <code>ludwig/modules/metric_modules.py</code>:</p> <pre><code>class TokenAccuracyMetric(MeanMetric):\n</code></pre>"},{"location":"developer_guide/add_a_metric/#2-implement-required-methods","title":"2. Implement required methods","text":""},{"location":"developer_guide/add_a_metric/#get_current_value","title":"get_current_value","text":"<p>If using <code>MeanMetric</code>, compute the value of the metric given a batch of feature outputs and target values in <code>get_current_value</code>.</p> <pre><code>def get_current_value(\n        self, preds: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    # Compute metric over a batch of predictions (preds) and truth values (target).\n    # Aggregate metric over batch.\n    return metric_value\n</code></pre> <p>Inputs</p> <ul> <li>preds (torch.Tensor): A batch of outputs from an output feature which are either predictions, probabilities, or logits depending on the return value of get_inputs.</li> <li>target (torch.Tensor): The batch of true labels for the dataset column corresponding to the metric's output feature.</li> </ul> <p>Return</p> <ul> <li>(torch.Tensor): The computed metric, in most cases this will be a scalar value.</li> </ul>"},{"location":"developer_guide/add_a_metric/#update-and-reset","title":"update and reset","text":"<p>If not using <code>MeanMetric</code>, implement <code>update</code> and <code>reset</code> instead of <code>get_current_value</code>.</p> <pre><code>def update(self, preds: torch.Tensor, target: torch.Tensor) -&gt; None:\n    # Compute metric over a batch of predictions (preds) and truth values (target).\n    # Accumulate metric values or aggregate statistics.\n</code></pre> <p>Inputs</p> <ul> <li>preds (torch.Tensor): A batch of outputs from an output feature which are either predictions, probabilities, or logits depending on the return value of get_inputs.</li> <li>target (torch.Tensor): The batch of true labels for the dataset column corresponding to the metric's output feature.</li> </ul> <pre><code>def reset(self) -&gt; None:\n    # Reset accumulated values.\n</code></pre> <p>Note</p> <p><code>MeanMetric</code>'s update method simply delegates metric computation to <code>get_current_value</code>.</p> <pre><code>def update(self, preds: torch.Tensor, target: torch.Tensor) -&gt; None:\n    self.avg.update(self.get_current_value(preds, target))\n</code></pre>"},{"location":"developer_guide/add_a_metric/#get_objective","title":"get_objective","text":"<p>The return value of <code>get_objective</code> tells Ludwig whether to minimize or maximize this metric in hyperparameter optimization.</p> <pre><code>@classmethod\ndef get_objective(cls):\n    return MAXIMIZE\n</code></pre> <p>Return</p> <ul> <li>(str): How this metric should be optimized, one of MINIMIZE or MAXIMIZE.</li> </ul>"},{"location":"developer_guide/add_a_metric/#get_inputs","title":"get_inputs","text":"<p>Determines which feature output is passed in to this metric's <code>update</code> or <code>get_current_value</code> method. Valid return values are:</p> <ul> <li><code>PREDICTIONS</code>: The predicted values of the output feature.</li> <li><code>PROBABILITIES</code>: The vector of probabilities.</li> <li><code>LOGITS</code>: The vector of outputs of the feature decoder's final layer (before the application of any sigmoid or softmax function).</li> </ul> <pre><code>@classmethod\ndef get_inputs(cls):\n    return PREDICTIONS\n</code></pre> <p>Return</p> <ul> <li>(str): Which output this metric derives its value from, one of <code>PREDICTIONS</code>, <code>PROBABILITIES</code>, or <code>LOGITS</code>.</li> </ul>"},{"location":"developer_guide/add_a_metric/#3-add-the-new-metric-class-to-the-registry","title":"3. Add the new metric class to the registry","text":"<p>Mapping between metric names in the config and metric classes is made by registering the class in a metric registry. The metric registry is defined in <code>ludwig/modules/metric_registry.py</code>. To register your class, add the <code>@register_metric</code> decorator on the line above its class definition, specifying the name of the metric and a list of the supported output feature types:</p> <pre><code>@register_metric(TOKEN_ACCURACY, [SEQUENCE, TEXT])\nclass TokenAccuracyMetric(MeanMetric):\n</code></pre>"},{"location":"developer_guide/add_a_pretrained_model/","title":"Add a Pretrained Model","text":"<p>For text and images, there exist a wide selection of pre-trained models from libraries like huggingface that can be useful to leverage in a Ludwig model, for instance as an encoder.</p> <p>Any pre-trained model implemented as a <code>torch.nn.Module</code> can be used within any <code>LudwigModule</code>, which is itself a torch.nn.Module.</p> <p>For demonstration purposes, we'll walk through how to implement huggingface's pre-trained BERT model as Ludwig text encoder. We recommend reading how to add an encoder as a first step.</p>"},{"location":"developer_guide/add_a_pretrained_model/#1-importload-the-pretrained-model","title":"1. Import/load the pretrained model","text":"<p>Load the pre-trained model in the <code>LudwigModule</code>'s constructor.</p> <pre><code>@register_encoder(\"bert\", TEXT)\nclass BERTEncoder(Encoder):\n    fixed_preprocessing_parameters = {\n        \"word_tokenizer\": \"hf_tokenizer\",\n        \"pretrained_model_name_or_path\": \"feature.pretrained_model_name_or_path\",\n    }\n\n    default_params = {\n        \"pretrained_model_name_or_path\": \"bert-base-uncased\",\n    }\n\n    def __init__(\n        self,\n        max_sequence_length: int,\n        use_pretrained: bool = True,\n        pretrained_model_name_or_path: str = \"bert-base-uncased\",\n        trainable: bool = True,\n        reduce_output: str = \"cls_pooled\",\n        vocab_size: int = 30522,\n        hidden_size: int = 768,\n        num_hidden_layers: int = 12,\n        num_attention_heads: int = 12,\n        intermediate_size: int = 3072,\n        hidden_act: Union[str, Callable] = \"gelu\",\n        hidden_dropout_prob: float = 0.1,\n        attention_probs_dropout_prob: float = 0.1,\n        max_position_embeddings: int = 512,\n        type_vocab_size: int = 2,\n        initializer_range: float = 0.02,\n        layer_norm_eps: float = 1e-12,\n        pad_token_id: int = 0,\n        gradient_checkpointing: bool = False,\n        position_embedding_type: str = \"absolute\",\n        classifier_dropout: float = None,\n        pretrained_kwargs: Dict = None,\n        **kwargs\n    ):\n        super().__init__()\n        try:\n            from transformers import BertConfig, BertModel\n        except ModuleNotFoundError:\n            logger.error(\n                \"The transformers library is not installed. \"\n                \"To use the huggingface pretrained models as a Ludwig text \"\n                \"encoders, please run pip install ludwig[text].\"\n            )\n            sys.exit(-1)\n\n        if use_pretrained:\n            pretrained_kwargs = pretrained_kwargs or {}\n            self.transformer = BertModel.from_pretrained(pretrained_model_name_or_path, **pretrained_kwargs)\n        else:\n            config = BertConfig(\n                vocab_size=vocab_size,\n                hidden_size=hidden_size,\n                num_hidden_layers=num_hidden_layers,\n                num_attention_heads=num_attention_heads,\n                intermediate_size=intermediate_size,\n                hidden_act=hidden_act,\n                hidden_dropout_prob=hidden_dropout_prob,\n                attention_probs_dropout_prob=attention_probs_dropout_prob,\n                max_position_embeddings=max_position_embeddings,\n                type_vocab_size=type_vocab_size,\n                initializer_range=initializer_range,\n                layer_norm_eps=layer_norm_eps,\n                pad_token_id=pad_token_id,\n                gradient_checkpointing=gradient_checkpointing,\n                position_embedding_type=position_embedding_type,\n                classifier_dropout=classifier_dropout,\n            )\n            self.transformer = BertModel(config)\n\n        self.reduce_output = reduce_output\n        if not self.reduce_output == \"cls_pooled\":\n            self.reduce_sequence = SequenceReducer(reduce_mode=reduce_output)\n        if trainable:\n            self.transformer.train()\n        self.transformer.resize_token_embeddings(vocab_size)\n        self.max_sequence_length = max_sequence_length\n</code></pre>"},{"location":"developer_guide/add_a_pretrained_model/#2-call-the-pre-trained-model-in-the-ludwigmodules-forward-pass","title":"2. Call the pre-trained model in the <code>LudwigModule</code>'s forward pass","text":"<pre><code>def forward(self, inputs: torch.Tensor,\n            mask: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:\n    if mask is not None:\n        mask = mask.to(torch.int32)\n    transformer_outputs = self.transformer(\n        input_ids=inputs,\n        attention_mask=mask,\n        token_type_ids=torch.zeros_like(inputs),\n    )\n\n    # Optionally reduce output.\n    if self.reduce_output == \"cls_pooled\":\n        hidden = transformer_outputs[1]\n    else:\n        hidden = transformer_outputs[0][:, 1:-1, :]\n        hidden = self.reduce_sequence(hidden, self.reduce_output)\n\n    return {\"encoder_output\": hidden}\n</code></pre>"},{"location":"developer_guide/add_a_pretrained_model/#3-use-pre-trained-models","title":"3. Use pre-trained models","text":"<p>Once the encoder has been registered, users can use the encoder right away in their Ludwig config.</p> <pre><code>input_features:\n- name: description\ntype: text\nencoder: bert\ntrainable: false\nmax_sequence_length: 128\nnum_attention_heads: 3\n</code></pre>"},{"location":"developer_guide/add_a_tokenizer/","title":"Add a Tokenizer","text":"<p>Tokenizers transform a text string into a sequence of tokens. Ludwig will call the tokenizer for each text column it is specified in, for each row of the dataset during preprocessing. It will then collect a list of unique tokens and assign an integer index to each unique token. This ordered list of unique tokens is called the vocabulary, and will be used by encoders to convert tokens to embeddings and by decoders to convert output predictions to tokens.</p> <p>A tokenizer is primarily responsible for splitting a string into a list of tokens, and may optionally perform other processing usually for the purpose of reducing the size of the vocabulary. Some examples of processing tokenizers may perform include:</p> <ul> <li>Splitting on a delimiter ex. underscore \"_\" or comma \",\".</li> <li>Removing punctuation characters</li> <li>Removing stop words, for example \"a\", \"an\", \"the\" in English.</li> <li>Lemmatization: Grouping inflected forms of the same word i.e. \"car\", \"cars\", \"car's\", \"cars'\" -&gt; \"car\"</li> </ul> <p>A tokenizer, once registered, can be used to preprocess any text input column by specifying its name as the value of <code>tokenizer</code> in the <code>preprocessing</code> config dictionary:</p> <pre><code>input_features:\n-   name: title\ntype: text\npreprocessing:\ntokenizer: &lt;NEW_TOKENIZER&gt;\n</code></pre> <p>Tokenizers are defined in <code>ludwig/utils/tokenizers.py</code>. To add a tokenizer, define a new subclass of <code>BaseTokenizer</code> and add it to the registry.</p>"},{"location":"developer_guide/add_a_tokenizer/#1-add-a-new-tokenizer-class","title":"1. Add a new tokenizer class","text":"<p>Tokenizers may define an optional constructor which can receive arguments from the config. Most tokenizers have no config parameters, and do not need a constructor. For an example of a tokenizer which uses a parameter in its constructor, see <code>HFTokenizer</code>.</p> <p>The <code>__call__</code> method does the actual processing, is called with a single string argument, and is expected to return a list of strings. The tokenizer will be called once for each example in the dataset during preprocessing. Preprocessed token sequences will be cached on disk in .hdf5 files and re-used in training and validation, thus the tokenizer will not be called during training.</p> <pre><code>class NewTokenizer(BaseTokenizer):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Initialize any variables or state\n\n    def __call__(self, text: str) -&gt; List[str]:\n        # tokenized_text = result of tokenizing\n        return tokenized_text\n</code></pre>"},{"location":"developer_guide/add_a_tokenizer/#2-add-the-tokenizer-to-the-registry","title":"2. Add the tokenizer to the registry","text":"<p>Tokenizer names are mapped to their implementations in the <code>tokenizer_registry</code> dictionary at the bottom of <code>ludwig/utils/tokenizers.py</code>.</p> <pre><code>tokenizer_registry = {\n    \"characters\": CharactersToListTokenizer,\n    \"space\": SpaceStringToListTokenizer,\n    ...\n    \"new_tokenizer\": NewTokenizer,  # Add your tokenizer as a new entry in the registry.\n</code></pre>"},{"location":"developer_guide/add_an_encoder/","title":"Add an Encoder","text":""},{"location":"developer_guide/add_an_encoder/#1-add-a-new-encoder-class","title":"1. Add a new encoder class","text":"<p>Source code for encoders lives under <code>ludwig/encoders/</code>. Encoders are grouped into modules by their input feature type. For instance, all new sequence encoders should be added to <code>ludwig/encoders/sequence_encoders.py</code>.</p> <p>Note</p> <p>An encoder may support multiple types, if so it should be defined in the module corresponding to its most generic supported type. If an encoder is generic with respect to input type, add it to <code>ludwig/encoders/generic_encoders.py</code>.</p> <p>To create a new encoder:</p> <ol> <li>Define a new encoder class. Inherit from <code>ludwig.encoders.base.Encoder</code> or one of its subclasses.</li> <li>Create all layers and state in the <code>__init__</code> method, after calling <code>super().__init__()</code>.</li> <li>Implement your encoder's forward pass in <code>def forward(self, inputs, mask=None):</code>.</li> <li>Define <code>@property input_shape</code> and <code>@property output_shape</code>.</li> <li>Define a schema class.</li> </ol> <p>Note: <code>Encoder</code> inherits from <code>LudwigModule</code>, which is itself a torch.nn.Module, so all the usual concerns of developing Torch modules apply.</p> <p>All encoder parameters should be provided as keyword arguments to the constructor, and must have a default value. For example the <code>StackedRNN</code> encoder takes the following list of parameters in its constructor:</p> <pre><code>from ludwig.constants import AUDIO, SEQUENCE, TEXT, TIMESERIES\nfrom ludwig.encoders.base import Encoder\nfrom ludwig.encoders.registry import register_encoder\n\n@register_encoder(\"rnn\", [AUDIO, SEQUENCE, TEXT, TIMESERIES])\nclass StackedRNN(Encoder):\n    def __init__(\n        self,\n        should_embed=True,\n        vocab=None,\n        representation=\"dense\",\n        embedding_size=256,\n        embeddings_trainable=True,\n        pretrained_embeddings=None,\n        embeddings_on_cpu=False,\n        num_layers=1,\n        max_sequence_length=None,\n        state_size=256,\n        cell_type=\"rnn\",\n        bidirectional=False,\n        activation=\"tanh\",\n        recurrent_activation=\"sigmoid\",\n        unit_forget_bias=True,\n        recurrent_initializer=\"orthogonal\",\n        dropout=0.0,\n        recurrent_dropout=0.0,\n        fc_layers=None,\n        num_fc_layers=0,\n        output_size=256,\n        use_bias=True,\n        weights_initializer=\"xavier_uniform\",\n        bias_initializer=\"zeros\",\n        norm=None,\n        norm_params=None,\n        fc_activation=\"relu\",\n        fc_dropout=0,\n        reduce_output=\"last\",\n        **kwargs,\n    ):\n    super().__init__()\n    # Initialize any modules, layers, or variable state\n</code></pre>"},{"location":"developer_guide/add_an_encoder/#2-implement-forward-input_shape-and-output_shape","title":"2. Implement <code>forward</code>, <code>input_shape</code>, and <code>output_shape</code>","text":"<p>Actual computation of activations takes place inside the <code>forward</code> method of the encoder. All encoders should have the following signature:</p> <pre><code>    def forward(self, inputs: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        # perform forward pass\n        # ...\n        # output_tensor = result of forward pass\n        return {\"encoder_output\": output_tensor}\n</code></pre> <p>Inputs</p> <ul> <li>inputs (torch.Tensor): input tensor.</li> <li>mask (torch.Tensor, default: <code>None</code>): binary tensor indicating which values in inputs should be masked out. Note: mask is not required, and is not implemented for most encoder types.</li> </ul> <p>Return</p> <ul> <li>(dict): A dictionary containing the key <code>encoder_output</code> whose value is the encoder output tensor. <code>{\"encoder_output\": output_tensor}</code></li> </ul> <p>The <code>input_shape</code> and <code>output_shape</code> properties must return the fully-specified shape of the encoder's expected input and output, without batch dimension:</p> <pre><code>    @property\n    def input_shape(self) -&gt; torch.Size:\n        return torch.Size([self.max_sequence_length])\n\n    @property\n    def output_shape(self) -&gt; torch.Size:\n        return self.recurrent_stack.output_shape\n</code></pre>"},{"location":"developer_guide/add_an_encoder/#3-add-the-new-encoder-class-to-the-encoder-registry","title":"3. Add the new encoder class to the encoder registry","text":"<p>Mapping between encoder names in the model definition and encoder classes is made by registering the class in an encoder registry. The encoder registry is defined in <code>ludwig/encoders/registry.py</code>. To register your class, add the <code>@register_encoder</code> decorator on the line above its class definition, specifying the name of the encoder and a list of supported input feature types:</p> <pre><code>@register_encoder(\"rnn\", [AUDIO, SEQUENCE, TEXT, TIMESERIES])\nclass StackedRNN(Encoder):\n</code></pre>"},{"location":"developer_guide/add_an_encoder/#4-define-a-schema-class","title":"4. Define a schema class","text":"<p>In order to ensure that user config validation for your custom defined encoder functions as desired, we need to define a schema class to go along with the newly defined encoder. To do this, we use a marshmallow_dataclass decorator on a class definition that contains all the inputs to your custom encoder as attributes. For each attribute, we use utility functions to validate that input from the <code>ludwig.schema.utils</code> directory. Lastly, we need to put a reference to this schema class on the custom encoder class. For example:</p> <pre><code>from marshmallow_dataclass import dataclass\n\nfrom ludwig.constants import SEQUENCE, TEXT\nfrom ludwig.schema.encoders.base import BaseEncoderConfig\nfrom ludwig.schema.encoders.utils import register_encoder_config\nimport ludwig.schema.utils as schema_utils\n\n@register_encoder_config(\"stacked_rnn\", [SEQUENCE, TEXT])\n@dataclass\nclass StackedRNNConfig(BaseEncoderConfig):\n        type: str = schema_utils.StringOptions(options=[\"stacked_rnn\"], default=\"stacked_rnn\")\n        should_embed: bool = schema_utils.Boolean(default=True, description=\"\")\n        vocab: list = schema_utils.List(list_type=str, default=None, description=\"\")\n        representation: str = schema_utils.StringOptions(options=[\"sparse\", \"dense\"], default=\"dense\", description=\"\")\n        embedding_size: int = schema_utils.Integer(default=256, description=\"\")\n        embeddings_trainable: bool = schema_utils.Boolean(default=True, description=\"\")\n        pretrained_embeddings: str = schema_utils.String(default=None, description=\"\")\n        embeddings_on_cpu: bool = schema_utils.Boolean(default=False, description=\"\")\n        num_layers: int = schema_utils.Integer(default=1, description=\"\")\n        max_sequence_length: int = schema_utils.Integer(default=None, description=\"\")\n        state_size: int = schema_utils.Integer(default=256, description=\"\")\n        cell_type: str = schema_utils.StringOptions(\n            options=[\"rnn\", \"lstm\", \"lstm_block\", \"ln\", \"lstm_cudnn\", \"gru\", \"gru_block\", \"gru_cudnn\"], \n            default=\"rnn\", description=\"\"\n        )\n        bidirectional: bool = schema_utils.Boolean(default=False, description=\"\")\n        activation: str = schema_utils.ActivationOptions(default=\"tanh\", description=\"\")\n        recurrent_activation: str = schema_utils.activations(default=\"sigmoid\", description=\"\")\n        unit_forget_bias: bool = schema_utils.Boolean(default=True, description=\"\")\n        recurrent_initializer: str = schema_utils.InitializerOptions(default=\"orthogonal\", description=\"\")\n        dropout: float = schema_utils.FloatRange(default=0.0, min=0, max=1, description=\"\")\n        recurrent_dropout: float = schema_utils.FloatRange(default=0.0, min=0, max=1, description=\"\")\n        fc_layers: list = schema_utils.DictList(default=None, description=\"\")\n        num_fc_layers: int = schema_utils.NonNegativeInteger(default=0, description=\"\")\n        output_size: int = schema_utils.Integer(default=256, description=\"\")\n        use_bias: bool = schema_utils.Boolean(default=True, description=\"\")\n        weights_initialize: str = schema_utils.InitializerOptions(default=\"xavier_uniform\", description=\"\")\n        bias_initializer: str = schema_utils.InitializerOptions(default=\"zeros\", description=\"\")\n        norm: str = schema_utils.StringOptions(options=[\"batch\", \"layer\"], default=None, description=\"\")\n        norm_params: dict = schema_utils.Dict(default=None, description=\"\")\n        fc_activation: str = schema_utils.ActivationOptions(default=\"relu\", description=\"\")\n        fc_dropout: float = schema_utils.FloatRange(default=0.0, min=0, max=1, description=\"\")\n        reduce_output: str = schema_utils.ReductionOptions(default=\"last\", description=\"\")\n</code></pre> <p>And lastly you should add a reference to the schema class on the custom encoder:</p> <pre><code>    @staticmethod\n    def get_schema_cls():\n        return StackedRNNConfig\n</code></pre>"},{"location":"developer_guide/add_an_integration/","title":"Add an Integration","text":"<p>Ludwig provides an open-ended method of third-party system integration. This makes it easy to integrate other systems or services with Ludwig which can be enabled simply by passing a flag to the command line interface.</p> <p>To contribute an integration, follow these steps:</p>"},{"location":"developer_guide/add_an_integration/#1-create-a-python-file-in-ludwigcontribs","title":"1. Create a Python file in <code>ludwig/contribs/</code>","text":"<p>The file should have an obvious name associated with the third-party system it integrates with i.e. <code>comet.py</code>, <code>wandb.py</code>. In this example, it is called <code>my_callback.py</code>.</p>"},{"location":"developer_guide/add_an_integration/#2-create-callback-class","title":"2. Create Callback class","text":"<p>Create a new class implementing the <code>ludwig.callbacks.Callback</code> interface. The new class should have a name associated with the third-party system it integrates with, matching its file name.</p> <pre><code>from ludwig.callbacks import Callback\n\nclass MyCallback(Callback):\n</code></pre>"},{"location":"developer_guide/add_an_integration/#3-implement-callback-methods","title":"3. Implement callback methods","text":"<p>Ludwig provides the following callbacks which you can implement to add functionality to Ludwig. All the following methods are optional:</p> <pre><code> def on_cmdline(self, cmd: str, *args: List[str]):\n\"\"\"Called when Ludwig is run on the command line with the callback enabled.\n\n     :param cmd: The Ludwig subcommand being run, ex.\n                 \"train\", \"evaluate\", \"predict\", ...\n     :param args: The full list of command-line arguments (sys.argv).\n     \"\"\"\n     pass\n\n def on_preprocess_start(self, config: Dict[str, Any]):\n\"\"\"Called before preprocessing starts.\n\n     :param config: The config dictionary.\n     \"\"\"\n     pass\n\n def on_preprocess_end(\n         self,\n         training_set,\n         validation_set,\n         test_set,\n         training_set_metadata: Dict[str, Any]\n    ):\n\"\"\"Called after preprocessing ends.\n\n     :param training_set: The training set.\n     :type training_set: ludwig.dataset.base.Dataset\n     :param validation_set: The validation set.\n     :type validation_set: ludwig.dataset.base.Dataset\n     :param test_set: The test set.\n     :type test_set: ludwig.dataset.base.Dataset\n     :param training_set_metadata: Values inferred from the training set,\n            including preprocessing settings, vocabularies, feature statistics,\n            etc. Same as training_set_metadata.json.\n     \"\"\"\n\n     pass\n\n def on_hyperopt_init(self, experiment_name: str):\n\"\"\"Called to initialize state before hyperparameter optimization begins.\n\n     :param experiment_name: The name of the current experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_preprocessing_start(self, experiment_name: str):\n\"\"\"Called before data preprocessing for hyperparameter optimization begins.\n\n     :param experiment_name: The name of the current experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_preprocessing_end(self, experiment_name: str):\n\"\"\"Called after data preprocessing for hyperparameter optimization is\n     completed.\n\n     :param experiment_name: The name of the current experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_start(self, experiment_name: str):\n\"\"\"Called before any hyperparameter optimization trials are started.\n\n     :param experiment_name: The name of the current experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_end(self, experiment_name: str):\n\"\"\"Called after all hyperparameter optimization trials are completed.\n\n     :param experiment_name: The name of the current experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_trial_start(self, parameters: Dict[str, Any]):\n\"\"\"Called before the start of each hyperparameter optimization trial.\n\n     :param parameters: The complete dictionary of parameters for this\n            hyperparameter optimization experiment.\n     \"\"\"\n     pass\n\n def on_hyperopt_trial_end(self, parameters: Dict[str, Any]):\n\"\"\"Called after the end of each hyperparameter optimization trial.\n\n     :param parameters: The complete dictionary of parameters for this\n            hyperparameter optimization experiment.\n     \"\"\"\n     pass\n\n def on_train_init(\n     self,\n     base_config: Dict[str, Any],\n     experiment_directory: str,\n     experiment_name: str,\n     model_name: str,\n     output_directory: str,\n     resume: Union[str, None],\n ):\n\"\"\"Called after preprocessing, but before the creation of the model and\n     trainer objects.\n\n     :param base_config: The user-specified config, before the insertion of\n            defaults or inferred values.\n     :param experiment_directory: The experiment directory, same as\n            output_directory if no experiment specified.\n     :param experiment_name: The experiment name.\n     :param model_name: The model name.\n     :param output_directory: file path to where training results are stored.\n     :param resume: model directory to resume training from, or None.\n     \"\"\"\n     pass\n\n def on_train_start(\n     self,\n     model,\n     config: Dict[str, Any],\n     config_fp: Union[str, None],\n ):\n\"\"\"Called after creation of trainer, before the start of training.\n\n     :param model: The ludwig model.\n     :type model: ludwig.utils.torch_utils.LudwigModule\n     :param config: The config dictionary.\n     :param config_fp: The file path to the config, or none if config was passed\n            to stdin.\n     \"\"\"\n     pass\n\n def on_train_end(self, output_directory: str):\n\"\"\"Called at the end of training, before the model is saved.\n\n     :param output_directory: file path to where training results are stored.\n     \"\"\"\n     pass\n\n def on_trainer_train_setup(self, trainer, save_path: str, is_coordinator: bool):\n\"\"\"Called in every trainer (distributed or local) before training starts.\n\n     :param trainer: The trainer instance.\n     :type trainer: trainer: ludwig.models.Trainer\n     :param save_path: The path to the directory model is saved in.\n     :param is_coordinator: Is this trainer the coordinator.\n     \"\"\"\n     pass\n\n def on_trainer_train_teardown(\n         self, trainer, progress_tracker, is_coordinator: bool\n     ):\n\"\"\"Called in every trainer (distributed or local) after training completes.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param is_coordinator: Is this trainer the coordinator.\n     \"\"\"\n     pass\n\n def on_batch_start(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator only before each batch.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_batch_end(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator only after each batch.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_epoch_start(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator only before the start of each epoch.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_epoch_end(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator only after the end of each epoch.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_validation_start(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator before validation starts.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_validation_end(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator after validation is complete.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_test_start(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator before testing starts.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_test_end(self, trainer, progress_tracker, save_path: str):\n\"\"\"Called on coordinator after testing ends.\n\n     :param trainer: The trainer instance.\n     :type trainer: ludwig.models.trainer.Trainer\n     :param progress_tracker: An object which tracks training progress.\n     :type progress_tracker: ludwig.models.trainer.ProgressTracker\n     :param save_path: The path to the directory model is saved in.\n     \"\"\"\n     pass\n\n def on_build_metadata_start(self, df, mode: str):\n\"\"\"Called before building metadata for dataset.\n\n     :param df: The dataset.\n     :type df: pd.DataFrame\n     :param mode: \"prediction\", \"training\", or None.\n     \"\"\"\n     pass\n\n def on_build_metadata_end(self, df, mode):\n\"\"\"Called after building dataset metadata.\n\n     :param df: The dataset.\n     :type df: pd.DataFrame\n     :param mode: \"prediction\", \"training\", or None.\n     \"\"\"\n     pass\n\n def on_build_data_start(self, df, mode):\n\"\"\"Called before build_data, which does preprocessing, handling missing\n     values, adding metadata to training_set_metadata.\n\n     :param df: The dataset.\n     :type df: pd.DataFrame\n     :param mode: \"prediction\", \"training\", or None.\n     \"\"\"\n     pass\n\n def on_build_data_end(self, df, mode):\n\"\"\"Called after build_data completes.\n\n     :param df: The dataset.\n     :type df: pd.DataFrame\n     :param mode: \"prediction\", \"training\", or None.\n     \"\"\"\n     pass\n\n def on_evaluation_start(self):\n\"\"\"Called before preprocessing for evaluation.\"\"\"\n     pass\n\n def on_evaluation_end(self):\n\"\"\"Called after evaluation is complete.\"\"\"\n     pass\n\n def on_visualize_figure(self, fig):\n\"\"\"Called after a visualization is generated.\n\n     :param fig: The figure.\n     :type fig: matplotlib.figure.Figure\n     \"\"\"\n     pass\n\n def prepare_ray_tune(\n         self,\n         train_fn: Callable,\n         tune_config: Dict[str, Any],\n         tune_callbacks: List[Callable]\n     ):\n\"\"\"Configures Ray Tune callback and config.\n\n     :param train_fn: The function which runs the experiment trial.\n     :param tune_config: The ray tune configuration dictionary.\n     :param tune_callbacks: List of callbacks (not used yet).\n\n     :returns: Tuple[Callable, Dict] The train_fn and tune_config, which will be\n               passed to ray tune.\n     \"\"\"\n     return train_fn, tune_config\n\n @staticmethod\n def preload():\n\"\"\"Will always be called when Ludwig CLI is invoked, preload gives the\n     callback an opportunity to import or create any shared resources.\n\n     Importing required 3rd-party libraries should be done here i.e. import wandb.\n     preload is guaranteed to be called before any other callback method, and will\n     only be called once per process.\n     \"\"\"\n     pass\n</code></pre> <p>If you would like to add additional actions not already handled by the above:</p> <ol> <li>Add them to the appropriate calling location.</li> <li>Add the associated method to your callback class.</li> <li>Write a docstring, and add it to this documentation page.</li> </ol> <p>See existing calls in <code>ludwig/callbacks.py</code> as a pattern to follow.</p>"},{"location":"developer_guide/add_an_integration/#4-import-the-new-callback","title":"4. Import the new callback","text":"<p>In <code>ludwig/contribs/__init__.py</code> add an import in this pattern, using your module and class names:</p> <pre><code>from my_callback import MyCallback\n</code></pre>"},{"location":"developer_guide/add_an_integration/#5-register-a-flag-for-the-callback","title":"5. Register a flag for the callback","text":"<p>In <code>ludwig/contribs/__init__.py</code> in the <code>contrib_registry[\"classes\"]</code> dictionary, add a key/value pair where the key is the flag which enables the callback and the value is the class:</p> <pre><code>contrib_registry = {\n    ...,\n    \"myflag\": MyCallback,\n}\n</code></pre>"},{"location":"developer_guide/api_annotations/","title":"Ludwig API Guarantees","text":"<p>It's exciting to see the creative ways Ludwig has been integrated into various data science workflows and products.</p> <p>To better support the engineers and scientists who use Ludwig as a platform, Ludwig has stability guarantees and expectations defined within the codebase. To make API stability clear in code, we\u2019ve adopted the python decorators below. You can find their python implementations in this module within the Ludwig codebase.</p>"},{"location":"developer_guide/api_annotations/#publicapi","title":"PublicAPI","text":"<p>Public APIs are classes and functions exposed to end users of Ludwig.  There are two types of PublicAPI, distinguished by the stability argument. If stability is not specified, it should be assumed that <code>stability=\u201dstable\u201d</code>.</p>"},{"location":"developer_guide/api_annotations/#publicapistabilitystable","title":"PublicAPI(stability=\u201dstable\u201d)","text":"<p>A stable PublicAPI means the API is mature and will not be changed or removed in minor Ludwig releases. It may be changed in major releases, but a deprecation message will be provided in a version before the change.</p>"},{"location":"developer_guide/api_annotations/#publicapistabilityexperimental","title":"PublicAPI(stability=\u201dexperimental\u201d)","text":"<p>An experimental PublicAPI is for new public features which are still in development. These APIs should be used by advanced users who are tolerant to and expect breaking changes. They will likely harden over the next 1-2 Ludwig releases and become stable PublicAPIs.</p>"},{"location":"developer_guide/api_annotations/#developerapi","title":"DeveloperAPI","text":"<p>Developer APIs are lower-level methods explicitly exposed to advanced Ludwig users and library developers. Their interfaces may change across minor Ludwig releases.</p>"},{"location":"developer_guide/api_annotations/#deprecated","title":"Deprecated","text":"<p>Deprecated APIs may be removed in future releases of Ludwig. Deprecated annotations will include a message with recommended alternatives, such as when a function has moved to a different import path, or the arguments of a function have changed.</p>"},{"location":"developer_guide/codebase_structure/","title":"Codebase Structure","text":"<pre><code>\u251c\u2500\u2500 docker                 - Ludwig Docker images\n\u251c\u2500\u2500 examples               - Configs demonstrating Ludwig on various tasks\n\u251c\u2500\u2500 ludwig                 - Ludwig library source code\n\u2502   \u251c\u2500\u2500 automl             - Configurations, defaults, and utilities for AutoML\n\u2502   \u251c\u2500\u2500 backend            - Execution backends (local, horovod, ray)\n\u2502   \u251c\u2500\u2500 benchmarking       - Performance benchmarks for training and hyperopt\n\u2502   \u251c\u2500\u2500 combiners          - Combiners used in ECD models\n\u2502   \u251c\u2500\u2500 contribs           - 3rd-party integrations (MLFlow, WandB, Comet)\n\u2502   \u251c\u2500\u2500 data               - Data loading, pre/postprocessing, sampling\n\u2502   \u251c\u2500\u2500 datasets           - Ludwig Dataset Zoo: API to download pre-configured datasets.\n\u2502   \u251c\u2500\u2500 decoders           - Output feature decoders\n\u2502   \u251c\u2500\u2500 encoders           - Input feature encoders\n\u2502   \u251c\u2500\u2500 explain            - Utilities for explaining model predictions\n\u2502   \u251c\u2500\u2500 features           - Implementations of feature types\n\u2502   \u251c\u2500\u2500 hyperopt\n\u2502   \u251c\u2500\u2500 models             - Implementations of ECD, trainer, predictor.\n\u2502   \u251c\u2500\u2500 modules            - Torch modules including layers, metrics, and losses\n\u2502   \u251c\u2500\u2500 profiling          - Dataset profiles\n\u2502   \u251c\u2500\u2500 schema             - The complete schema of the ludwig config.yaml\n\u2502   \u251c\u2500\u2500 trainers\n\u2502   \u251c\u2500\u2500 utils              - Various internal utilities used by ludwig python modules\n\u2502   \u251c\u2500\u2500 api.py             - Entry point for python API. Declares LudwigModel.\n\u2502   \u251c\u2500\u2500 api_annotations.py - Provides @PublicAPI, @DevelopAPI annotation decorators\n\u2502   \u2514\u2500\u2500 cli.py             - ludwig command-line tool\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 integration_tests  - End-to-end tests of Ludwig workflows\n    \u2514\u2500\u2500 ludwig             - Unit tests. Subdirectories match ludwig/ structure\n</code></pre> <p>The codebase is organized in a modular, datatype / feature centric way. Adding a feature for a new datatype can be done with minimal edits to existing code:</p> <ol> <li>Add a module implementing the new feature</li> <li>Import it in the appropriate registry file i.e. <code>ludwig/features/feature_registries.py</code></li> <li>Add the new module to the intended registries i.e. <code>input_type_registry</code></li> </ol> <p>All datatype-specific logic lives in the corresponding feature module, all of which are under <code>ludwig/features/</code>.</p>"},{"location":"developer_guide/codebase_structure/#features","title":"Features","text":"<p>Feature classes provide raw data preprocessing logic specific to each data type in datatype mixin classes, i.e. <code>BinaryFeatureMixin</code>, <code>NumberFeatureMixin</code>, <code>CategoryFeatureMixin</code>. Feature mixins contain data preprocessing functions to obtain feature metadata (<code>get_feature_meta</code>, one-time dataset-wide operations to collect things like min, max, average, vocabulary, etc.) and to transform raw data into tensors using the previously calculated metadata (<code>add_feature_data</code>, which usually work on a dataset row basis).</p> <p>Output features also contain datatype-specific logic to compute data postprocessing, to transform model predictions back into data space, and output metrics such as loss or accuracy.</p>"},{"location":"developer_guide/codebase_structure/#model-architectures","title":"Model Architectures","text":"<p>Encoders and decoders are modularized as well (they are under <code>ludwig/encoders/</code> and <code>ludwig/decoders/</code> respectively) so that they can be used by multiple features. For example sequence encoders are shared by text, sequence, and timeseries features.</p> <p>Various model architecture components which can be reused are also split into dedicated modules (i.e. convolutional modules, fully connected layers, attention, etc...) which are available in <code>ludwig/modules/</code>.</p>"},{"location":"developer_guide/codebase_structure/#training-and-inference","title":"Training and Inference","text":"<p>The training logic resides in <code>ludwig/trainers/trainer.py</code> which initializes a training session, feeds the data, and executes the training loop. Prediction logic including batch prediction and evaluation resides in <code>ludwig/models/predictor.py</code>.</p>"},{"location":"developer_guide/codebase_structure/#ludwig-cli","title":"Ludwig CLI","text":"<p>The command line interface is managed by the <code>ludwig/cli.py</code> script, which imports the other scripts in the <code>ludwig/</code> top-level directory which perform various sub-commands (experiment, evaluate, export, visualize, etc...).</p> <p>The programmatic interface (which is also used by the CLI commands) is available in the <code>ludwig/api.py</code>.</p>"},{"location":"developer_guide/codebase_structure/#testing","title":"Testing","text":"<p>All test code is in the <code>tests/</code> directory. The <code>tests/integration_tests/</code> subdirectory contains test cases which aim to provide end-to-end test coverage of all workflows provided by Ludwig.</p> <p>The <code>tests/ludwig/</code> directory contains unit tests, organized in a subdirectory tree parallel to the <code>ludwig/</code> source tree. For more details on testing, see Style Guidelines and Tests.</p>"},{"location":"developer_guide/codebase_structure/#misc","title":"Misc","text":"<p>Hyperparameter optimization logic is implemented in the scripts in the <code>ludwig/hyperopt/</code> package.</p> <p>The <code>ludwig/utils/</code> package contains various internal utilities used by ludwig python modules.</p> <p>Finally the <code>ludwig/contrib/</code> packages contains user contributed code that integrates with external libraries.</p>"},{"location":"developer_guide/contributing/","title":"How to Contribute","text":""},{"location":"developer_guide/contributing/#contributing","title":"Contributing","text":"<p>Everybody is welcome to contribute, and we value everybody\u2019s contribution. Code is thus not the only way to help the community. Answering questions, helping others, reaching out and improving the documentation are immensely valuable contributions as well.</p> <p>It also helps us if you spread the word: reference the library in blog posts on the awesome projects it made possible, shout out on Twitter every time it has helped you, or simply star the repo to say \"thank you\".</p> <p>Check out the official ludwig docs to get oriented around the codebase, and join the community!</p>"},{"location":"developer_guide/contributing/#open-issues","title":"Open Issues","text":"<p>Issues are listed at: https://github.com/ludwig-ai/ludwig/issues</p> <p>If you would like to work on any of them, make sure it is not already assigned to someone else.</p> <p>You can self-assign it by commenting on the Issue page with one of the keywords: <code>#take</code> or <code>#self-assign</code>.</p> <p>Work on your self-assigned issue and eventually create a Pull Request.</p>"},{"location":"developer_guide/contributing/#creating-pull-requests","title":"Creating Pull Requests","text":"<ol> <li> <p>Fork the repository by clicking on the \"Fork\" button on    the repository's page. This creates a copy of the code under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> </li> </ol> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/ludwig.git\ncd ludwig\ngit remote add upstream https://github.com/ludwig-ai/ludwig.git\n</code></pre> <ol> <li>Create a new branch to hold your development changes:</li> </ol> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> <p>Do not work on the <code>master</code> branch.</p> <ol> <li>Set up a development environment by running the following command in a virtual environment:</li> </ol> <pre><code>pip install -e .\npip install pre-commit\npre-commit install\n</code></pre> <ol> <li> <p>Develop features on your branch.</p> </li> <li> <p>Format your code by running pre-commits so that your newly added files look nice:</p> </li> </ol> <pre><code>pre-commit run\n</code></pre> <p>Pre-commits also run automatically when committing.</p> <ol> <li>Once you're happy with your changes, make a commit to record your changes locally:</li> </ol> <pre><code>git add .\ngit commit\n</code></pre> <p>It is a good idea to sync your copy of the code with the original repository regularly. This    way you can quickly account for changes:</p> <pre><code>git fetch upstream\ngit rebase upstream/master\n</code></pre> <p>Push the changes to your account using:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <ol> <li>Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send    your contribution to the project maintainers for review.</li> </ol>"},{"location":"developer_guide/contributing/#other-tips","title":"Other tips","text":"<ul> <li>Add unit tests for any new code you write.</li> <li>Make sure tests pass. See the Developer Guide for more details.</li> </ul>"},{"location":"developer_guide/contributing/#attribution","title":"Attribution","text":"<p>This contributing guideline is adapted from <code>huggingface</code>, available at https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md.</p>"},{"location":"developer_guide/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be mindful of and adhere to the Linux Foundation's Code of Conduct when contributing to Ludwig.</p>"},{"location":"developer_guide/release_process/","title":"Release Process","text":""},{"location":"developer_guide/release_process/#standard-release-process","title":"Standard release process","text":""},{"location":"developer_guide/release_process/#1-determine-the-version-name","title":"1. Determine the version name","text":"<p>Note</p> <p>Version names always begin with <code>v</code>.</p> <p>Examples of version names:</p> <pre><code>\"vX.Y\"      # Release major version X (starts at 0), minor version Y (starts at 1).\n\"vX.Y.Z\"    # Release major version X (starts at 0), minor version Y (starts at 1), patch Z (starts at 1).\n\"vX.YrcZ\"   # Release candidate Z, without a period. (starts at 1)\n\"vX.Y.dev\"  # Developer version, with a period.\n</code></pre> <p>Inspiration:</p> <ul> <li>Python pre-releases.</li> <li>PEP0440 pre-releases.</li> </ul>"},{"location":"developer_guide/release_process/#2-update-ludwig-versions-in-code","title":"2. Update Ludwig versions in code","text":"<p>Create a new branch off of a target_release_branch, e.g. <code>master</code> or <code>release-X.Y</code>.</p> <pre><code>git checkout &lt;TARGET_RELEASE_BRANCH&gt;\ngit checkout -b ludwig_release\ngit push --set-upstream origin ludwig_release\n</code></pre> <p>Update the versions referenced in globals and setup. Reference PR.</p> <pre><code>git commit -m \"Update ludwig version to vX.YrcZ.\"\ngit push\n</code></pre> <p>Create a PR with the change requesting a merge from <code>ludwig_release</code> to the target branch.</p> <p>Get approval from a Ludwig maintainer.</p> <p>Merge PR (with squashing).</p>"},{"location":"developer_guide/release_process/#3-tag-the-latest-commit-and-push-the-tag","title":"3. Tag the latest commit, and push the tag","text":"<p>After merging the PR from step 2, the latest commit on the target_release_branch should be the PR that upgrades ludwig versions in code.</p> <p>Pull the change from head.</p> <pre><code>git checkout &lt;TARGET_RELEASE_BRANCH&gt;\ngit pull\n</code></pre> <p>Add a tag to the commit locally:</p> <pre><code>git tag -a vX.YrcZ -m \"Ludwig vX.YrcZ\"\n</code></pre> <p>Push tags to the repo.</p> <pre><code>git push --follow-tags\n</code></pre>"},{"location":"developer_guide/release_process/#4-in-github-go-to-releases-and-draft-a-new-release","title":"4. In Github, go to releases and \"Draft a new release\"","text":"<p>Loom walk-through.</p> <p>Release candidates don't need release notes. Full releases should have detailed release notes. All releases should include a full list of changes (Github supports generating this automatically).</p> <p>Do not upload assets manually. These will be created automatically by Github.</p> <p>For release candidates, check \"pre-release\".</p>"},{"location":"developer_guide/release_process/#5-click-publish","title":"5. Click publish","text":"<p>When the release notes are ready, click <code>Publish release</code> on Github. Ludwig's CI will automatically update PyPI.</p>"},{"location":"developer_guide/release_process/#6-update-ludwig-docs","title":"6. Update Ludwig docs","text":"<p>Check that the Ludwig PyPi has been updated with the newest version.</p> <p>Go to the ludwig-docs repo and update the auto-generated docs there.</p> <pre><code>&gt; cd ludwig-docs\n&gt; git pull\n&gt; git checkout -b update_docs\n&gt; pip install ludwig --upgrade\n&gt; python code_doc_autogen.py\n</code></pre> <p>If there are any changes, commit them.</p> <pre><code>&gt; git commit -m \"Update auto-generated docs.\"\n&gt; git push --set-upstream origin update_docs\n</code></pre> <p>Create a PR.</p>"},{"location":"developer_guide/release_process/#7-for-major-releases-create-an-release-xy-branch","title":"7. For major releases, create an release-X.Y branch","text":"<pre><code>&gt; git checkout master\n&gt; git checkout -b release-X.Y\n&gt; git push --set-upstream origin release-X.Y\n</code></pre> <p>All subsequent minor releases on top of this major version will be based from this branch.</p>"},{"location":"developer_guide/release_process/#8-spread-the-word","title":"8. Spread the word","text":"<p>Announce the release on Slack.</p> <pre><code>Ludwig X.Y.Z Released\nFeatures: Improvements to &lt;CONTENT&gt;. See &lt;LINK&gt;release notes&lt;LINK&gt; for complete details.\nDocs: https://ludwig.ai/latest/\n</code></pre> <p>If it's a major version release, consider other forms of publicization like coordinating sharing the release on other social media, or writing a blog post.</p>"},{"location":"developer_guide/release_process/#cherrypicking-bugfix-commits-from-master-to-stable-release-branches","title":"Cherrypicking bugfix commits from master to stable release branches","text":""},{"location":"developer_guide/release_process/#1-gather-a-list-of-commit-hashes-that-should-be-cherrypicked","title":"1. Gather a list of commit hashes that should be cherrypicked","text":"<p>You can use either the full hashes from <code>git log</code> or partial hashes from the Github PR UI, i.e.</p> <p></p>"},{"location":"developer_guide/release_process/#2-cherry-pick-each-commit-in-a-cherrypick-xy-branch","title":"2. Cherry pick each commit in a cherrypick-X.Y branch","text":"<pre><code>git checkout release-X.Y\ngit checkout -b cherrypick-X.Y\ngit cherry-pick &lt;commit_1&gt;  # One commit at a time.\ngit cherry-pick &lt;commit_2&gt; &lt;commit_3&gt; &lt;commit_4&gt; ...  # Or multiple commits all at once.\n</code></pre> <p>Ensure that all cherry-picks have been correctly applied.</p> <p>Note</p> <p>Empty cherry-picks could mean that that commit already exists.</p>"},{"location":"developer_guide/release_process/#3-create-a-pr-with-the-cherry-pick-changes-merging-into-release-xy","title":"3. Create a PR with the cherry-pick changes, merging into <code>release-X.Y</code>","text":"<p>Push the cherrypick branch.</p> <pre><code>git push --set-upstream origin cherrypick-X.Y\n</code></pre> <p>Create a PR with the change requesting a merge from <code>cherrypick-X.Y</code> to <code>release-X.Y</code>.</p> <p>Get approval from a Ludwig maintainer.</p> <p>Merge PR without squashing.</p> <p>Continue with the standard release process.</p>"},{"location":"developer_guide/release_process/#appendix","title":"Appendix","text":""},{"location":"developer_guide/release_process/#oops-more-prs-were-merged-after-the-version-bump","title":"Oops, more PRs were merged after the version bump","text":"<p>If there were some last minute PRs merged after the version bump, reorder the commits to make the version bump be the last commit that gets tagged before the release.</p> <p>Reordering Commits in Git</p>"},{"location":"developer_guide/release_process/#oops-i-tagged-the-wrong-commit-and-pushed-it-to-github-already","title":"Oops, I tagged the wrong commit, and pushed it to github already","text":"<pre><code>git tag -d &lt;tagname&gt;                  # delete the old tag locally\ngit push origin :refs/tags/&lt;tagname&gt;  # delete the old tag remotely\ngit tag &lt;tagname&gt; &lt;commitId&gt;          # make a new tag locally\ngit push origin &lt;tagname&gt;             # push the new local tag to the remote\n</code></pre>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/","title":"Run Tests on GPU Using Ray","text":"<p>As part of Ludwig's GitHub actions PR checks, all Ludwig tests must pass with and without GPU availability.</p> <p>To debug a specific test on GPU, it may be useful to run Ludwig GPU tests using Ray.</p>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#setup","title":"Setup","text":""},{"location":"developer_guide/run_tests_on_gpu_using_ray/#1-set-up-an-aws-ami-with-a-gpu","title":"1. Set up an AWS AMI with a GPU","text":"<p>Reach out to your AWS account administrator, or set up an account for yourself.</p>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#2-test-if-you-have-the-aws-cli","title":"2. Test if you have the AWS CLI","text":"<pre><code>aws s3 ls\n</code></pre> <p>If not, install it from here.</p>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#3-set-up-aws-keys","title":"3. Set up AWS keys","text":"<ol> <li> <p>AWS Credentials [you will need to set this up for Ray to authenticate you]</p> <p>How to create AWS Access Key ID</p> <p>Once created, download your access key so you can refer to it.</p> </li> <li> <p>Run <code>aws configure</code> to configure your AWS CLI with your access credentials</p> <p>Configuration and credential file settings - AWS Command Line Interface</p> </li> <li> <p>(optional) Get an AWS PEM file</p> <p>Not needed for unit tests on GPU, which never spins up new nodes, but it will be needed if you ever want to enable Ray to launch new nodes.</p> <p>Amazon EC2 key pairs and Linux instances - Amazon Elastic Compute Cloud</p> </li> </ol>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#4-get-ray","title":"4. Get Ray","text":"<p>Install ray locally:</p> <pre><code>pip install -U \"ray[default]\" boto3\n</code></pre>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#5-set-up-a-ray-config","title":"5. Set up a Ray Config","text":"<pre><code>vim $HOME/.clusters/cluster.yaml\n</code></pre> <p>Copy the sample ray config below and edit all the <code>&lt;...&gt;</code> values to match your local dev environment.</p> <pre><code>cluster_name: &lt;$USER&gt;-ludwig-ray-g4dn\n\nmax_workers: 3\n\ndocker:\nimage: \"ludwigai/ludwig-ray-gpu:master\"\ncontainer_name: \"ray_container\"\npull_before_run: True\nrun_options: # Extra options to pass into \"docker run\"\n- --ulimit nofile=65536:65536\n\nprovider:\ntype: aws\nregion: &lt;us-east-2&gt;\navailability_zone: &lt;us-east-2a&gt;\n\navailable_node_types:\nray.head.default:\nresources: {}\nnode_config:\nInstanceType: g4dn.4xlarge\nImageId: latest_dlami\nBlockDeviceMappings:\n- DeviceName: /dev/sda1\nEbs:\nVolumeSize: 100\nray.worker.default:\nmin_workers: 0\nmax_workers: 0\nresources: {}\nnode_config:\nInstanceType: g4dn.4xlarge\nImageId: latest_dlami\n\nhead_node_type: ray.head.default\n\nfile_mounts:\n{\n    /home/ubuntu/ludwig/: &lt;/Users/$USER/ludwig&gt;,  # Ludwig Repo.\n    /home/ray/.aws: &lt;/Users/$USER/.aws&gt;,  # AWS credentials.\n}\n\nrsync_exclude:\n- \"**/.git\"\n- \"**/.git/**\"\n\nrsync_filter:\n- \".gitignore\"\n\nsetup_commands:\n- pip uninstall -y ludwig &amp;&amp; pip install -e /home/ubuntu/ludwig/.\n- pip install s3fs==2021.10.0 aiobotocore==1.4.2 boto3==1.17.106\n- pip install pandas==1.1.4\n- pip install hydra-core --upgrade\n\nhead_start_ray_commands:\n- ray stop --force\n- ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\n\nworker_start_ray_commands:\n- ray stop --force\n- ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n</code></pre> <p>Set an environment variable mapping to the location (can be relative) of your cluster config:</p> <pre><code>export CLUSTER=\"$HOME/.clusters/cluster.yaml\"\n</code></pre>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#developer-workflow","title":"Developer Workflow","text":""},{"location":"developer_guide/run_tests_on_gpu_using_ray/#once-launch-the-ray-cluster","title":"(once) Launch the ray cluster","text":"<p>export CLUSTER=\"$HOME/cluster_g4dn.yaml\" export CLUSTER_CPU=\"$HOME/cluster_cpu.yaml\" ray up $CLUSTER</p>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#make-local-changes","title":"Make local changes","text":"<p>Run tests locally.</p> <pre><code>pytest tests/...\n</code></pre>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#rsync-your-local-changes-to-the-ray-gpu-cluster","title":"Rsync your local changes to the ray GPU cluster","text":"<pre><code>ray rsync_up $CLUSTER -A '/Users/$USER/ludwig/' '/home/ubuntu/ludwig'\nray rsync_up $CLUSTER_CPU -A '/Users/$USER/ludwig/' '/home/ubuntu/ludwig'\n</code></pre> <p>Warning</p> <p>The trailing backslash <code>/</code> is important!</p>"},{"location":"developer_guide/run_tests_on_gpu_using_ray/#run-tests-on-the-gpu-cluster-from-the-ray-mounted-ludwig-directory","title":"Run tests on the GPU cluster from the Ray-mounted ludwig directory","text":"<pre><code>ray exec $CLUSTER \"cd /home/ubuntu/ludwig &amp;&amp; pytest tests/\"\n</code></pre> <p>You can also connect directly to a terminal on the cluster head:</p> <pre><code>ray attach $CLUSTER\n</code></pre>"},{"location":"developer_guide/style_guidelines_and_tests/","title":"Style Guidelines and Tests","text":""},{"location":"developer_guide/style_guidelines_and_tests/#coding-style-guidelines","title":"Coding Style Guidelines","text":"<p>We expect contributions to mimic existing patterns in the codebase and demonstrate good practices: the code should be concise, readable, PEP8-compliant, and limit each line to 120 characters.</p> <p>See codebase structure for guidelines on where new modules should be added.</p>"},{"location":"developer_guide/style_guidelines_and_tests/#pre-commitci","title":"pre-commit.ci","text":"<p>The Ludwig repository integrates with pre-commit.ci, which enforces basic code style guidelines and automatically fixes minor style issues by adding a commit to pull requests. So, check the results of pre-commit.ci after creating a new pull request. There may be automatic fixes to pull, or issues which require manual editing to fix.</p> <p>To run pre-commit on local branches, you can install pre-commit locally with pip:</p> <pre><code># Installs pre-commit tool\npip install pre-commit\n\n# Adds pre-commit hooks to local clone of git repository.\npre-commit install\n\n# Runs pre-commit on all files\npre-commit run --all-files\n\n# To disable, simply uninstall pre-commit from the local clone.\npre-commit uninstall\n</code></pre>"},{"location":"developer_guide/style_guidelines_and_tests/#docstrings","title":"Docstrings","text":"<p>All new files, classes, and methods should have a docstring. Docstrings should give a concise description of the method and describe the meaning for each parameter, as well as any possible return value. Type hints should also be used in the function signature wherever possible, and should use the most specific type accepted by the method.</p> <p>Example:</p> <pre><code>def load_processed_dataset(\n        self,\n        split\n) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]]:\n\"\"\"Loads the processed Parquet into a dataframe.\n\n    :param split: Splits along 'split' column if present.\n    :returns: The preprocessed dataset, or a tuple of (train, validation, test).\n    \"\"\"\n</code></pre> <p>Functions with no arguments or return value may have a isingle-line docstring, ie:</p> <pre><code>@pytest.fixture()\ndef csv_filename():\n\"\"\"Yields a csv filename for holding temporary data.\"\"\"\n</code></pre>"},{"location":"developer_guide/style_guidelines_and_tests/#tests","title":"Tests","text":"<p>Ludwig uses two types of tests: unit tests and integration tests. Unit tests test a single module, and should individually be very fast. Integration tests run an end-to-end test of a single Ludwig functionality, like hyperopt or visualization. Ludwig tests are organized in the following directories:</p> <pre><code>\u251c\u2500\u2500 ludwig                 - Ludwig library source code\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 integration_tests  - End-to-end tests of Ludwig workflows\n    \u2514\u2500\u2500 ludwig             - Unit tests. Subdirectories match ludwig/ structure\n</code></pre> <p>We are using <code>pytest</code> as our testing framework. For more information, see the pytest docs.</p> <p>Note</p> <p>Ludwig's test coverage is a work in progress, and many modules do not have proper test coverage yet. Contributions which get us closer to the goal of 100% test coverage will be welcomed!</p>"},{"location":"developer_guide/style_guidelines_and_tests/#checklist","title":"Checklist","text":"<p>Before running tests, make sure:</p> <ol> <li>Your python environment is properly setup to run Ludwig.</li> <li>All required dependencies for testing are installed: <code>pip install ludwig[test]</code></li> <li>You have write access on the machine. Some tests require saving temporary files to disk.</li> </ol>"},{"location":"developer_guide/style_guidelines_and_tests/#running-tests","title":"Running tests","text":"<p>To run all tests, execute <code>python -m pytest</code> from the ludwig root directory. Note that you don't need to have ludwig module installed. Running tests from the ludwig source root is useful for development as the test will import ludwig modules directly from the source tree.</p> <p>To run all unit tests (will take a few minutes):</p> <pre><code>python -m pytest tests/ludwig/\n</code></pre> <p>Run a single test module:</p> <pre><code>python -m pytest tests/ludwig/decoders/test_sequence_decoder.py\n</code></pre> <p>To run a single test case of a module, you can use <code>-k</code> to specify the test case name:</p> <pre><code>python -m pytest tests/integration_tests/test_experiment.py \\\n-k \"test_visual_question_answering\"\n</code></pre> <p>Another useful tool for debugging is the <code>-vs</code> flag, which runs the test with eager stdout. This prints log messages to the console in real time. Also, individual test cases can be specified with the <code>module::test_case</code> pattern instead of <code>-k</code>:</p> <pre><code>python -m pytest \\\ntests/integration_tests/test_api.py::test_api_training_determinism -vs\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/","title":"Unit Test Design Guidelines","text":""},{"location":"developer_guide/unit_test_design_guidelines/#general-guidelines","title":"General Guidelines","text":""},{"location":"developer_guide/unit_test_design_guidelines/#create-a-unit-test-for-every-module","title":"Create a unit test for every module","text":"<p>Unit tests are in <code>tests/ludwig/</code> which parallels the <code>ludwig/</code> source tree. Every source file in <code>ludwig/</code> with testable functionality should have a corresponding unit test file in <code>test/ludwig/</code>, with a filename corresponding to the source file, prefixed by <code>test_</code>.</p> <p>Examples:</p> Module Test <code>ludwig/data/dataset_synthesizer.py</code> <code>tests/ludwig/data/test_dataset_synthesizer.py</code> <code>ludwig/features/audio_feature.py</code> <code>tests/ludwig/features/test_audio_feature.py</code> <code>ludwig/modules/convolutional_modules.py</code> <code>tests/ludwig/modules/test_convolutional_modules.py</code> <p>Note</p> <p>At the time of writing, not all modules in Ludwig have proper unit tests or match the guidelines here. These guidelines are the goals we aspire to, and we believe incremental improvement is better than demanding perfection. Testing of Ludwig is a work in progress, any changes that get us closer to the goal of 100% test coverage are welcomed!</p>"},{"location":"developer_guide/unit_test_design_guidelines/#what-should-be-tested","title":"What should be tested","text":"<p>Unit tests should generally test every function or method a module exports, with some exceptions. A good rule is that if a function or method fulfills a requirement and will be called from outside the module, it should have a test.</p> <ul> <li>Test the common cases. This will provide a notification when something breaks.</li> <li>Test the edge cases of complex methods if you think they might have errors.</li> <li>Test failure cases If a method must fail, ex. when input out of range, ensure it does.</li> <li>Bugs When you find a bug, write a test case to cover it before fixing it.</li> </ul>"},{"location":"developer_guide/unit_test_design_guidelines/#parameterize-tests","title":"Parameterize tests","text":"<p>Use <code>@pytest.mark.parameterize</code> to test combinations of parameters that drive through all code paths, to ensure correct behavior of the function under a variety of situations.</p> <pre><code># test combinations of parameters to exercise all code paths\n@pytest.mark.parameterize(\n    'num_total_blocks, num_shared_blocks',\n    [(4, 2), (6, 4), (3, 1)]\n)\n@pytest.mark.parameterize('virtual_batch_size', [None, 7])\n@pytest.mark.parameterize('size', [4, 12])\n@pytest.mark.parameterize('input_size', [2, 6])\ndef test_feature_transformer(\n    input_size: int,\n    size: int,\n    virtual_batch_size: Optional[int],\n    num_total_blocks: int,\n    num_shared_blocks: int\n) -&gt; None:\n    feature_transformer = FeatureTransformer(\n        input_size,\n        size,\n        bn_virtual_bs=virtual_batch_size,\n        num_total_blocks=num_total_blocks,\n        num_shared_blocks=num_shared_blocks\n    )\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#test-edge-cases","title":"Test edge cases","text":"<p>Test edge cases when possible. For example, if a method takes multiple inputs: test with an empty input, a single input, and a large number of inputs.</p> <pre><code>@pytest.mark.parametrize(\"virtual_batch_size\", [None, 7, 64])  # Test with no virtual batch norm, odd size, or large.\n@pytest.mark.parametrize(\"input_size\", [1, 8, 256])  # Test with single input feature or many inputs.\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#tensor-type-and-shape","title":"Tensor Type and Shape","text":"<p>At minimum, tests related to tensors should confirm no errors are raised when processing tensor(s) and that resulting tensors are of correct shape and type. This provides minimal assurance that the function is operating as expected.</p> <pre><code># pass input features through combiner\ncombiner_output = combiner(input_features)\n\n# check for required attributes in the generated output\nassert hasattr(combiner, 'input_dtype')\nassert hasattr(combiner, 'output_shape')\n\n# check for correct data type\nassert isinstance(combiner_output, dict)\n\n# required key present\nassert 'combiner_output' in combiner_output\n\n# check for correct output shape\nassert (combiner_output['combiner_output'].shape\n       == (batch_size, *combiner.output_shape))\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#trainable-modules","title":"Trainable Modules","text":"<p>When testing a trainable module (layer, encoder, decoder, combiner or model), make sure that all the variables / weights get updates after one training step. This will ensure that the computation graph does not contain dangling nodes. This catches subtle issues which don\u2019t manifest as crashes, which are not caught by looking at the loss scores or by training the model to convergence (albeit to a usually bad loss), For more details see how to unit test machine learning code.</p> <p>Add type checking based on torch input and outputs. Ensure all module outputs have the expected torch datatype, dimensionality, and tensor shape.</p> <p>For trainable modules, we recommend adding at least one overfitting test. Ensure that a small ECD model containing the module is able to overfit on a small dataset. Ensures that models are able to converge on reasonable targets and catches any unscoped issues that are not captured by shape, type, or weight update tests.</p>"},{"location":"developer_guide/unit_test_design_guidelines/#best-practices","title":"Best practices","text":"<p>There's lots of great advice on the web for writing good tests. Here are a few highlights from Microsoft's recommendations that we ascribe to:</p> <ul> <li>Characteristics of a good unit test</li> <li>Arranging unit tests</li> <li>Write minimally passing tests</li> <li>Avoid logic in tests</li> <li>Avoid multiple acts</li> </ul>"},{"location":"developer_guide/unit_test_design_guidelines/#what-not-to-test","title":"What not to test","text":"<p>It isn't necessary to test every function, a good rule is that if a function or method fulfills a requirement and will be called from outside the module, it should have a test.</p> <p>Things that don't need unit tests:</p> <ul> <li>Constructors or properties. Test them only if they contain validations.</li> <li>Configurations like constants, readonly fields, configs, etc.</li> <li>Facades or wrappers around other frameworks or libraries.</li> <li>Private methods</li> </ul>"},{"location":"developer_guide/unit_test_design_guidelines/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"developer_guide/unit_test_design_guidelines/#use-pytestmarkparameterize","title":"Use pytest.mark.parameterize","text":"<p>Automates setup for test cases. Be careful to only test case parameter values which are meaningfully different, as the total number of tests cases grows combinatorially.</p> <pre><code>@pytest.mark.parameterize('enc_should_embed', [True, False])\n@pytest.mark.parameterize('enc_reduce_output', [None, 'sum'])\n@pytest.mark.parameterize('enc_norm', [None, 'batch', 'layer'])\n@pytest.mark.parameterize('enc_num_layers', [1, 2])\n@pytest.mark.parameterize('enc_dropout', [0, 0.2])\n@pytest.mark.parameterize('enc_cell_type', ['rnn', 'gru', 'lstm'])\n@pytest.mark.parameterize('enc_encoder', ENCODERS + ['passthrough'])\ndef test_sequence_encoders(\n        enc_encoder: str,\n        enc_cell_type: str,\n        enc_dropout: float,\n        enc_num_layers: int,\n        enc_norm: Union[None, str],\n        enc_reduce_output: Union[None, str],\n        enc_should_embed: bool,\n        input_sequence: torch.Tensor\n):\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#use-temp_path-or-tmpdir-for-generated-data","title":"Use temp_path or tmpdir for generated data","text":"<p>Use temporary directories for any generated data. PyTest provides fixtures for temporary directories, which are guaranteed unique for each test run and will be cleaned up automatically. We recommend using <code>tmpdir</code>, which provides a <code>py.path.local</code> object which is compatible with <code>os.path</code> methods. If you are using <code>pathlib</code>, PyTest also provides <code>tmp_path</code>, which is a <code>pathlib.Path</code>.</p> <p>For more details, see Temporary directories and files in the PyTest docs.</p> <p>Example:</p> <pre><code>@pytest.mark.parametrize(\"skip_save_processed_input\", [True, False])\n@pytest.mark.parametrize(\"in_memory\", [True, False])\n@pytest.mark.parametrize(\"image_source\", [\"file\", \"tensor\"])\n@pytest.mark.parametrize(\"num_channels\", [1, 3])\ndef test_basic_image_feature(\n        num_channels, image_source, in_memory, skip_save_processed_input, tmpdir\n):\n    # Image Inputs\n    image_dest_folder = os.path.join(tmpdir, \"generated_images\")\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#consolidate-tests-which-require-setup","title":"Consolidate tests which require setup","text":"<p>For example, multiple tests may rely on the same training/test data sets which take time to load. If multiple tests rely on the same common resources, group these tests into a single module and use appropriately scoped <code>@pytest.fixture</code> to reduce overhead of repeatedly performing setup.</p> <p>Examples of reusable test fixtures can be found in <code>tests/conftest.py</code>. This module contains reusable <code>@pytest.fixtures</code> that have applicability across many tests.</p>"},{"location":"developer_guide/unit_test_design_guidelines/#deterministic-tests","title":"Deterministic tests","text":"<p>Wherever possible, every test run with the same parameters should produce the same result. When using a random number generator, always specify a seed. A test will be difficult to debug if it produces different output on different runs.</p> <pre><code>import torch\n\nRANDOM_SEED = 1919\n\n# setup synthetic tensor, ensure reproducibility by setting seed\ntorch.manual_seed(RANDOM_SEED)\ninput_tensor = torch.randn([BATCH_SIZE, input_size], dtype=torch.float32)\n</code></pre>"},{"location":"developer_guide/unit_test_design_guidelines/#test-for-parameter-updates","title":"Test for parameter updates","text":"<p>The utility function <code>check_module_parameters_updated()</code> in the <code>tests.integration_tests.parameter_utils</code> module is available to test whether Ludwig modules, e.g., encoders, combiners, decoders and related sub-components are updating parameters during the sequence of forward pass-backward pass-optimize step.</p> <p>Guidelines for implementing parameter updating test:</p> <ul> <li>Not required for very simple modules, like the fully-connected layer or for well-known pre-trained modules, like the Huggingface text encoders.</li> <li>Before implementing the parameter update test ensure that the test does not generate run-time exceptions, the generated output conforms to the expected data structure and the shape of the output is correct.</li> </ul> <p><code>check_module_parameters_updated(module, input, target)</code> function requires three positional arguments:</p> <ul> <li><code>module</code> is the Ludwig component to be tested, i.e., encoder, combiner or decoder</li> <li><code>input</code> is tuple that is the input the Ludwig component's forward method</li> <li><code>target</code> is a synthetic tensor representing the target values for computing loss at the end of the forward pass.</li> </ul> <p>The <code>module</code> and <code>input</code> arguments can be the same as those used in the early part of the test to ensure no run-time exceptions and correct output.</p> <p>A two-step process is recommended steps for implementing the test:</p> <p>Step 1:</p> <p>Set the random seed for repeatability.  Partially implement the parameter update test to print the counts of parameters, e.g.,</p> <pre><code>    # check for parameter updating\n    target = torch.randn(output.shape)\n    fpc, tpc, upc, not_updated = check_module_parameters_updated(sequence_rnn_decoder, (combiner_outputs, None), target)\n    print(fpc, tpc, upc, not_updated)\n</code></pre> <p><code>target</code> is a tensor with synthetic data that is used to in computing the loss after the forward pass.  </p> <p><code>fpc</code> is the count of frozen parameters.  <code>tpc</code> is the count of trainable parameters.  <code>upc</code> is the updated parameter count, i.e., the number of parameters that were updated during the cycle of forward pass-backward pass-optimize step. Inspect the values for correctness, such as</p> <ul> <li><code>fpc</code> + <code>tpc</code> should equal the total number of parameters in the module</li> <li><code>fpc</code> should be zero except for pre-trained modules like the Huggingface text encoders.</li> <li><code>upc</code> &lt;= <code>tpc</code></li> </ul> <p>In the case that some parameters are not updated, <code>not_updated</code> is a Python list containing parameter names that were not updated.</p> <p>In the ideal case, <code>upc</code> == <code>tpc</code>, i.e, count of trainable parameters is equal to the count of updated parameters. However, there may be situations where <code>upc</code> &lt; <code>tpc</code>.  This may occur when dropout is used or with batch normalization with a single training example or conditional processing in the <code>forward()</code> method.  The preceding is not an exhaustive list.  Whenever <code>upc</code> &lt; <code>tpc</code>, the developer should confirm that the counts are correct for the situation.</p> Tips and Tricks to Understand Ludwig Module Parameter Structure <p>While working in Step 1, these code fragments may be temporarily used to gain insight into the structure  and parameters in a Ludwig module.</p> <p>The <code>print()</code> will display the layers that make up a Ludwig module.  From the output the developer can confirm the correct structure and configuration values.</p> <pre><code>encoder = create_encoder(\n    Stacked2DCNN, num_channels=image_size[0], height=image_size[1], width=image_size[2], **encoder_kwargs\n)\nprint(encoder)\n\n# output\nStacked2DCNN(\n  (conv_stack_2d): Conv2DStack(\n    (stack): ModuleList(\n      (0): Conv2DLayer(\n        (layers): ModuleList(\n          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n          (1): ReLU()\n          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n      )\n      (1): Conv2DLayer(\n        (layers): ModuleList(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n          (1): ReLU()\n          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n      )\n    )\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc_stack): FCStack(\n    (stack): ModuleList(\n      (0): FCLayer(\n        (layers): ModuleList(\n          (0): Linear(in_features=32, out_features=28, bias=True)\n          (1): ReLU()\n        )\n      )\n    )\n  )\n)\n</code></pre> <p>Use the <code>named_parameters()</code> method to understand the parameters contained in a Ludwig module. This method returns a list of tuples.  The first element in the tuple is the parameter's name.  The second element is the instance of the PyTorch <code>Parameter</code> object.  In the following example there are 6 parameters, all of them are trainable.</p> <pre><code>encoder = create_encoder(\n    Stacked2DCNN, num_channels=image_size[0], height=image_size[1], width=image_size[2], **encoder_kwargs\n)\nfor p in encoder.named_parameters():\n    print(f\"name: {p[0]}, shape: {p[1].shape}, trainable: {p[1].requires_grad}\")\n\n# output\nname: conv_stack_2d.stack.0.layers.0.weight, shape: torch.Size([32, 3, 3, 3]), trainable: True\nname: conv_stack_2d.stack.0.layers.0.bias, shape: torch.Size([32]), trainable: True\nname: conv_stack_2d.stack.1.layers.0.weight, shape: torch.Size([32, 32, 3, 3]), trainable: True\nname: conv_stack_2d.stack.1.layers.0.bias, shape: torch.Size([32]), trainable: True\nname: fc_stack.stack.0.layers.0.weight, shape: torch.Size([28, 32]), trainable: True\nname: fc_stack.stack.0.layers.0.bias, shape: torch.Size([28]), trainable: True\n</code></pre> <p>Step 2:</p> <p>Once all the differences between <code>tpc</code> and <code>upc</code> are accounted for then replace the <code>print()</code> statement with the appropriate set of <code>assert</code> statements.  Here are some examples:</p> <pre><code>    # check for parameter updating\n    target = torch.randn(output.shape)\n    fpc, tpc, upc, not_updated = check_module_parameters_updated(sequence_rnn_decoder, (combiner_outputs, None), target)\n    assert upc == tpc, f\"Failed to update parameters.  Parameters not update: {not_updated}\"\n</code></pre> <pre><code>    target = torch.randn(conv1_stack.output_shape)\n    fpc, tpc, upc, not_updated = check_module_parameters_updated(conv1_stack, (input,), target)\n    if dropout == 0:\n        # all trainable parameters should be updated\n        assert tpc == upc, (\n            f\"All parameter not updated. Parameters not updated: {not_updated}\" f\"\\nModule structure:\\n{conv1_stack}\"\n        )\n    else:\n        # with specified config and random seed, non-zero dropout update parameter count could take different values\n        assert (tpc == upc) or (upc == 1), (\n            f\"All parameter not updated. Parameters not updated: {not_updated}\" f\"\\nModule structure:\\n{conv1_stack}\"\n        )\n</code></pre> <p>Example of a full test with parameter update checking:</p> <pre><code>@pytest.mark.parametrize(\"cell_type\", [\"rnn\", \"gru\"])\n@pytest.mark.parametrize(\"num_layers\", [1, 2])\n@pytest.mark.parametrize(\"batch_size\", [20, 1])\ndef test_sequence_rnn_decoder(cell_type, num_layers, batch_size):\n    hidden_size = 256\n    vocab_size = 50\n    max_sequence_length = 10\n\n    # make repeatable\n    set_random_seed(RANDOM_SEED)\n\n    combiner_outputs = {HIDDEN: torch.rand([batch_size, hidden_size])}\n    sequence_rnn_decoder = SequenceRNNDecoder(\n        hidden_size, vocab_size, max_sequence_length, cell_type, num_layers=num_layers\n    )\n\n    output = sequence_rnn_decoder(combiner_outputs, target=None)\n\n    assert list(output.size()) == [batch_size, max_sequence_length, vocab_size]\n\n    # check for parameter updating\n    target = torch.randn(output.shape)\n    fpc, tpc, upc, not_updated = check_module_parameters_updated(sequence_rnn_decoder, (combiner_outputs, None), target)\n    assert upc == tpc, f\"Failed to update parameters.  Parameters not update: {not_updated}\"\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This section contains several examples of how to build models with Ludwig for a variety of tasks. For each task we show an example dataset and a sample model definition that can be used to train a model from that data.</p> <p>Write-ups in the Tutorials section show both Ludwig's command line interface and Python API.  For these tutorials, there are ready-to-run notebooks that work in Google's Colab Service.  The notebooks provide the user a starting point for learning about Ludwig capabilities.</p> <p>The Example Use Cases section illustrate how Ludwig can be applied to a variety of machine learning tasks, such as, natural language understanding, timeseries forcasting, multi-label classification to name just a few.</p> <p>In addition to the examples here, on the Ludwig medium publication you can find a three part tutorial on Sentiment Analysis with Ludwig:</p> <ul> <li>Part I (Training models from scratch)</li> <li>Part II (Finetuning pretrained models)</li> <li>Part III (Hyperparameter Optimization)</li> </ul>"},{"location":"examples/adult_census_income/","title":"Tabular Data Classification","text":"<p>This is a complete example of training a model for binary classification.</p> <p>These interactive notebooks follow the steps of this example:</p> <ul> <li>Ludwig CLI: </li> <li>Ludwig Python API: </li> </ul>"},{"location":"examples/adult_census_income/#download-the-adult-census-income-dataset","title":"Download The Adult Census Income dataset","text":"<p>Adult Census Income is an extract of 1994 Census data for predicting whether a person's income exceeds $50K per year.  The data set consists of over 49K records with 14 attributes with missing data.</p> <pre><code>ludwig datasets download adult_census_income\n</code></pre> <p>This command will create a dataset <code>adult_census_income.csv</code> in the current directory.</p> <p>The columns in the dataset are</p> column description age numeric variable, age of person workclass categorical variable, Type of empolyment fnlwgt numeric variable, no defintion education categorical variable, education level education-num nmeric variable, no definition marital-status categorical variable, marital status occupation categorical variable, occupation relationship categorical variable, Relationship to household race categorical variable, race sex categorical variable, gender capital-gain numeric variable, no definition capital-loss numeric variable, no definition hours-per-week numeric variable, hours worked per week native-country categorical variable, Country of origin income binary variable, \" &lt;=50K\" or \" &gt;50K\" split numeric variable, indicating data split training(0), test(2)"},{"location":"examples/adult_census_income/#train","title":"Train","text":"<p>The Ludwig configuration file describes the machine learning task.  There is a vast array of options to control the learning process.  This example only covers a small fraction of the options.  Only the options used in this example are described.  Please refer to the Configuration Section for all the details.</p> <p>First, the defaults section defines the global preprocessing options. All numeric features are z-scored normalized, i.e., mean centered and scaled by the standard deviation.  Numeric missing values are filled in with the mean of non-missing values.</p> <p>The <code>input_features</code> section describes each of the predictor variables, i.e., the column name and type of input variable: number or category</p> <p>The 'combiner' section defines how the input features are combined to be passed to the output decoder.  This example uses the <code>concat</code> combiner, which simply concatenates the output of the input feature encoders.  The combined data is passed through a three layer fully connected network of 128 cells in each layer with dropout regularization.</p> <p>Next the <code>output_features</code> are defined.  In this example, there is one response variable called <code>income</code>.  This is a binary feature with two possible values: \" &lt;=50K\" or \" &gt;50K\".  Because thes values are not conventional binary values, i.e., \"True\" and \"False\", a feature specific preprocessing option is specified to indicate which string (\" &gt;50K\") is interpreted as \"True\".  A four layer fully connected decoder of 32 cells in each layer is specified for this output feature.</p> <p>The last section in this configuration file describes options for how the the <code>trainer</code> will operate.  In this example the <code>trainer</code> will process the training data for 10 epochs.  The optimizer type is \"adam\".</p> clipython <pre><code>defaults:\nnumber:\npreprocessing:\nnormalization: zscore\nmissing_value_strategy: fill_with_mean\n\ninput_features:\n- name: age\ntype: number\n- name: workclass\ntype: category\n- name: fnlwgt\ntype: number\n- name: education\ntype: category\n- name: education-num\ntype: number\n- name: marital-status\ntype: category\n- name: occupation\ntype: category\n- name: relationship\ntype: category\n- name: race\ntype: category\n- name: sex\ntype: category\n- name: capital-gain\ntype: number\n- name: capital-loss\ntype: number\n- name: hours-per-week\ntype: number\n- name: native-country\ntype: category\n\ncombiner:\ntype: concat\nnum_fc_layers: 3\noutput_size: 128\ndropout: 0.2\n\noutput_features:\n- name: income\ntype: binary\npreprocessing:\nfallback_true_label: \" &gt;50K\"\ndecoder:\nnum_fc_layers: 4\noutput_size: 32\n\ntrainer:\nepochs: 10\noptimizer:\ntype: adam\n</code></pre> <p>LudwigModel</p> <pre><code># create Ludwig configuration dictionary\n# define model configuration\nconfig = {'combiner': {'dropout': 0.2,\n              'num_fc_layers': 3,\n              'output_size': 128,\n              'type': 'concat'},\n 'input_features': [{'name': 'age', 'type': 'number'},\n                    {'name': 'workclass', 'type': 'category'},\n                    {'name': 'fnlwgt', 'type': 'number'},\n                    {'name': 'education', 'type': 'category'},\n                    {'name': 'education-num', 'type': 'number'},\n                    {'name': 'marital-status', 'type': 'category'},\n                    {'name': 'occupation', 'type': 'category'},\n                    {'name': 'relationship', 'type': 'category'},\n                    {'name': 'race', 'type': 'category'},\n                    {'name': 'sex', 'type': 'category'},\n                    {'name': 'capital-gain', 'type': 'number'},\n                    {'name': 'capital-loss', 'type': 'number'},\n                    {'name': 'hours-per-week', 'type': 'number'},\n                    {'name': 'native-country', 'type': 'category'}],\n 'output_features': [{'name': 'income',\n                      'decoder': {\n                            'num_fc_layers': 4,\n                            'output_size': 32\n                      },\n                      'preprocessing': {'fallback_true_label': ' &gt;50K'},\n                      'loss': {'type': 'binary_weighted_cross_entropy'},\n                      'type': 'binary'}],\n 'defaults': {\n    'number': {\n      'preprocessing': {\n        'missing_value_strategy': 'fill_with_mean',\n        'normalization': 'zscore'\n      }\n    }\n }\n 'trainer': {'epochs': 10, 'optimizer': {'type': 'adam'}}}\n\n# instantiate Ludwig model object\nmodel = LudwigModel(config=config, logging_level=logging.INFO)\n</code></pre> <p>Train the model.</p> clipython <p><code>ludwig train</code> command</p> <pre><code>ludwig train \\\n--dataset adult_census_income.csv \\\n--config config.yaml\n</code></pre> <p>train() method</p> <pre><code># Trains the model. This cell might take a few minutes.\ntrain_stats, preprocessed_data, output_directory = model.train(training_set=train_df,\n                                                               test_set=test_df)\n</code></pre>"},{"location":"examples/adult_census_income/#evaluate","title":"Evaluate","text":"clipython <p><code>ludwig evaluate</code> command</p> <pre><code>ludwig evaluate --model_path results/experiment_run/model \\\n--dataset evaluation_dataset.csv \\\n--output_directory test_results\n</code></pre> <p>evaluate() method</p> <pre><code># Generates predictions and performance statistics for the test set.\ntest_stats, predictions, output_directory = model.evaluate(\n  eval_df,\n  collect_predictions=True,\n  collect_overall_stats=True,\n  skip_save_eval_stats=False,\n  skip_save_predictions=False,\n  output_directory=\"test_results\",\n  return_type=\"dict\"\n)\n</code></pre>"},{"location":"examples/adult_census_income/#visualize-metrics","title":"Visualize Metrics","text":""},{"location":"examples/adult_census_income/#roc-curve","title":"ROC Curve","text":"clipython <p><code>ludwig visualize roc_curves</code> command</p> <pre><code>!ludwig visualize --visualization roc_curves \\\n--ground_truth evaluation_dataset.csv \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--probabilities test_results/predictions.parquet \\\n--output_feature_name income \\\n--output_directory visualizations \\\n--model_names \"Adult Census Income Model\" \\\n--file_format png\n</code></pre> <p><code>visualize.roc_curves()</code> function</p> <pre><code>from ludwig.visualize import roc_curves\n\nroc_curves(\n    [predictions['income']['probabilities']],\n    eval_df['income'],\n    preprocessed_data[-1],\n    'income',\n    '1',\n    model_names=[\"Adult Census Income\"],\n    output_directory='visualization',\n    file_format='png'\n)\n</code></pre> <p></p>"},{"location":"examples/adult_census_income/#binary-threshold-metrics","title":"Binary Threshold Metrics","text":"clipython <p><code>ludwig visualize binary_threshole_vs_metric</code> command</p> <pre><code>ludwig visualize --visualization binary_threshold_vs_metric \\\n--ground_truth evaluation_dataset.csv \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--probabilities test_results/predictions.parquet \\\n--output_feature_name income \\\n--positive_label 1 \\\n--output_directory visualizations \\\n--model_names \"Adult Census Income Model\" \\\n--metrics accuracy precision recall f1\\\n--file_format png\n</code></pre> <p><code>visualize.binary_threshold_vs_metric()</code> function</p> <pre><code>from ludwig.visualize import binary_threshold_vs_metric\n\nbinary_threshold_vs_metric(\n    [predictions[\"income\"][\"probabilities\"]],\n    eval_df[\"income\"],\n    preprocessed_data[-1],\n    \"income\",\n    [\"accuracy\", \"precision\", \"recall\", \"f1\"],\n    1,\n    model_names=[\"Adult Census Income\"],\n    output_directory=\"visualization\",\n    file_format=\"png\",\n)\n</code></pre>"},{"location":"examples/adult_census_income/#accuracy-metric","title":"Accuracy Metric","text":""},{"location":"examples/adult_census_income/#precision-metric","title":"Precision Metric","text":""},{"location":"examples/adult_census_income/#recall-metric","title":"Recall Metric","text":""},{"location":"examples/adult_census_income/#f1-metric","title":"F1 Metric","text":""},{"location":"examples/adult_census_income/#predictions","title":"Predictions","text":"clipython <p><code>ludwig predict</code> command</p> <pre><code>ludwig predict --model_path results/experiment_run/model \\\n--dataset evaluation_dataset.csv \\\n--output_directory predictions\n</code></pre> <p><code>predict()</code> method</p> <pre><code>predictions, prediction_results = model.predict(dataset=eval_df, skip_save_predictions=False, output_directory=\"predictions_results\")\n</code></pre> <p>Sample predictions </p>"},{"location":"examples/forecasting/","title":"Timeseries forecasting","text":"<p>While direct timeseries prediction is a work in progress Ludwig can ingest timeseries input feature data and make number predictions. Below is an example of a model trained to forecast timeseries at five different horizons.</p> timeseries_data y1 y2 y3 y4 y5 15.07 14.89 14.45 ... 16.92 16.67 16.48 17.00 17.02 14.89 14.45 14.30 ... 16.67 16.48 17.00 17.02 16.48 14.45 14.3 14.94 ... 16.48 17.00 17.02 16.48 15.82 <pre><code>ludwig experiment \\\n--dataset timeseries_data.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: timeseries_data\ntype: timeseries\n\noutput_features:\n-\nname: y1\ntype: number\n-\nname: y2\ntype: number\n-\nname: y3\ntype: number\n-\nname: y4\ntype: number\n-\nname: y5\ntype: number\n</code></pre>"},{"location":"examples/fraud/","title":"Fraud Detection","text":"transaction_id card_id customer_id customer_zipcode merchant_id merchant_name merchant_category merchant_zipcode merchant_country transaction_amount authorization_response_code atm_network_xid cvv_2_response_xflg fraud_label 469483 9003 1085 23039 893 Wright Group 7917 91323 GB 1962 C C N 0 926515 9009 1001 32218 1011 Mums Kitchen 5813 10001 US 1643 C D M 1 730021 9064 1174 9165 916 Keller 7582 38332 DE 1184 D B M 0 <pre><code>ludwig experiment \\\n--dataset transactions.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: customer_id\ntype: category\n-\nname: card_id\ntype: category\n-\nname: merchant_id\ntype: category\n-\nname: merchant_category\ntype: category\n-\nname: merchant_zipcode\ntype: category\n-\nname: transaction_amount\ntype: number\n-\nname: authorization_response_code\ntype: category\n-\nname: atm_network_xid\ntype: category\n-\nname: cvv_2_response_xflg\ntype: category\n\ncombiner:\ntype: concat\nnum_fc_layers: 1\noutput_size: 48\n\noutput_features:\n-\nname: fraud_label\ntype: binary\n</code></pre>"},{"location":"examples/fuel_efficiency/","title":"Simple Regression - Fuel Efficiency Prediction","text":"<p>This example replicates the Keras example at https://www.tensorflow.org/tutorials/keras/basic_regression to predict the miles per gallon of a car given its characteristics in the Auto MPG dataset.</p> MPG Cylinders Displacement Horsepower Weight Acceleration ModelYear Origin 18.0 8 307.0 130.0 3504.0 12.0 70 1 15.0 8 350.0 165.0 3693.0 11.5 70 1 18.0 8 318.0 150.0 3436.0 11.0 70 1 16.0 8 304.0 150.0 3433.0 12.0 70 1 <pre><code>ludwig experiment \\\n--dataset auto_mpg.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>training:\nbatch_size: 32\nepochs: 1000\nearly_stop: 50\nlearning_rate: 0.001\noptimizer:\ntype: rmsprop\ninput_features:\n-\nname: Cylinders\ntype: number\n-\nname: Displacement\ntype: number\n-\nname: Horsepower\ntype: number\n-\nname: Weight\ntype: number\n-\nname: Acceleration\ntype: number\n-\nname: ModelYear\ntype: number\n-\nname: Origin\ntype: category\noutput_features:\n-\nname: MPG\ntype: number\noptimizer:\ntype: mean_squared_error\ndecoder:\nnum_fc_layers: 2\noutput_size: 64\n</code></pre>"},{"location":"examples/gbm_fraud/","title":"GBMs in Ludwig","text":"<p>Welcome to this tutorial on training a GBM model for detecting credit card fraud using the creditcard_fraud Ludwig dataset!</p> <p>Open this example in an interactive notebook: </p>"},{"location":"examples/gbm_fraud/#load-data","title":"Load data","text":"<p>First, let's download the dataset from Kaggle.</p> <pre><code>!ludwig datasets download creditcard_fraud\n</code></pre> <p>This command will download the credit card fraud dataset from Kaggle.</p> <p>The creditcard_fraud dataset contains over 284K records with 31 features and a binary label indicating whether a transaction was fraudulent or not.</p> <pre><code>from ludwig.benchmarking.utils import load_from_module\nfrom ludwig.datasets import creditcard_fraud\n\ndf = load_from_module(creditcard_fraud, {'name': 'Class', 'type': 'binary'})\n</code></pre> <p>This will load the dataset into a Pandas DataFrame and add a reproducible train/valid/test split, stratified on the output feature.</p> <pre><code>df.groupby('split').Class.value_counts()\n</code></pre> split Class count 0 0 178180 1 349 1 0 19802 1 34 2 0 86333 1 109"},{"location":"examples/gbm_fraud/#training","title":"Training","text":"<p>Next, let's create a Ludwig configuration to define our machine learning task. In this configuration, we will specify that we want to use a GBM model for training. We will also specify the input and output features and set some hyperparameters for the model. For more details about the available trainer parameters, see the user guide.</p> <pre><code>import yaml\n\nconfig = yaml.safe_load(\n\"\"\"\nmodel_type: gbm\n\ninput_features:\n  - name: Time\n    type: number\n  - name: V1\n    type: number\n  - name: V2\n    type: number\n  - name: V3\n    type: number\n  - name: V4\n    type: number\n  - name: V5\n    type: number\n  - name: V6\n    type: number\n  - name: V7\n    type: number\n  - name: V8\n    type: number\n  - name: V9\n    type: number\n  - name: V10\n    type: number\n  - name: V11\n    type: number\n  - name: V12\n    type: number\n  - name: V13\n    type: number\n  - name: V14\n    type: number\n  - name: V15\n    type: number\n  - name: V16\n    type: number\n  - name: V17\n    type: number\n  - name: V18\n    type: number\n  - name: V19\n    type: number\n  - name: V20\n    type: number\n  - name: V21\n    type: number\n  - name: V22\n    type: number\n  - name: V23\n    type: number\n  - name: V24\n    type: number\n  - name: V25\n    type: number\n  - name: V26\n    type: number\n  - name: V27\n    type: number\n  - name: V28\n    type: number\n  - name: Amount\n    type: number\n\noutput_features:\n  - name: Class\n    type: binary\n\ntrainer:\n  num_boost_round: 300\n  lambda_l1: 0.00011379587942715957\n  lambda_l2: 8.286477350867434\n  bagging_fraction: 0.4868130193152093\n  feature_fraction: 0.462444410839139\n  evaluate_training_set: false\n\"\"\"\n)\n</code></pre> <p>Now that we have our data and configuration set up, we can train our GBM model using the following command:</p> <pre><code>import logging\nfrom ludwig.api import LudwigModel\n\nmodel = LudwigModel(config, logging_level=logging.INFO)\ntrain_stats, preprocessed_data, output_directory = model.train(df)\n</code></pre>"},{"location":"examples/gbm_fraud/#evaluation","title":"Evaluation","text":"<p>Once the training is complete, we can evaluate the performance of our model using the <code>model.evaluate</code> command:</p> <pre><code>train, valid, test, metadata = preprocessed_data\n\nevaluation_statistics, predictions, output_directory = model.evaluate(test, collect_overall_stats=True)\n</code></pre> <p>ROC AUC</p> <pre><code>evaluation_statistics['Class'][\"roc_auc\"]\n</code></pre> <p>0.9429567456245422</p> <p>Accuracy</p> <pre><code>evaluation_statistics['Class'][\"accuracy\"]\n</code></pre> <p>0.9995435476303101</p> <p>Precision, recall and F1</p> <pre><code>evaluation_statistics['Class'][\"overall_stats\"]\n</code></pre> <pre><code>{'token_accuracy': 0.9995435633656935,\n 'avg_precision_macro': 0.9689512098036177,\n 'avg_recall_macro': 0.8917086143188491,\n 'avg_f1_score_macro': 0.9268520044110913,\n 'avg_precision_micro': 0.9995435633656935,\n 'avg_recall_micro': 0.9995435633656935,\n 'avg_f1_score_micro': 0.9995435633656935,\n 'avg_precision_weighted': 0.9995435633656935,\n 'avg_recall_weighted': 0.9995435633656935,\n 'avg_f1_score_weighted': 0.9995230814612599,\n 'kappa_score': 0.8537058585299842}\n</code></pre>"},{"location":"examples/gbm_fraud/#visualization","title":"Visualization","text":"<p>In addition to evaluating the performance of our model with metrics such as accuracy, precision, and recall, it can also be helpful to visualize the results of our model. Ludwig provides several options for visualizing the results of our model, including confusion matrices and ROC curves.</p> <pre><code>from ludwig import visualize\n</code></pre> <p>Confusion matrix</p> <p>We can use the <code>visualize.confusion_matrix</code> function from Ludwig to create a confusion matrix, which shows the number of true positive, true negative, false positive, and false negative predictions made by our model. To do this, we can use the following code, which will display a confusion matrix plot showing the performance of our model.</p> <pre><code>visualize.confusion_matrix(\n    [evaluation_statistics],\n    model.training_set_metadata,\n    'Class',\n    top_n_classes=[2],\n    model_names=[''],\n    normalize=True\n)\n</code></pre> <p> </p> <p>ROC curve</p> <p>We can also create an ROC curve, which plots the true positive rate against the false positive rate at different classification thresholds. To do this, we can use the following code:</p> <pre><code>visualize.roc_curves(\n    [predictions['Class_probabilities']],\n    test.to_df()['Class_mZFLky'],\n    test.to_df(),\n    'Class_mZFLky',\n    '1',\n    model_names=[\"Credit Card Fraud\"],\n    output_directory='visualization',\n    file_format='png'\n)\n</code></pre> <p></p> <p>We hope these visualizations have been helpful in understanding the performance of our GBM model for detecting credit card fraud. For more information on the various visualization options available in Ludwig, please refer to the documentation.</p> <p>Thank you for following along with our tutorial on training a GBM model on the credit card fraud dataset. We hope that you found the tutorial helpful and gained a better understanding of how to use GBM models in your own machine learning projects.</p> <p>If you have any questions or feedback, please feel free to reach out to our community!</p>"},{"location":"examples/hyperopt/","title":"Hyperparameter Optimization","text":"<p>This is a complete example of Ludwig's hyperparameter optimization capability.</p> <p>These interactive notebooks follow the steps of this example:</p> <ul> <li>Ludwig CLI: </li> <li>Ludwig Python API: </li> </ul>"},{"location":"examples/hyperopt/#download-the-adult-census-income-dataset","title":"Download the Adult Census Income dataset","text":"<p>Adult Census Income is an extract of 1994 Census data for predicting whether a person's income exceeds $50K per year.  The data set consists of over 49K records with 14 attributes with missing data.</p> <pre><code>ludwig datasets download adult_census_income\n</code></pre> <p>This command will create a dataset <code>adult_census_income.csv</code> in the current directory.</p> <p>The columns in the dataset are</p> column description age numeric variable, age of person workclass categorical variable, Type of empolyment fnlwgt numeric variable, no defintion education categorical variable, education level education-num nmeric variable, no definition marital-status categorical variable, marital status occupation categorical variable, occupation relationship categorical variable, Relationship to household race categorical variable, race sex categorical variable, gender capital-gain numeric variable, no definition capital-loss numeric variable, no definition hours-per-week numeric variable, hours worked per week native-country categorical variable, Country of origin income binary variable, \" &lt;=50K\" or \" &gt;50K\" split numeric variable, indicating data split training(0), test(2)"},{"location":"examples/hyperopt/#setup-for-hyperparameter-optimization-run","title":"Setup for hyperparameter optimization run","text":"<p>Hyperparameter optimization is defined with the <code>hyperopt</code> section of the Ludwig configuration specification.  </p> clipython <p>ludwig hyperopt</p> <pre><code>preprocessing:\n...\ninput_features:\n...\ncombiner:\n...\noutput_features:\n...\ntrainer:\n...\ndefaults:\n...\n\n# hyperopt specification \nhyperopt:\n# specify parameters for the Ray Tune to executor to run the hyperparameter optimization\nexecutor:\n...\n# specify Ray Tune search algorithm to use\nsearch_alg:\n...\n# hyperparameter search space for the optimization\nparameters:\n...\n# minimize or maximize the metric score\ngoal: ...\n# metric score to optimize\nmetric: ...\n# name of the output feature\noutput_feature: ...\n</code></pre> <pre><code># define model configuration\nconfig = {\n    'combiner': ... ,\n    'input_features': ... ,\n    'output_features': ... ,\n    'preprocessing': ...,\n    'trainer': ... ,\n    'defaults': ... ,\n\n    # hyperopt specification \n    'hyperopt':  {\n        # specify parameters for the Ray Tune to executor to run the hyperparameter optimization\n        'executor': {'type': 'ray', ... },\n        # specify Ray Tune search algorithm to use\n        'search_alg': {... },\n        # hyperparameter search space for the optimization\n        'parameters': {...},\n        # minimize or maximize the metric score\n        'goal': ...,\n        # metric score to optimize\n        'metric': ...,\n        # name of the output feature\n        'output_feature': ...,\n    }\n}\n</code></pre>"},{"location":"examples/hyperopt/#hyperparameter-search-space-specification","title":"Hyperparameter Search Space Specification","text":"<p>For this example, we want to determine the effect of Ludwig's Trainer's <code>learning_rate</code> and <code>num_fc_layers</code> of the <code>income</code> output feature on model's <code>roc_auc</code> metric.  To do this we will use two different hyperparameter optimization approaches: Random Search and Grid Search.</p>"},{"location":"examples/hyperopt/#random-search","title":"Random Search","text":"clipython <pre><code>hyperopt:\nexecutor:\nnum_samples: 16\ngoal: maximize\nmetric: roc_auc\noutput_feature: income\nparameters: income.decoder.num_fc_layers: space: randint\nlower: 2\nupper: 9\ntrainer.learning_rate:\nspace: loguniform\nlower: 0.001\nupper: 0.1\nsearch_alg:\ntype: variant_generator\nrandom_state: 1919\n</code></pre> <pre><code> 'hyperopt': {\n    'executor': {'num_samples': 16, },\n    'goal': 'maximize',\n    'metric': 'roc_auc',\n    'output_feature': 'income',\n    'parameters': {\n        'income.decoder.num_fc_layers': {\n            'space': 'randint',\n            'lower': 2,\n            'upper': 9\n        },\n        'trainer.learning_rate': {\n            'space': 'loguniform',\n            'lower': 0.001,\n            'upper': 0.1}\n        },\n    'search_alg': {'type': 'variant_generator', 'random_state': 1919, }\n},\n</code></pre>"},{"location":"examples/hyperopt/#grid-search","title":"Grid Search","text":"clipython <pre><code>hyperopt:\nexecutor:\nnum_samples: 1\ngoal: maximize\nmetric: roc_auc\noutput_feature: income\nparameters: income.decoder.num_fc_layers: space: grid_search\nvalues: [2, 4, 6, 8]\ntrainer.learning_rate:\nspace: grid_search\nvalues: [0.001, 0.003, 0.007, 0.01]\nsearch_alg:\ntype: variant_generator\nrandom_state: 1919\n</code></pre> <pre><code>'hyperopt': {\n    'executor': {'num_samples': 1,},\n    'goal': 'maximize',\n    'metric': 'roc_auc',\n    'output_feature': 'income',\n    'parameters': {\n        'income.decoder.num_fc_layers': {'space': 'grid_search', 'values': [2, 4, 6, 8]},\n        'trainer.learning_rate': {'space': 'grid_search', 'values': [0.001, 0.003, 0.007, 0.01]}},\n    'search_alg': {'type': 'variant_generator', 'random_state': 1919, }\n},\n</code></pre>"},{"location":"examples/hyperopt/#run-hyperparameter-optimization","title":"Run Hyperparameter Optimization","text":"<p>Here are example commands/function call to run Ludwig's hyperparameter optimization capability.</p> clipython <p><code>ludwig hyperopt</code> command</p> <pre><code>ludwig hyperopt --dataset adult_census_income.csv \\\n--config config.yaml \\\n--output_directory results \\\n--hyperopt_log_verbosity 1\n</code></pre> <p>hyperopt() method</p> <pre><code>hyperopt_results = hyperopt(\n    config, \n    dataset=adult_census_df, \n    output_directory=\"results\", \n    hyperopt_log_verbosity=1\n)\n</code></pre>"},{"location":"examples/hyperopt/#visualize-hyperparameter-optimization-results","title":"Visualize Hyperparameter Optimization Results","text":"clipython <p><code>ludwig visualize hyperopt_report</code> command</p> <p><code>ludwig visualize hyperopt_hiplot</code> command</p> <pre><code># generate visualizations on hyperparameter effects on the metric\nludwig visualize --visualization hyperopt_report \\\n--hyperopt_stats_path results/hyperopt_statistics.json \\\n--output_directory visualizations \\\n--file_format png\n\n# generate hyperopt hiplot parallel coordinate visualization\nludwig visualize --visualization hyperopt_hiplot \\\n--hyperopt_stats_path results/hyperopt_statistics.json \\\n--output_directory visualizations\n</code></pre> <p><code>visualize.hyperopt_report()</code> function</p> <p><code>visualize.hyperopt_hiplot()</code> function</p> <pre><code>hyperopt_report(\"./rs_output/hyperopt_statistics.json\")\n\nhyperopt_hiplot(\"./rs_output/hyperopt_statistics.json\", output_directory=\"visualizations\")\n</code></pre>"},{"location":"examples/hyperopt/#hyperopt_report","title":"hyperopt_report","text":""},{"location":"examples/hyperopt/#hyperopt_hiplot","title":"hyperopt_hiplot","text":""},{"location":"examples/machine_translation/","title":"Machine Translation","text":"english italian Hello! How are you doing? Ciao, come stai? I got promoted today Oggi sono stato promosso! Not doing well today Oggi non mi sento bene <pre><code>ludwig experiment \\\n  --dataset translation.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: english\ntype: text\nencoder: type: rnn\ncell_type: lstm\nreduce_output: null\npreprocessing:\ntokenizer: english_tokenize\n\noutput_features:\n-\nname: italian\ntype: text\ndecoder: type: generator\ncell_type: lstm\nattention: bahdanau\nreduce_input: null\nloss:\ntype: softmax_cross_entropy\npreprocessing:\ntokenizer: italian_tokenize\n\ntraining:\nbatch_size: 96\n</code></pre>"},{"location":"examples/mnist/","title":"Image Classification","text":"<p>This is a complete example of training an image classification model on the MNIST handwritten digit dataset.</p> <p>These interactive notebooks follow the steps of this example:</p> <ul> <li>Ludwig CLI: </li> <li>Ludwig Python API: </li> </ul>"},{"location":"examples/mnist/#download-the-mnist-dataset","title":"Download the MNIST dataset","text":"<p>MNIST is a collection of gray-scale images of handwritten digits. This collection is made up of 60,000 images for training and 10,000 images for testing model performance.  Each image is 28 X 28 pixels in gray-scale.</p> clipython <pre><code>ludwig datasets download mnist\n</code></pre> <p>This command will create a dataset <code>mnist_dataset.csv</code> in the current directory.  In addition, there will be directories <code>training/</code> and <code>testing/</code> containing the images.</p> <p>The columns in the dataset are</p> column description image_path file path string for the image label single digit 0 to 9 indicating what digit is shown in the image split integer value indicating a training example (0) or test example (2) <p><pre><code>from ludwig.datasets import mnist\n\n# Loads the dataset as a pandas.DataFrame\ntrain_df, test_df, _ = mnist.load(split=True)\n</code></pre> This will create two pandas DataFrames.  <code>train_df</code> contains file path information to the 60K training images.  <code>test_df</code> has same information for the 10K test images.</p> column description image_path file path string for the image label single digit 0 to 9 indicating what digit is shown in the image <p>Sample of images with <code>label</code>. </p>"},{"location":"examples/mnist/#train","title":"Train","text":"<p>The Ludwig configuration file describes the machine learning task. This example only uses a small subset of the options provided by Ludwig. Please refer to the Configuration Section for all the details.</p> <p>First it defines the <code>input_features</code>.  For the image feature, the configuration specifies the type of neural network architecture to encode the image.  In this example the encoder is a two layer Stacked Convolutional Neural Network followed by a fully connected layer with dropout regularization.</p> <p>Next the <code>output_features</code> are defined.  In this case, there is only one output feature called <code>label</code>.  This is a categorical feature that indicates the digit the image represents, 0, 1, 2, ..., 9.</p> <p>The last section in this configuration file describes options for how the the <code>trainer</code> will operate.  In this example the <code>trainer</code> will process the training data for 5 epochs.</p> clipython <pre><code># config.yaml\ninput_features:\n- name: image_path\ntype: image\nencoder: type: stacked_cnn\nconv_layers:\n- num_filters: 32\nfilter_size: 3\npool_size: 2\npool_stride: 2\n- num_filters: 64\nfilter_size: 3\npool_size: 2\npool_stride: 2\ndropout: 0.4\nfc_layers:\n- output_size: 128\ndropout: 0.4\n\noutput_features:\n- name: label\ntype: category\n\ntrainer:\nepochs: 5\n</code></pre> <p>LudwigModel</p> <pre><code># create Ludwig configuration dictionary\nconfig = {\n  'input_features': [\n    {\n      'name': 'image_path',\n      'type': 'image',\n      'preprocessing': {'num_processes': 4},\n      'encoder': {\n          'stacked_cnn',\n          'conv_layers': [\n            {'num_filters': 32, 'filter_size': 3, 'pool_size': 2, 'pool_stride': 2},\n            {'num_filters': 64, 'filter_size': 3, 'pool_size': 2, 'pool_stride': 2, 'dropout': 0.4}\n          ],\n         'fc_layers': [{'output_size': 128, 'dropout': 0.4}]\n        }\n    }\n  ],\n  'output_features': [{'name': 'label', 'type': 'category'}],\n  'trainer': {'epochs': 5}\n}\n\n# Constructs Ludwig model from config dictionary\nmodel = LudwigModel(config, logging_level=logging.INFO)\n</code></pre> <p>Train the model.</p> clipython <p><code>ludwig train</code> command</p> <pre><code>ludwig train \\\n--dataset mnist_dataset.csv \\\n--config config.yaml\n</code></pre> <p>train() method</p> <pre><code># Trains the model. This cell might take a few minutes.\ntrain_stats, preprocessed_data, output_directory = model.train(dataset=train_df)\n</code></pre>"},{"location":"examples/mnist/#evaluate","title":"Evaluate","text":"<p>Evaluate the trained model.</p> clipython <p><code>ludwig evaluate</code> command</p> <pre><code>ludwig evaluate --model_path results/experiment_run/model \\\n--dataset mnist_dataset.csv \\\n--split test \\\n--output_directory test_results\n</code></pre> <p>evaluate() method</p> <pre><code># Generates predictions and performance statistics for the test set.\ntest_stats, predictions, output_directory = model.evaluate(\n  test_df,\n  collect_predictions=True,\n  collect_overall_stats=True\n)\n</code></pre>"},{"location":"examples/mnist/#visualize-metrics","title":"Visualize Metrics","text":"<p>Display Confusion Matrix and Class Entropy plots.</p> clipython <p><code>ludwig visualize confusion_matrix</code> command</p> <pre><code>ludwig visualize --visualization confusion_matrix \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--test_statistics test_results/test_statistics.json \\\n--output_directory visualizations \\\n--file_format png\n</code></pre> <p><code>visualize.confusion_matrix()</code> function</p> <pre><code># Visualizes confusion matrix, which gives an overview of classifier performance\n# for each class.\nfrom ludwig.visualize import confusion_matrix\n\nconfusion_matrix(\n  [test_stats],\n  model.training_set_metadata,\n  'label',\n  top_n_classes=[5],\n  model_names=[''],\n  normalize=True,\n)\n</code></pre> <p></p> <p>Display Learning Curves plots.</p> clipython <p><code>ludwig visualize learning_curves</code> command</p> <pre><code>ludwig visualize --visualization learning_curves \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--training_statistics results/experiment_run/training_statistics.json \\\n--file_format png \\\n--output_directory visualizations\n</code></pre> <p><code>visualize.learning_curves()</code> function</p> <pre><code># Visualizes learning curves, which show how performance metrics changed over\n# time during training.\nfrom ludwig.visualize import learning_curves\n\nlearning_curves(train_stats, output_feature_name='label')\n</code></pre> <p></p>"},{"location":"examples/mnist/#predictions","title":"Predictions","text":"<p>Generate predictions from test dataset.</p> clipython <p><code>ludwig predict</code> command</p> <pre><code>ludwig predict --model_path results/experiment_run/model \\\n--dataset mnist_dataset.csv \\\n--split test \\\n--output_directory predictions\n</code></pre> <p><code>predict()</code> method</p> <pre><code>predictions, output_directory = model.predict(test_df)\n</code></pre> <p>Sample test images displaying true(\"label\") and predicted(\"pred\") labels. </p>"},{"location":"examples/movie_ratings/","title":"Movie rating prediction","text":"year duration nominations categories rating 1921 3240 0 comedy drama 8.4 1925 5700 1 adventure comedy 8.3 1927 9180 4 drama comedy scifi 8.4 <pre><code>ludwig experiment \\\n--dataset movie_ratings.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: year\ntype: number\n-\nname: duration\ntype: number\n-\nname: nominations\ntype: number\n-\nname: categories\ntype: set\n\noutput_features:\n-\nname: rating\ntype: number\n</code></pre>"},{"location":"examples/multi_label/","title":"Multi-label classification","text":"image_path tags images/image_000001.jpg car man images/image_000002.jpg happy dog tie images/image_000003.jpg boat water <pre><code>ludwig experiment \\\n--dataset image_data.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: image_path\ntype: image\nencoder: type: stacked_cnn\n\noutput_features:\n-\nname: tags\ntype: set\n</code></pre>"},{"location":"examples/multi_task/","title":"Multi-Task Learning","text":"<p>This example is inspired by the classic paper Natural Language Processing (Almost) from Scratch by Collobert et al..</p> sentence chunks part_of_speech named_entities San Francisco is very foggy B-NP I-NP B-VP B-ADJP I-ADJP NNP NNP VBZ RB JJ B-Loc I-Loc O O O My dog likes eating sausage B-NP I-NP B-VP B-VP B-NP PRP NN VBZ VBG NN O O O O O Brutus Killed Julius Caesar B-NP B-VP B-NP I-NP NNP VBD NNP NNP B-Per O B-Per I-Per <pre><code>ludwig experiment \\\n--dataset nl_data.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: sentence\ntype: sequence\nencoder: type: rnn\ncell: lstm\nbidirectional: true\nreduce_output: null\n\noutput_features:\n-\nname: chunks\ntype: sequence\ndecoder: type: tagger\n-\nname: part_of_speech\ntype: sequence\ndecoder: type: tagger\n-\nname: named_entities\ntype: sequence\ndecoder: type: tagger\n</code></pre>"},{"location":"examples/multimodal_classification/","title":"Multimodal Classification","text":"<p>This example shows how to build a multimodal classifier with Ludwig.</p> <p>If you'd like to run this example interactively in Colab, open one of these notebooks and try it out:</p> <ul> <li>Ludwig CLI: </li> <li>Ludwig Python API: </li> </ul> <p>Note: you will need your Kaggle API token</p> <p>We'll be using the twitter human-bots dataset, originally uploaded to Kaggle by David Mart\u00edn Guti\u00e9rrez. The dataset is composed of 37438 rows each corresponding to a Twitter user account. Each row contains 20 feature columns collected via the Twitter API. These features contain multiple data modalities, including the account description and the profile image.</p> <p>The target column account_type has two unique values: bot or human. 25013 user accounts were annotated as human accounts, the remaining 12425 are bots.</p> <p>This dataset contains 20 columns, but we'll only use these 16 (15 input + 1 target):</p> column type description default_profile binary Does the account have a default profile default_profile_image binary Does the account have a default profile image description text User account description favorites_count number Total number of favorited tweets followers_count number Total number of followers friends_count number Total number of friends geo_enabled binary Does the account has the geographic location enabled lang category Language of the account location category Location of the account profile_background_image_path image Profile background image path profile_image_path image Profile image path statuses_count number Total number of tweets verified binary Has the account been verified average_tweets_per_day number Average tweets posted per day account_age_days number Account age measured in days account_type binary \"human\" or \"bot\", true if the account is a bot"},{"location":"examples/multimodal_classification/#kaggle-api-token-kagglejson","title":"Kaggle API Token (kaggle.json)","text":"<p>To download datasets using the Kaggle CLI, you'll need a Kaggle API Token.</p> <p>If you already have one, it should be installed at <code>~/.kaggle/kaggle.json</code>. Run this command in a shell, and copy the output:</p> <pre><code>cat ~/.kaggle/kaggle.json\n</code></pre> <p>If you don't have a <code>kaggle.json</code> file:</p> <ol> <li>Sign in to Kaggle. If you don't already have an account, create one.</li> <li>Go to \"Account\", and click the \"Create New API Token\" button. This should start the download. </li> <li>Following the Kaggle instructions, copy your <code>kaggle.json</code> from its download location to a directory called <code>.kaggle</code> in your home directory.</li> <li>If you want to run this example in either of the example Colab notebooks, open kaggle.json and copy its contents to the clipboard. The kaggle.json file should look similar to:</li> </ol> <pre><code>{\"username\":\"your_user_name\",\"key\":\"_______________________________\"}\n</code></pre>"},{"location":"examples/multimodal_classification/#download-dataset","title":"Download Dataset","text":"<p>Downloads the dataset and creates <code>twitter_human_bots_dataset.csv</code> in the current directory.</p> <pre><code># Downloads the dataset to the current working directory\nkaggle datasets download danieltreiman/twitter-human-bots-dataset\n\n# Unzips the downloaded dataset, creates twitter_human_bots_dataset.csv\nunzip -q -o twitter-human-bots-dataset.zip\n</code></pre>"},{"location":"examples/multimodal_classification/#train","title":"Train","text":""},{"location":"examples/multimodal_classification/#define-ludwig-config","title":"Define ludwig config","text":"<p>The Ludwig config declares the machine learning task: which columns to use, their datatypes, and which columns to predict.</p> <p>Note</p> <p>There are only 20 unique background images, so we've declared <code>profile_background_image_path</code> as a category instead of an image. Image encoders need a large number of unique images to perform well and will quickly overfit given such a small sample.</p> clipython <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n- name: default_profile\ntype: binary\n- name: default_profile_image\ntype: binary\n- name: description\ntype: text\n- name: favourites_count\ntype: number\n- name: followers_count\ntype: number\n- name: friends_count\ntype: number\n- name: geo_enabled\ntype: binary\n- name: lang\ntype: category\n- name: location\ntype: category\n- name: profile_background_image_path\ntype: category\n- name: profile_image_path\ntype: image\n- name: statuses_count\ntype: number\n- name: verified\ntype: binary\n- name: average_tweets_per_day\ntype: number\n- name: account_age_days\ntype: number\noutput_features:\n- name: account_type\ntype: binary\n</code></pre> <p>With config defined in a python dict:</p> <pre><code>config = {\n  \"input_features\": [\n    {\n      \"name\": \"default_profile\",\n      \"type\": \"binary\",\n    },\n    {\n      \"name\": \"default_profile_image\",\n      \"type\": \"binary\",\n    },\n    {\n      \"name\": \"description\",\n      \"type\": \"text\",\n    },\n    {\n      \"name\": \"favourites_count\",\n      \"type\": \"number\",\n    },\n    {\n      \"name\": \"followers_count\",\n      \"type\": \"number\",\n    },\n    {\n      \"name\": \"friends_count\",\n      \"type\": \"number\",\n    },\n    {\n      \"name\": \"geo_enabled\",\n      \"type\": \"binary\",\n    },\n    {\n      \"name\": \"lang\",\n      \"type\": \"category\",\n    },\n    {\n      \"name\": \"location\",\n      \"type\": \"category\",\n    },\n    {\n      \"name\": \"profile_background_image_path\",\n      \"type\": \"category\",\n    },\n    {\n      \"name\": \"profile_image_path\",\n      \"type\": \"image\",\n    },\n    {\n      \"name\": \"statuses_count\",\n      \"type\": \"number\",\n    },\n    {\n      \"name\": \"verified\",\n      \"type\": \"binary\",\n    },\n    {\n      \"name\": \"average_tweets_per_day\",\n      \"type\": \"number\",\n    },\n    {\n      \"name\": \"account_age_days\",\n      \"type\": \"number\",\n    },\n  ],\n  \"output_features\": [\n    {\n      \"name\": \"account_type\",\n      \"type\": \"binary\",\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/multimodal_classification/#create-and-train-a-model","title":"Create and train a model","text":"clipython <pre><code>ludwig train --dataset twitter_human_bots_dataset.csv -c config.yaml\n</code></pre> <pre><code>import pandas as pd\n\n# Reads the dataset from CSV file.\ndataset_df = pd.read_csv(\"twitter_human_bots_dataset.csv\")\n\n# Constructs Ludwig model from config dictionary\nmodel = LudwigModel(config, logging_level=logging.INFO)\n\n# Trains the model. This cell might take a few minutes.\ntrain_stats, preprocessed_data, output_directory = model.train(dataset=dataset_df)\n</code></pre>"},{"location":"examples/multimodal_classification/#evaluate","title":"Evaluate","text":"<p>Generates predictions and performance statistics for the test set.</p> clipython <pre><code>ludwig evaluate \\\n--model_path results/experiment_run/model \\\n--dataset twitter_human_bots_dataset.csv \\\n--split test \\\n--output_directory test_results\n</code></pre> <pre><code># Generates predictions and performance statistics for the test set.\ntest_stats, predictions, output_directory = model.evaluate(\n  dataset_df[dataset_df.split == 1],\n  collect_predictions=True,\n  collect_overall_stats=True\n)\n</code></pre>"},{"location":"examples/multimodal_classification/#visualize-metrics","title":"Visualize Metrics","text":"<p>Visualizes confusion matrix, which gives an overview of classifier performance for each class.</p> clipython <pre><code>ludwig visualize \\\n--visualization confusion_matrix \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--test_statistics test_results/test_statistics.json \\\n--output_directory visualizations \\\n--file_format png\n</code></pre> <pre><code>from ludwig.visualize import confusion_matrix\n\nconfusion_matrix(\n  [test_stats],\n  model.training_set_metadata,\n  'account_type',\n  top_n_classes=[2],\n  model_names=[''],\n  normalize=True,\n)\n</code></pre> Confusion Matrix Class Entropy <p>Visualizes learning curves, which show how performance metrics changed over time during training.</p> clipython <pre><code>ludwig visualize \\\n--visualization learning_curves \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--training_statistics results/experiment_run/training_statistics.json \\\n--file_format png \\\n--output_directory visualizations\n</code></pre> <pre><code># Visualizes learning curves, which show how performance metrics changed over time during training.\nfrom ludwig.visualize import learning_curves\n\nlearning_curves(train_stats, output_feature_name='account_type')\n</code></pre> Losses Metrics"},{"location":"examples/ner_tagging/","title":"Named Entity Recognition Tagging","text":"utterance tag Blade Runner is a 1982 neo-noir science fiction film directed by Ridley Scott Movie Movie O O Date O O O O O O Person Person Harrison Ford and Rutger Hauer starred in it Person Person O Person person O O O Philip Dick 's novel Do Androids Dream of Electric Sheep ? was published in 1968 Person Person O O Book Book Book Book Book Book Book O O O Date <pre><code>ludwig experiment \\\n  --dataset sequence_tags.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: utterance\ntype: text\nencoder: type: rnn\ncell_type: lstm\nreduce_output: null\npreprocessing:\ntokenizer: space\n\noutput_features:\n-\nname: tag\ntype: sequence\ndecoder: type: tagger\n</code></pre>"},{"location":"examples/nlu/","title":"Natural Language Understanding","text":"utterance intent slots I want a pizza order_food O O O B-Food_type Book a flight to Boston book_flight O O O O B-City Book a flight at 7pm to London book_flight O O O O B-Departure_time O B-City <pre><code>ludwig experiment \\\n  --dataset nlu.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: utterance\ntype: text\nencoder: type: rnn\ncell_type: lstm\nbidirectional: true\nnum_layers: 2\nreduce_output: null\npreprocessing:\ntokenizer: space\n\noutput_features:\n-\nname: intent\ntype: category\nreduce_input: sum\ndecoder:\nnum_fc_layers: 1\noutput_size: 64\n-\nname: slots\ntype: sequence\ndecoder: type: tagger\n</code></pre>"},{"location":"examples/oneshot/","title":"One-shot Learning with Siamese Networks","text":"<p>This example can be considered a simple baseline for one-shot learning on the Omniglot dataset. The task is, given two images of two handwritten characters, recognize if they are two instances of the same character or not.</p> image_path_1 image_path_2 similarity balinese/character01/0108_13.png balinese/character01/0108_18.png 1 balinese/character01/0108_13.png balinese/character08/0115_12.png 0 balinese/character01/0108_04.png balinese/character01/0108_08.png 1 balinese/character01/0108_11.png balinese/character05/0112_02.png 0 <pre><code>ludwig experiment \\\n--dataset balinese_characters.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: image_path_1\ntype: image\nencoder: type: stacked_cnn\npreprocessing:\nwidth: 28\nheight: 28\nresize_image: true\n-\nname: image_path_2\ntype: image\nencoder: type: stacked_cnn\npreprocessing:\nwidth: 28\nheight: 28\nresize_image: true\ntied: image_path_1\n\ncombiner:\ntype: concat\nnum_fc_layers: 2\noutput_size: 256\n\noutput_features:\n-\nname: similarity\ntype: binary\n</code></pre>"},{"location":"examples/sentiment_analysis/","title":"Sentiment Analysis","text":"review sentiment The movie was fantastic! positive Great acting and cinematography positive The acting was terrible! negative <pre><code>ludwig experiment \\\n  --dataset sentiment.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: review\ntype: text\nencoder: type: parallel_cnn\n\noutput_features:\n-\nname: sentiment\ntype: category\n</code></pre>"},{"location":"examples/seq2seq/","title":"Chit-Chat Dialogue Modeling through Sequence2Sequence","text":"user1 user2 Hello! How are you doing? Doing well, thanks! I got promoted today Congratulations! Not doing well today I\u2019m sorry, can I do something to help you? <pre><code>ludwig experiment \\\n  --dataset chitchat.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: user1\ntype: text\nencoder: type: rnn\ncell_type: lstm\nreduce_output: null\n\noutput_features:\n-\nname: user2\ntype: text\ndecoder: type: generator\ncell_type: lstm\nattention: bahdanau\nloss:\ntype: softmax_cross_entropy\n\ntraining:\nbatch_size: 96\n</code></pre>"},{"location":"examples/speaker_verification/","title":"Speaker Verification","text":"<p>This example describes how to use Ludwig for a simple speaker verification task. We assume to have the following data with label 0 corresponding to an audio file of an unauthorized voice and label 1 corresponding to an audio file of an authorized voice. The sample data looks as follows:</p> audio_path label audiodata/audio_000001.wav 0 audiodata/audio_000002.wav 0 audiodata/audio_000003.wav 1 audiodata/audio_000004.wav 1 <pre><code>ludwig experiment \\\n--dataset speaker_verification.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: audio_path\ntype: audio\npreprocessing:\naudio_file_length_limit_in_s: 7.0\naudio_feature:\ntype: stft\nwindow_length_in_s: 0.04\nwindow_shift_in_s: 0.02\nencoder: type: cnnrnn\n\noutput_features:\n-\nname: label\ntype: binary\n</code></pre>"},{"location":"examples/speech_recognition/","title":"Spoken Digit Speech Recognition","text":"<p>This is a complete example of training an spoken digit speech recognition model on the \"MNIST dataset of speech recognition\".</p>"},{"location":"examples/speech_recognition/#download-the-free-spoken-digit-dataset","title":"Download the free spoken digit dataset","text":"<pre><code>git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\nmkdir speech_recog_digit_data\ncp -r free-spoken-digit-dataset/recordings speech_recog_digit_data\ncd speech_recog_digit_data\n</code></pre>"},{"location":"examples/speech_recognition/#create-an-csv-dataset","title":"Create an CSV dataset","text":"<pre><code>echo \"audio_path\",\"label\" &gt;&gt; \"spoken_digit.csv\"\ncd \"recordings\"\nls | while read -r file_name; do\n   audio_path=$(readlink -m \"${file_name}\")\n   label=$(echo ${file_name} | cut -c1)\n   echo \"${audio_path},${label}\" &gt;&gt; \"../spoken_digit.csv\"\ndone\ncd \"../\"\n</code></pre> <p>Now you should have <code>spoken_digit.csv</code> containing 2000 examples having the following format</p> audio_path label .../speech_recog_digit_data/recordings/0_jackson_0.wav 0 .../speech_recog_digit_data/recordings/0_jackson_10.wav 0 .../speech_recog_digit_data/recordings/0_jackson_11.wav 0 ... ... .../speech_recog_digit_data/recordings/1_jackson_0.wav 1"},{"location":"examples/speech_recognition/#train-a-model","title":"Train a model","text":"<p>From the directory where you have virtual environment with ludwig installed:</p> <pre><code>ludwig experiment \\\n  --dataset &lt;PATH_TO_SPOKEN_DIGIT_CSV&gt; \\\n  --config config_file.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: audio_path\ntype: audio\nencoder: type: stacked_cnn\nreduce_output: concat\nconv_layers:\n-\nnum_filters: 16\nfilter_size: 6\npool_size: 4\npool_stride: 4\ndropout: 0.4\n-\nnum_filters: 32\nfilter_size: 3\npool_size: 2\npool_stride: 2\ndropout: 0.4\nfc_layers:\n-\noutput_size: 64\ndropout: 0.4\npreprocessing:\naudio_feature:\ntype: fbank\nwindow_length_in_s: 0.025\nwindow_shift_in_s: 0.01\nnum_filter_bands: 80\naudio_file_length_limit_in_s: 1.0\nnorm: per_file\n\noutput_features:\n-\nname: label\ntype: category\n\ntraining:\nearly_stop: 10\n</code></pre>"},{"location":"examples/text_classification/","title":"Text Classification","text":"<p>This example shows how to build a text classifier with Ludwig.</p> <p>These interactive notebooks follow the steps of this example:</p> <ul> <li>Ludwig CLI: </li> <li>Ludwig Python API: </li> </ul> <p>We'll be using AG's news topic classification dataset, a common benchmark dataset for text classification. This dataset is a subset of the full AG news dataset, constructed by choosing the four largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 with 7,600 total testing samples. The original split does not include a validation set, so we've labeled the first 5% of each training set class as the validation set.</p> <p>This dataset contains four columns:</p> column description class_index An integer from 1 to 4: \"world\", \"sports\", \"business\", \"sci_tech\" respectively class A string, one of \"world\", \"sports\", \"business\", \"sci_tech\" title Title of the news article description Description of the news article <p>Ludwig also provides several other text classification benchmark datasets which can be used, including:</p> <ul> <li>Amazon Reviews</li> <li>BBC News</li> <li>IMDB</li> <li>Yelp Reviews</li> </ul>"},{"location":"examples/text_classification/#download-dataset","title":"Download Dataset","text":"clipython <p>Downloads the dataset and write to <code>agnews.csv</code> in the current directory.</p> <pre><code>ludwig datasets download agnews\n</code></pre> <p>Downloads the AG news dataset into a pandas dataframe.</p> <pre><code>from ludwig.datasets import agnews\n\n# Loads the dataset as a pandas.DataFrame\ntrain_df, test_df, _ = agnews.load()\n</code></pre> <p>The dataset contains the above four columns plus an additional <code>split</code> column which is one of 0: train, 1: test, 2: validation.</p> <p>Sample (description text omitted for space):</p> <pre><code>class_index,title,description,split,class\n3,Carlyle Looks Toward Commercial Aerospace (Reuters),...,0,business\n3,Oil and Economy Cloud Stocks' Outlook (Reuters),...,0,business\n3,Iraq Halts Oil Exports from Main Southern Pipeline (Reuters),...,0,business\n</code></pre>"},{"location":"examples/text_classification/#train","title":"Train","text":""},{"location":"examples/text_classification/#define-ludwig-config","title":"Define ludwig config","text":"<p>The Ludwig config declares the machine learning task. It tells Ludwig what to predict, what columns to use as input, and optionally specifies the model type and hyperparameters.</p> <p>Here, for simplicity, we'll try to predict class from title.</p> clipython <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: title\ntype: text\nencoder: type: parallel_cnn\noutput_features:\n-\nname: class\ntype: category\ntrainer:\nepochs: 3\n</code></pre> <p>With config defined in a python dict:</p> <pre><code>config = {\n  \"input_features\": [\n    {\n      \"name\": \"title\",            # The name of the input column\n      \"type\": \"text\",             # Data type of the input column\n      \"encoder\": {\n            \"type\": \"parallel_cnn\"\n       }                          # The model architecture we should use for encoding this column\n    }\n  ],\n  \"output_features\": [\n    {\n      \"name\": \"class\",\n      \"type\": \"category\",\n    }\n  ],\n  \"trainer\": {\n    \"epochs\": 3,  # We'll train for three epochs. Training longer might give\n                  # better performance.\n  }\n}\n</code></pre>"},{"location":"examples/text_classification/#create-and-train-a-model","title":"Create and train a model","text":"clipython <pre><code>ludwig train --dataset agnews.csv -c config.yaml\n</code></pre> <pre><code># Constructs Ludwig model from config dictionary\nmodel = LudwigModel(config, logging_level=logging.INFO)\n\n# Trains the model. This cell might take a few minutes.\ntrain_stats, preprocessed_data, output_directory = model.train(dataset=train_df)\n</code></pre>"},{"location":"examples/text_classification/#evaluate","title":"Evaluate","text":"<p>Generates predictions and performance statistics for the test set.</p> clipython <pre><code>ludwig evaluate \\\n--model_path results/experiment_run/model \\\n--dataset agnews.csv \\\n--split test \\\n--output_directory test_results\n</code></pre> <pre><code># Generates predictions and performance statistics for the test set.\ntest_stats, predictions, output_directory = model.evaluate(\n  test_df,\n  collect_predictions=True,\n  collect_overall_stats=True\n)\n</code></pre>"},{"location":"examples/text_classification/#visualize-metrics","title":"Visualize Metrics","text":"<p>Visualizes confusion matrix, which gives an overview of classifier performance for each class.</p> clipython <pre><code>ludwig visualize \\\n--visualization confusion_matrix \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--test_statistics test_results/test_statistics.json \\\n--output_directory visualizations \\\n--file_format png\n</code></pre> <pre><code>from ludwig.visualize import confusion_matrix\n\nconfusion_matrix(\n  [test_stats],\n  model.training_set_metadata,\n  'class',\n  top_n_classes=[5],\n  model_names=[''],\n  normalize=True,\n)\n</code></pre> Confusion Matrix Class Entropy <p>Visualizes learning curves, which show how performance metrics changed over time during training.</p> clipython <pre><code>ludwig visualize \\\n--visualization learning_curves \\\n--ground_truth_metadata results/experiment_run/model/training_set_metadata.json \\\n--training_statistics results/experiment_run/training_statistics.json \\\n--file_format png \\\n--output_directory visualizations\n</code></pre> <pre><code># Visualizes learning curves, which show how performance metrics changed over\n# time during training.\nfrom ludwig.visualize import learning_curves\n\nlearning_curves(train_stats, output_feature_name='class')\n</code></pre> Losses Metrics"},{"location":"examples/text_classification/#make-predictions-on-new-data","title":"Make Predictions on New Data","text":"<p>Lastly we'll show how to generate predictions for new data.</p> <p>The following are some recent news headlines. Feel free to edit or add your own strings to text_to_predict to see how the newly trained model classifies them.</p> clipython <p>With <code>text_to_predict.csv</code>:</p> <pre><code>title\nGoogle may spur cloud cybersecurity M&amp;A with $5.4B Mandiant buy\nEurope struggles to meet mounting needs of Ukraine's fleeing millions\nHow the pandemic housing market spurred buyer's remorse across America\n</code></pre> <pre><code>ludwig predict \\\n--model_path results/experiment_run/model \\\n--dataset text_to_predict.csv \\\n--output_directory predictions\n</code></pre> <pre><code>text_to_predict = pd.DataFrame({\n  \"title\": [\n    \"Google may spur cloud cybersecurity M&amp;A with $5.4B Mandiant buy\",\n    \"Europe struggles to meet mounting needs of Ukraine's fleeing millions\",\n    \"How the pandemic housing market spurred buyer's remorse across America\",\n  ]\n})\n\npredictions, output_directory = model.predict(text_to_predict)\n</code></pre> <p>This command will write predictions to <code>output_directory</code>. Predictions outputs are written in multiple formats including csv and parquet. For instance, <code>predictions/predictions.parquet</code> contains the predicted classes for each example as well as the psuedo-probabilities for each class:</p> class_predictions class_probabilities class_probability class_probabilities_&lt;UNK&gt; class_probabilities_sci_tech class_probabilities_sports class_probabilities_world class_probabilities_business sci_tech [1.9864278277825775e-10, ... 0.954650 1.986428e-10 0.954650 0.000033 0.002563 0.042754 world [8.458710176739714e-09, ... 0.995293 8.458710e-09 0.002305 0.000379 0.995293 0.002022 business [3.710099008458201e-06, ... 0.490741 3.710099e-06 0.447916 0.000815 0.060523 0.490741"},{"location":"examples/titanic/","title":"Binary Classification (Titanic)","text":"<p>This example describes how to use Ludwig to train a model for the kaggle competition, on predicting a passenger's probability of surviving the Titanic disaster. Here's a sample of the data:</p> Pclass Sex Age SibSp Parch Fare Survived Embarked 3 male 22 1 0 7.2500 0 S 1 female 38 1 0 71.2833 1 C 3 female 26 0 0 7.9250 0 S 3 male 35 0 0 8.0500 0 S <p>The full data and the column descriptions can be found here.</p> <p>After downloading the data, to train a model on this dataset using Ludwig,</p> <pre><code>ludwig experiment \\\n  --dataset &lt;PATH_TO_TITANIC_CSV&gt; \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: Pclass\ntype: category\n-\nname: Sex\ntype: category\n-\nname: Age\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: SibSp\ntype: number\n-\nname: Parch\ntype: number\n-\nname: Fare\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: Embarked\ntype: category\n\noutput_features:\n-\nname: Survived\ntype: binary\n</code></pre> <p>Better results can be obtained with morerefined feature transformations and preprocessing, but this example has the only aim to show how this type do tasks and data can be used in Ludwig.</p>"},{"location":"examples/visual_qa/","title":"Visual Question Answering","text":"image_path question answer imdata/image_000001.jpg Is there snow on the mountains? yes imdata/image_000002.jpg What color are the wheels blue imdata/image_000003.jpg What kind of utensil is in the glass bowl knife <pre><code>ludwig experiment \\\n--dataset vqa.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: image_path\ntype: image\nencoder: type: stacked_cnn\n-\nname: question\ntype: text\nencoder: type: parallel_cnn\n\noutput_features:\n-\nname: answer\ntype: text\ndecoder: type: generator\ncell_type: lstm\nloss:\ntype: softmax_cross_entropy\n</code></pre>"},{"location":"examples/weather/","title":"Timeseries forecasting (Weather)","text":"<p>This example illustrates univariate timeseries forecasting using historical temperature data for Los Angeles.</p> <p>Dowload and unpack historical hourly weather data available on Kaggle https://www.kaggle.com/selfishgene/historical-hourly-weather-data</p> <p>Run the following python script to prepare the training dataset:</p> <pre><code>import pandas as pd\nfrom ludwig.utils.data_utils import add_sequence_feature_column\n\ndf = pd.read_csv(\n    '&lt;PATH_TO_FILE&gt;/temperature.csv',\n    usecols=['Los Angeles']\n).rename(\n    columns={\"Los Angeles\": \"temperature\"}\n).fillna(method='backfill').fillna(method='ffill')\n\n# normalize\ndf.temperature = ((df.temperature-df.temperature.mean()) /\n                  df.temperature.std())\n\ntrain_size = int(0.6 * len(df))\nvali_size = int(0.2 * len(df))\n\n# train, validation, test split\ndf['split'] = 0\ndf.loc[\n    (\n        (df.index.values &gt;= train_size) &amp;\n        (df.index.values &lt; train_size + vali_size)\n    ),\n    ('split')\n] = 1\ndf.loc[\n    df.index.values &gt;= train_size + vali_size,\n    ('split')\n] = 2\n\n# prepare timeseries input feature colum\n# (here we are using 20 preceding values to predict the target)\nadd_sequence_feature_column(df, 'temperature', 20)\ndf.to_csv('&lt;PATH_TO_FILE&gt;/temperature_la.csv')\n</code></pre> <pre><code>ludwig experiment \\\n--dataset &lt;PATH_TO_FILE&gt;/temperature_la.csv \\\n  --config config.yaml\n</code></pre> <p>With <code>config.yaml</code>:</p> <pre><code>input_features:\n-\nname: temperature_feature\ntype: timeseries\nencoder: type: rnn\nembedding_size: 32\nstate_size: 32\n\noutput_features:\n-\nname: temperature\ntype: number\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Welcome to Ludwig's Getting Started Guide.</p> <p>This will go through a full workflow using the Rotten Tomatoes dataset, a CSV file with variety of feature types and a binary target:</p> <ul> <li>Installation</li> <li>Dataset preparation</li> <li>Training</li> <li>Prediction and evaluation</li> <li>Hyperparameter optimization</li> <li>Serving</li> <li>Distributed training on Ray</li> </ul>"},{"location":"getting_started/docker/","title":"Ludwig with Docker","text":"<p>You can also run Ludwig using the docker images available on dockerhub. These images come with a full set of pre-requiste packages to support the capabilities of Ludwig</p>"},{"location":"getting_started/docker/#repositories","title":"Repositories","text":"<p>The following repositories each contain a version of Ludwig with full features built from the <code>master</code> branch.</p> <ul> <li><code>ludwigai/ludwig</code> Ludwig packaged with PyTorch</li> <li><code>ludwigai/ludwig-gpu</code> Ludwig packaged with gpu-enabled version of PyTorch</li> <li><code>ludwigai/ludwig-ray</code> Ludwig packaged with PyTorch   and 2.2 version of ray-project/ray</li> <li><code>ludwigai/ludwig-ray-gpu</code> Ludwig packaged with gpu-enabled versions of PyTorch   and 2.2 version of ray-project/ray</li> </ul>"},{"location":"getting_started/docker/#image-tags","title":"Image Tags","text":"<p>The following are the image tags that can be used when pulling and running the docker images.</p> <ul> <li><code>master</code> - built from Ludwig's <code>master</code> branch</li> <li><code>nightly</code> - nightly build of Ludwig's software.</li> <li><code>sha-&lt;commit point&gt;</code> - version of Ludwig software at designated git sha1   7-character commit point.</li> </ul>"},{"location":"getting_started/docker/#running-containers","title":"Running Containers","text":"<p>Here are some examples of using the <code>ludwigai/ludwig:master</code> image to:</p> <ul> <li>run the <code>ludwig cli</code> command or</li> <li>run Python program containing Ludwig api or</li> <li>view Ludwig results with Tensorboard</li> </ul> <p>For purposes of the examples assume this host directory structure</p> <pre><code>/top/level/directory/path/\n    data/\n        train.csv\n    src/\n        config.yaml\n        ludwig_api_program.py\n</code></pre>"},{"location":"getting_started/docker/#run-ludwig-cli","title":"Run Ludwig CLI","text":"<pre><code># set shell variable to parent directory\nparent_path=/top/level/directory/path\n\n# invoke docker run command to execute the ludwig cli\n# map host directory ${parent_path}/data to container /data directory\n# map host directory ${parent_path}/src to container /src directory\ndocker run -v ${parent_path}/data:/data  \\\n-v ${parent_path}/src:/src \\\nludwigai/ludwig:master \\\nexperiment --config /src/config.yaml \\\n--dataset /data/train.csv \\\n--output_directory /src/results\n</code></pre> <p>Experiment results can be found in host directory <code>/top/level/directory/path/src/results</code></p>"},{"location":"getting_started/docker/#run-python-program-using-ludwig-apis","title":"Run Python program using Ludwig APIs","text":"<pre><code># set shell variable to parent directory\nparent_path=/top/level/directory/path\n\n# invoke docker run command to execute Python interpreter\n# map host directory ${parent_path}/data to container /data directory\n# map host directory ${parent_path}/src to container /src directory\n# set current working directory to container /src directory\n# change default entrypoint from ludwig to python\ndocker run  -v ${parent_path}/data:/data  \\\n-v ${parent_path}/src:/src \\\n-w /src \\\n--entrypoint python \\\nludwigai/ludwig:master /src/ludwig_api_program.py\n</code></pre> <p>Ludwig results can be found in host directory <code>/top/level/directory/path/src/results</code></p>"},{"location":"getting_started/evaluate/","title":"Prediction and Evaluation","text":"<p>After the model has been trained, it can be used to predict the target output features on new data.</p> <p>We've created a small test dataset containing input features for 10 movie reviews that we can use for testing. Download the test dataset here.</p> <p>Let's make some predictions on the test dataset!</p> CLIPythonDocker CLI <pre><code>ludwig predict --model_path results/experiment_run/model --dataset rotten_tomatoes_test.csv\n</code></pre> <pre><code># This step can be skipped if you are working in a notebook, and you can simply\n# re-use the model created in the training section.\nmodel = LudwigModel.load('results/experiment_run/model')\n\npredictions, _ = model.predict(dataset='rotten_tomatoes_test.csv')\npredictions.head()\n</code></pre> <pre><code>docker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig predict --model_path /rotten_tomatoes_data/results/experiment_run/model --dataset /rotten_tomatoes_data/rotten_tomatoes.csv\n</code></pre> <p>Running this command will return model predictions. Your results should look something like this:</p> Index recommended_probabilities recommended_predictions recommended_probabilities_False recommended_probabilities_True recommended_probability 0 [0.09741002321243286, 0.9025899767875671] True 0.097410 0.902590 0.902590 1 [0.6842662990093231, 0.3157337009906769] False 0.684266 0.315734 0.684266 2 [0.026504933834075928, 0.973495066165 9241] True 0.026505 0.973495 0.973495 3 [0.022977590560913086, 0.9770224094390869] True 0.022978 0.977022 0.977022 4 [0.9472369104623795, 0.052763089537620544] False 0.947237 0.052763 0.947237 <p>A handy <code>ludwig experiment</code> CLI command is also available. This one command performs training and then prediction using the checkpoint with the best validation metric.</p> <p>In addition to predictions, Ludwig also computes a suite of evaluation metrics, depending on the output feature's type. The exact metrics that are computed for each output feature type can be found here.</p> <p>Note</p> <p>Non-loss evaluation metrics, like accuracy, require ground truth values of the target outputs.</p> CLIPythonDocker CLI <pre><code>ludwig evaluate --dataset path/to/data.csv --model_path /path/to/model\n</code></pre> <pre><code>eval_stats, _, _ = model.evaluate(dataset='rotten_tomatoes_test.csv')\n</code></pre> <pre><code>cp rotten_tomatoes_test.csv ./rotten_tomatoes_data\ndocker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig evaluate --dataset /rotten_tomatoes_data/rotten_tomatoes_test.csv --model_path /rotten_tomatoes_data/results/experiment_run/model\n</code></pre> <p>Evaluation performance can be visualized using <code>ludwig visualize</code>. This enables us to visualize metrics like for omparing performances and predictions across different models. For instance, if you have two models which you want to compare evaluation statistics for, you could use the following commands:</p> CLIPythonDocker CLI <pre><code>ludwig visualize --visualization compare_performance --test_statistics path/to/test_statistics_model_1.json path/to/test_statistics_model_2.json\n</code></pre> <pre><code>from ludwig.visualize import compare_performance\n\ncompare_performance([eval_stats_model_1, eval_stats_model_2])\n</code></pre> <pre><code>docker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig visualize --visualization compare_performance --test_statistics /rotten_tomatoes_data/path/to/test_statistics_model_1.json /rotten_tomatoes_data/path/to/test_statistics_model_2.json\n</code></pre> <p>This will return a bar plot comparing the performance of each model on different metrics like the example below.</p> <p></p>"},{"location":"getting_started/hyperopt/","title":"Hyperopt","text":"<p>After training our first model and using it to predict new data with reasonable accuracy, how can we make the model better?</p> <p>Ludwig can perform hyperparameter optimization by simply adding <code>hyperopt</code> to the Ludwig config.</p> rotten_tomatoes.yaml<pre><code>input_features:\n- name: genres\ntype: set\n- name: content_rating\ntype: category\n- name: top_critic\ntype: binary\n- name: runtime\ntype: number\n- name: review_content\ntype: text\nencoder: type: embed\noutput_features:\n- name: recommended\ntype: binary\nhyperopt:\ngoal: maximize\noutput_feature: recommended\nmetric: accuracy\nsplit: validation\nparameters:\ntraining.learning_rate:\nspace: loguniform\nlower: 0.0001\nupper: 0.1\ntraining.optimizer.type:\nspace: choice\ncategories: [sgd, adam, adagrad]\nreview_content.embedding_size:\nspace: choice\ncategories: [128, 256]\nsearch_alg:\ntype: variant_generator\nexecutor:\nnum_samples: 10\n</code></pre> <p>In this example we have specified a basic hyperopt config with the following specifications:</p> <ul> <li>We have set the <code>goal</code> to maximize the accuracy metric on the validation split</li> <li>The parameters we are optimizing are the learning rate, the optimizer type, and the embedding_size of text representation to use.</li> <li>When optimizing learning rate we are randomly selecting values on a log scale between 0.0001 and 0.1.</li> <li>When optimizing the optimizer type, we randomly select the optimizer from sgd, adam, and adagrad optimizers.</li> <li>When optimizing the embedding_size of text representation we randomly chose between 128 or 256.</li> <li>We set hyperopt <code>executor</code> to use Ray Tune's <code>variant_generator</code> search algorithm and generates 10 random hyperparameter combinations from the search space we defined.  The execution will locally run trials in parallel.</li> <li>Ludwig supports advanced hyperparameter sampling algorithms like Bayesian optimization and genetical algorithms. See this guide for details.</li> </ul> <p>The hyperparameter optimization strategy is run using the ludwig hyperopt command:</p> CLIPythonDocker CLI <pre><code>ludwig hyperopt --config rotten_tomatoes.yaml --dataset rotten_tomatoes.csv\n</code></pre> <pre><code>from ludwig.hyperopt.run import hyperopt\nimport pandas\n\ndf = pandas.read_csv('rotten_tomatoes.csv')\nresults = hyperopt(config='rotten_tomatoes.yaml', dataset=df)\n</code></pre> <pre><code>docker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig hyperopt --config /rotten_tomatoes_data/rotten_tomatoes.yaml --dataset /rotten_tomatoes_data/rotten_tomatoes.csv\n</code></pre> <p>Every parameter within the config can be tuned using hyperopt. Refer to the full hyperopt guide to learn more.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>Ludwig is a declarative deep learning framework that allows users to train, evaluate, and deploy models without the need to write code.</p> <p>Being declarative means you only need to tell Ludwig what columns in your data are input and output features, and Ludwig will figure out how to train the best model.</p> <p>For users familiar with Python, we recommend installing with <code>pip</code> within an isolated virtual environment. If not, you can use our pre-built <code>docker</code> images. Advanced users can also install Ludwig from <code>git</code>.</p> <p>For large or long-running workloads, Ludwig can be run remotely in the cloud or on a private compute cluster using <code>Ray</code>.</p>"},{"location":"getting_started/installation/#with-pip","title":"Python (with Pip) recommended","text":"<pre><code>pip install ludwig\n</code></pre> <p>This will install Ludwig's basic requirements for modeling with binary, category, number, text, image, and audio features. The requirements for additional functionality are separated out so that users are able to install only the ones they actually need:</p> <ul> <li><code>ludwig[serve]</code> for serving dependencies.</li> <li><code>ludwig[viz]</code> for visualization dependencies.</li> <li><code>ludwig[hyperopt]</code> for hyperparameter optimization dependencies.</li> <li><code>ludwig[distributed]</code> for distributed training on Ray using Dask and Horovod.</li> </ul> <p>The full set of dependencies can be installed with:</p> <pre><code>pip install 'ludwig[full]'\n</code></pre> <p>Warning</p> <p>If this is your first time installing Ludwig into your environment, you may need to temporarily install with <code>pip install ludwig</code> before installing with full dependencies until hummingbird#688 is resolved.</p>"},{"location":"getting_started/installation/#gpu-support","title":"GPU support","text":"<p>If your machine has a GPU to accelerate the training process, make sure you install a GPU-enabled version of PyTorch before installing Ludwig:</p> <pre><code>pip install torch -f https://download.pytorch.org/whl/cu113/torch_stable.html\n</code></pre> <p>The example above will install the latest version of PyTorch with CUDA 11.3. See the official PyTorch docs for more details on installing the right version of PyTorch for your environment.</p>"},{"location":"getting_started/installation/#with-docker","title":"Docker","text":"<p>The Ludwig team publishes official Docker images that come with the full set of dependencies pre-installed. You can pull the <code>latest</code> images (for the most recent official Ludwig release) by running:</p> <pre><code>docker pull ludwigai/ludwig:latest\n</code></pre> <p>The <code>ludwig</code> command line tool is provided as the entrypoint for all commands.</p>"},{"location":"getting_started/installation/#gpu-support_1","title":"GPU support","text":"<p>If your machine has a GPU to accelerate the training process, pull the official Ludwig image with GPU support:</p> <pre><code>docker pull ludwigai/ludwig-gpu:latest\n</code></pre>"},{"location":"getting_started/installation/#with-git","title":"Git","text":"<p>For developers who wish to build the source code from the GitHub repository, first clone the repo locally:</p> <pre><code>git clone git@github.com:ludwig-ai/ludwig.git\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install -e '.[test]'\n</code></pre> <p>The <code>test</code> extra will pull in all Ludwig dependencies in addition to test dependencies.</p>"},{"location":"getting_started/prepare_data/","title":"Dataset preparation","text":"<p>Ludwig can train on any table-like dataset, meaning that every feature has its own column and every example its own row.</p> <p>In this example, we'll use this Rotten Tomatoes dataset, a CSV file with variety of feature types and a binary target.</p> <p>Download the data locally here.</p> <p>Let's take a look at the first 5 rows to see how the data is arranged:</p> CLIPython <pre><code>head -n 5 rotten_tomatoes.csv\n</code></pre> <pre><code>import pandas as pd\n\ndf = pd.read_csv('rotten_tomatoes.csv')\ndf.head()\n</code></pre> <p>Your results should look a little something like this:</p> movie_title content_rating genres runtime top_critic review_content recommended Deliver Us from Evil R Action &amp; Adventure, Horror 117.0 TRUE Director Scott Derrickson and his co-writer, Paul Harris Boardman, deliver a routine procedural with unremarkable frights. 0 Barbara PG-13 Art House &amp; International, Drama 105.0 FALSE Somehow, in this stirring narrative, Barbara manages to keep hold of her principles, and her humanity and courage, and battles to save a dissident teenage girl whose life the Communists are trying to destroy. 1 Horrible Bosses R Comedy 98.0 FALSE These bosses cannot justify either murder or lasting comic memories, fatally compromising a farce that could have been great but ends up merely mediocre. 0 Money Monster R Drama 98.0 FALSE A satire about television that feels like it was made by the kind of people who claim they don't even watch TV. 0 Battle Royale NR Action &amp; Adventure, Art House &amp; International, Drama, Mystery &amp; Suspense 114.0 FALSE Battle Royale is The Hunger Games not diluted for young audiences. 1"},{"location":"getting_started/ray/","title":"Distributed training on Ray","text":"<p>Ludwig has strong support for Ray, a framework for distributed computing that makes it easy to scale up code that runs on your local machine to execute in parallel across a cluster of machines.</p> <p>Let's spin up a ray cluster, so we can try out distributed training and hyperparameter tuning in parallel. Make sure you have access to an AWS EC2 node provider.</p> <p>First install the Ray Cluster Launcher:</p> <pre><code>pip install ray\n</code></pre> <p>Next let's make a configuration file named <code>cluster.yaml</code> for the Ray Cluster:</p> cluster.yaml<pre><code>cluster_name: ludwig-ray-gpu-latest\n\nmin_workers: 4\nmax_workers: 4\n\ndocker:\nimage: \"ludwigai/ludwig-ray-gpu:latest\"\ncontainer_name: \"ray_container\"\n\nhead_node:\nInstanceType: m5.2xlarge\nImageId: latest_dlami\n\nworker_nodes:\nInstanceType: g4dn.2xlarge\nImageId: latest_dlami\n</code></pre> <p>Finally, you can spin up the cluster with the following command:</p> <pre><code>ray up cluster.yaml\n</code></pre> <p>In order to run a distributed training job, make sure you have your dataset stored in an S3 bucket, and run this command:</p> <pre><code>ray submit cluster.yaml ludwig train --config rotten_tomatoes.yaml --dataset s3://mybucket/rotten_tomatoes.csv\n</code></pre> <p>You can also run a distributed hyperopt job with this command:</p> <pre><code>ray submit cluster.yaml ludwig hyperopt --config rotten_tomatoes.yaml --dataset s3://mybucket/rotten_tomatoes.csv\n</code></pre> <p>For more information on using Ray with Ludwig, refer to the ray configuration guide.</p>"},{"location":"getting_started/serve/","title":"Serving","text":"<p>Model pipelines trained with Ludwig can be served by spawning a Rest API using the FastAPI library.</p> <p>Let's serve the model we just created.</p> CLIDocker CLI <pre><code>ludwig serve --model_path ./results/experiment_run/model\n</code></pre> <pre><code>docker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig serve --model_path /rotten_tomatoes_data/results/experiment_run/model\n</code></pre> <p>Now that our server is up and running, you can make a POST request on the endpoint to get predictions back:</p> <pre><code>curl http://0.0.0.0:8000/predict -X POST -F \"movie_title=Friends With Money\" -F \"content_rating=R\" -F \"genres=Art House &amp; International, Comedy, Drama\" -F \"runtime=88.0\" -F \"top_critic=TRUE\" -F \"review_content=The cast is terrific, the movie isn't.\"\n</code></pre> <p>Since the output feature is a binary type feature, the output from the POST call will look something like this:</p> <pre><code>{\n   \"review_content_predictions\": false,\n   \"review_content_probabilities_False\": 0.76,\n   \"review_content_probabilities_True\": 0.24,\n   \"review_content_probability\": 0.76\n}\n</code></pre> <p>Note</p> <p>Users can also send POST requests to the <code>/batch_predict</code> endpoint to run inference on multiple examples at once. Read more about ludwig serve to learn more about ludwig deployments.</p>"},{"location":"getting_started/train/","title":"Training","text":"<p>To train a model with Ludwig, we first need to create a Ludwig configuration. The config specifies input features, output features, preprocessing, model architecture, training loop, hyperparameter search, and backend infrastructure -- everything that's needed to build, train, and evaluate a model.</p> <p>At a minimum, the config must specify the model's input and output features.</p> <p>For now, let's use a basic config that just specifies the inputs and output and leaves the rest to Ludwig:</p> rotten_tomatoes.yaml<pre><code>input_features:\n- name: genres\ntype: set\npreprocessing:\ntokenizer: comma\n- name: content_rating\ntype: category\n- name: top_critic\ntype: binary\n- name: runtime\ntype: number\n- name: review_content\ntype: text\nencoder: type: embed\noutput_features:\n- name: recommended\ntype: binary\n</code></pre> <p>This config file tells Ludwig that we want to train a model that uses the following input features:</p> <ul> <li>The genres associated with the movie will be used as a set feature</li> <li>The movie's content rating will be used as a category feature</li> <li>Whether the review was done by a top critic or not will be used as a binary feature</li> <li>The movie's runtime will be used as a number feature</li> <li>The review content will be used as text feature</li> </ul> <p>This config file also tells Ludwig that we want our model to have the following output features:</p> <ul> <li>The recommendation of whether to watch the movie or not will be output as a binary feature</li> </ul> <p>Once you've created the <code>rotten_tomatoes.yaml</code> file with the contents above, you're ready to train your first model:</p> CLIPythonDocker CLI <pre><code>ludwig train --config rotten_tomatoes.yaml --dataset rotten_tomatoes.csv\n</code></pre> <pre><code>from ludwig.api import LudwigModel\nimport pandas\n\ndf = pandas.read_csv('rotten_tomatoes.csv')\nmodel = LudwigModel(config='rotten_tomatoes.yaml')\nresults = model.train(dataset=df)\n</code></pre> <pre><code>mkdir rotten_tomatoes_data\nmv rotten_tomatoes.yaml ./rotten_tomatoes_data\nmv rotten_tomatoes.csv ./rotten_tomatoes_data\ndocker run -t -i --mount type=bind,source={absolute/path/to/rotten_tomatoes_data},target=/rotten_tomatoes_data ludwigai/ludwig train --config /rotten_tomatoes_data/rotten_tomatoes.yaml --dataset /rotten_tomatoes_data/rotten_tomatoes.csv --output_directory /rotten_tomatoes_data\n</code></pre> <p>Note</p> <p>In this example, we encoded the text feature with an <code>embed</code> encoder, which assigns an embedding for each word and sums them. Ludwig provides many options for tokenizing and embedding text like with CNNs, RNNs, Transformers, and pretrained models such as BERT or GPT-2 (provided through huggingface). Using a different text encoder is simple as changing encoder option in the config from <code>embed</code> to <code>bert</code>. Give it a try!</p> <pre><code>input_features:\n- name: genres\ntype: set\npreprocessing:\ntokenizer: comma\n- name: content_rating\ntype: category\n- name: top_critic\ntype: binary\n- name: runtime\ntype: number\n- name: review_content\ntype: text\nencoder: type: bert\noutput_features:\n- name: recommended\ntype: binary\n</code></pre> <p>Ludwig is very flexible. Users can configure just about any parameter in their models including training parameters, preprocessing parameters, and more, directly from the configuration. Check out the config documentation for the full list of parameters available in the configuration.</p>"},{"location":"user_guide/","title":"User Guide","text":"<p>Welcome to the Ludwig User Guide!</p> <p>Here, you can read about What Ludwig is, How Ludwig works, and various features that Ludwig supports like AutoML, Hyperparameter optimization, Distributed Training, Model Export, Serving, Visualization, the Dataset Zoo, and more, with code snippets and examples scattered throughout.</p> <p>For a new-user-friendly guide, check out Getting Started.</p> <p>For comprehensive documentation about what parameters are available in the Ludwig configuration, what they do, and how to use them, check out Configuration.</p> <p>For complete end-to-end examples, check out the Examples.</p> <p>Happy reading!</p>"},{"location":"user_guide/automl/","title":"AutoML","text":"<p>Ludwig AutoML takes a dataset, the target column, and a time budget, and returns a trained Ludwig model.</p> <p>Ludwig AutoML is currently experimental and is focused on tabular datasets.  A blog describing its development, evaluation, and use is here.</p> <p>Ludwig AutoML infers the types of the input and output features, chooses the model architecture, and launches a Ray Tune Async HyperBand search job across a set of hyperparameters and ranges, limited by the specified time budget.  It returns the set of models produced by the trials in the search sorted from best to worst, along with a hyperparameter search report, which can be inspected manually or post-processed by various Ludwig visualization tools.</p> <p>Users can audit and interact with Ludwig AutoML in various ways, described below.</p>"},{"location":"user_guide/automl/#auto_train","title":"auto_train","text":"<p>The basic API for Ludwig AutoML is <code>auto_train</code>.  A simple example of its invocation can be found here.</p> <pre><code>import logging\nimport pprint\n\nfrom ludwig.automl import auto_train\nfrom ludwig.datasets import mushroom_edibility\nfrom ludwig.utils.dataset_utils import get_repeatable_train_val_test_split\n\nmushroom_df = mushroom_edibility.load()\nmushroom_edibility_df = get_repeatable_train_val_test_split(mushroom_df, 'class', random_seed=42)\n\nauto_train_results = auto_train(\n    dataset=mushroom_edibility_df,\n    target='class',\n    time_limit_s=7200,\n    tune_for_memory=False,\n    user_config={'preprocessing': {'split': {'column': 'split', 'type': 'fixed'}}},\n)\n\npprint.pprint(auto_train_results)\n</code></pre>"},{"location":"user_guide/automl/#create_auto_config","title":"create_auto_config","text":"<p>The Ludwig AutoML <code>create_auto_config</code> API outputs <code>auto_train</code>\u2019s hyperparameter search configuration without running the search. This API is useful for examining AutoML's chosen input and output feature types, model architecture, and hyperparameters and ranges. A simple example of its invocation:</p> <pre><code>import logging\nimport pprint\n\nfrom ludwig.automl import create_auto_config\nfrom ludwig.datasets import mushroom_edibility\nfrom ludwig.utils.dataset_utils import get_repeatable_train_val_test_split\n\nmushroom_df = mushroom_edibility.load()\nmushroom_edibility_df = get_repeatable_train_val_test_split(mushroom_df, 'class', random_seed=42)\n\nauto_config = create_auto_config(\n    dataset=mushroom_edibility_df,\n    target='class',\n    time_limit_s=7200,\n    tune_for_memory=False,\n    user_config={'preprocessing': {'split': {'column': 'split', 'type': 'fixed'}}},\n)\n\npprint.pprint(auto_config)\n</code></pre> <p>Source</p> <p>The API is also useful for manual refinement of the AutoML-generated search; the output of this API can be edited and then directly used as the input configuration for a Ludwig hyperparameter search job.</p>"},{"location":"user_guide/automl/#overriding-auto-configs-with-user_config","title":"Overriding auto configs with user_config","text":"<p>The <code>user_config</code> parameter can be provided to the <code>auto_train</code> or <code>create_auto_config</code> APIs to override specified parts of the configuration produced.</p> <p>For example, we can specify that the <code>TripType</code> output feature for the Walmart Recruiting dataset specifies be set to type <code>category</code>, to override the Ludwig AutoML type detection system\u2019s characterization of the feature as a <code>number</code> feature.</p> <pre><code>import logging\nimport pprint\n\nfrom ludwig.automl import auto_train\nfrom ludwig.datasets import walmart_recruiting\nfrom ludwig.utils.dataset_utils import get_repeatable_train_val_test_split\n\nwalmart_df = walmart_recruiting.load()\nwalmart_recruiting_df = get_repeatable_train_val_test_split(walmart_df, 'TripType', random_seed=42)\n\nauto_train_results = auto_train(\n    dataset=walmart_recruiting_df,\n    target='TripType',\n    time_limit_s=3600,\n    tune_for_memory=False,\n    user_config={'output_features': [{'column': 'TripType', 'name': 'TripType', 'type': 'category'}],\n        'preprocessing': {'split': {'column': 'split', 'type': 'fixed'}}},\n)\n\npprint.pprint(auto_train_results)\n</code></pre> <p>Source</p> <p>We can also specify that the hyperparameter search job optimize for maximum accuracy of the specified output feature rather than minimal loss of all combined output features, which is the default.</p> <pre><code>import logging\nimport pprint\n\nfrom ludwig.automl import auto_train\nfrom ludwig.datasets import mushroom_edibility\nfrom ludwig.utils.dataset_utils import get_repeatable_train_val_test_split\n\nmushroom_df = mushroom_edibility.load()\nmushroom_edibility_df = get_repeatable_train_val_test_split(mushroom_df, 'class', random_seed=42)\n\nauto_train_results = auto_train(\n    dataset=mushroom_edibility_df,\n    target='class',\n    time_limit_s=3600,\n    tune_for_memory=False,\n    user_config={'hyperopt': {'goal': 'maximize', 'metric': 'accuracy', 'output_feature': 'class'},\n        'preprocessing': {'split': {'column': 'split', 'type': 'fixed'}}},\n)\n\npprint.pprint(auto_train_results)\n</code></pre> <p>Source</p>"},{"location":"user_guide/cloud_storage/","title":"Cloud Storage","text":"<p>Cloud object storage systems like Amazon S3 are useful for working with large datasets or running on a cluster of machines for distributed training. Ludwig provides out-of-the-box support for reading and writing to remote systems through fsspec.</p> <p>Example:</p> <pre><code>ludwig train \\\n--dataset s3://my_datasets/subdir/dataset.parquet \\\n--output_directory s3://my_experiments/foo\n</code></pre>"},{"location":"user_guide/cloud_storage/#environment-setup","title":"Environment Setup","text":"<p>The sections below cover how to read and write between your preferred remote filesystem in Ludwig.</p>"},{"location":"user_guide/cloud_storage/#amazon-s3","title":"Amazon S3","text":"<p>Install filesystem driver in your Docker image: <code>pip install s3fs</code>.</p> <p>Mount your <code>$HOME/.aws/credentials</code> file into the container or set the following enviroment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> </ul> <p>Refer to paths with protocol <code>s3://</code>.</p>"},{"location":"user_guide/cloud_storage/#minio","title":"MinIO","text":"<p>MinIO uses the same protocol as s3, but requires an additional environment variable to be set:</p> <ul> <li><code>AWS_ENDPOINT_URL</code></li> </ul>"},{"location":"user_guide/cloud_storage/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Install filesystem driver in your Docker image: <code>pip install gcsfs</code>.</p> <p>Generate a token as described here.</p> <p>Mount the token file into the container at one of the locations described in the <code>gcsfs</code> docs.</p> <p>Refer to paths with protocol <code>gs://</code> or <code>gcs://</code>.</p>"},{"location":"user_guide/cloud_storage/#azure-storage","title":"Azure Storage","text":"<p>Install filesystem driver in your Docker image: <code>pip install adlfs</code>.</p> <p>Mount your <code>$HOME/.aws/credentials</code> file into the container or set the following enviroment variables:</p> <ul> <li><code>AZURE_STORAGE_CONNECTION_STRING</code></li> </ul> <p>See <code>adlfs</code> docs for more details.</p> <p>Refer to paths with protocol <code>az://</code> or <code>abfs://</code>.</p>"},{"location":"user_guide/cloud_storage/#additional-configuration","title":"Additional Configuration","text":""},{"location":"user_guide/cloud_storage/#remote-dataset-cache","title":"Remote Dataset Cache","text":"<p>Often your input datasets will be in a read-only location such as a shared data lake. In these cases, you won't want to rely on Ludwig's default caching behavior of writing to the same base directory as the input dataset. Instead, you can configure Ludwig to write to a dedicated cache directory / bucket by configuring the <code>backend</code> section of the config:</p> <pre><code>backend:\ncache_dir: \"s3://ludwig_cache\"\n</code></pre> <p>Individual entries will be written using a filename computed from the checksum of the dataset and Ludwig config used for training.</p> <p>One additional benefit of setting up a dedicated cache is to make use of cache eviction policies. For example, setting up a TTL so cached datasets are automatically cleaned up after a few days.</p>"},{"location":"user_guide/cloud_storage/#using-different-cache-and-dataset-filesystems","title":"Using different cache and dataset filesystems","text":"<p>In some cases you may want your dartaset cache to reside in a different filesystem or account than your input dataset. Since this requires maintaing two sets of credentials -- one of the input data and one for the cache -- Ludwig provides additional configuration options for the cache credentials.</p> <p>Credentials can be provided explicitly in the config:</p> <pre><code>backend:\ncache_dir: \"s3://ludwig_cache\"\ncache_credentials:\ns3:\nclient_kwargs:\naws_access_key_id: \"test\"\naws_secret_access_key: \"test\"\n</code></pre> <p>Or in a mounted file for additional security:</p> <pre><code>backend:\ncache_dir: \"s3://ludwig_cache\"\ncache_credentials: /home/user/.credentials.json\n</code></pre>"},{"location":"user_guide/command_line_interface/","title":"Command Line Interface","text":""},{"location":"user_guide/command_line_interface/#commands","title":"Commands","text":"<p>Ludwig provides several functions through its command line interface.</p> Mode Description <code>train</code> Trains a model <code>predict</code> Predicts using a pretrained model <code>evaluate</code> Evaluate a pretrained model's performance <code>experiment</code> Runs a full experiment training a model and evaluating it <code>hyperopt</code> Perform hyperparameter optimization <code>serve</code> Serves a pretrained model <code>visualize</code> Visualizes experiment results <code>init_config</code> Initialize a user config from a dataset and targets <code>render_config</code> Renders the fully populated config with all defaults set <code>collect_summary</code> Prints names of weights and layers activations to use with other collect commands <code>collect_weights</code> Collects tensors containing a pretrained model weights <code>collect_activations</code> Collects tensors for each datapoint using a pretrained model <code>export_torchscript</code> Exports Ludwig models to Torchscript <code>export_neuropod</code> Exports Ludwig models to Neuropod <code>export_mlflow</code> Exports Ludwig models to MLflow <code>preprocess</code> Preprocess data and saves it into HDF5 and JSON format <code>synthesize_dataset</code> Creates synthetic data for testing purposes <p>These are described in detail below.</p>"},{"location":"user_guide/command_line_interface/#train","title":"train","text":"<p>Train a model from your data.</p> <pre><code>ludwig train [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.train [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig train [options]\n\nThis script trains a model\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  --experiment_name EXPERIMENT_NAME\n                        experiment name\n  --model_name MODEL_NAME\n                        name for the model\n  --dataset DATASET     input data file path. If it has a split column, it\n                        will be used for splitting (0: train, 1: validation,\n                        2: test), otherwise the dataset will be randomly split\n  --training_set TRAINING_SET\n                        input train data file path\n  --validation_set VALIDATION_SET\n                        input validation data file path\n  --test_set TEST_SET   input test data file path\n  --training_set_metadata TRAINING_SET_METADATA\n                        input metadata JSON file path. An intermediate\n                        preprocessed  containing the mappings of the input\n                        file created the first time a file is used, in the\n                        same directory with the same name and a .json\n                        extension\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\n                        format of the input data\n  -sspi, --skip_save_processed_input\n                        skips saving intermediate HDF5 and JSON files\n  -c CONFIG, --config CONFIG\n                        Path to the YAML file containing the model configuration\n  -cs CONFIG_STR, --config_str CONFIG_STRING\n                        JSON or YAML serialized string of the model configuration. Ignores --config\n  -mlp MODEL_LOAD_PATH, --model_load_path MODEL_LOAD_PATH\n                        path of a pretrained model to load as initialization\n  -mrp MODEL_RESUME_PATH, --model_resume_path MODEL_RESUME_PATH\n                        path of the model directory to resume training of\n  -sstd, --skip_save_training_description\n                        disables saving the description JSON file\n  -ssts, --skip_save_training_statistics\n                        disables saving training statistics JSON file\n  -ssm, --skip_save_model\n                        disables saving weights each time the model improves.\n                        By default Ludwig saves weights after each epoch the\n                        validation metric improves, but if the model is really\n                        big that can be time consuming. If you do not want to\n                        keep the weights and just find out what performance\n                        can a model get with a set of hyperparameters, use\n                        this parameter to skip it\n  -ssp, --skip_save_progress\n                        disables saving weights after each epoch. By default\n                        ludwig saves weights after each epoch for enabling\n                        resuming of training, but if the model is really big\n                        that can be time consuming and will save twice as much\n                        space, use this parameter to skip it\n  -ssl, --skip_save_log\n                        disables saving TensorBoard logs. By default Ludwig\n                        saves logs for the TensorBoard, but if it is not\n                        needed turning it off can slightly increase the\n                        overall speed\n  -rs RANDOM_SEED, --random_seed RANDOM_SEED\n                        a random seed that is going to be used anywhere there\n                        is a call to a random number generator: data\n                        splitting, parameter initialization and training set\n                        shuffling\n  -g GPUS [GPUS ...], --gpus GPUS [GPUS ...]\n                        list of gpus to use\n  -gml GPU_MEMORY_LIMIT, --gpu_memory_limit GPU_MEMORY_LIMIT\n                        maximum memory in MB to allocate per GPU device\n  -dpt, --disable_parallel_threads\n                        disable Torch from using multithreading for\n                        reproducibility\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n</code></pre> <p>When Ludwig trains a model it creates two intermediate files, one HDF5 and one JSON. The HDF5 file contains the data mapped to numpy ndarrays, while the JSON file contains the mappings from the values in the tensors to their original labels.</p> <p>For instance, for a categorical feature with 3 possible values, the HDF5 file will contain integers from 0 to 3 (with 0 being a <code>&lt;UNK&gt;</code> category), while the JSON file will contain a <code>idx2str</code> list containing all tokens (<code>[&lt;UNK&gt;, label_1, label_2, label_3]</code>), a <code>str2idx</code> dictionary (<code>{\"&lt;UNK&gt;\": 0, \"label_1\": 1, \"label_2\": 2, \"label_3\": 3}</code>) and a <code>str2freq</code> dictionary (<code>{\"&lt;UNK&gt;\": 0, \"label_1\": 93, \"label_2\": 55, \"label_3\": 24}</code>).</p> <p>The reason to have those  intermediate files is two-fold: on one hand, if you are going to train your model again Ludwig will try to load them instead of recomputing all tensors, which saves a considerable amount of time, and on the other hand when you want to use your model to predict, data has to be mapped to tensors in exactly the same way it was mapped during training, so you'll be required to load the JSON metadata file in the <code>predict</code> command.</p> <p>The first time you provide a UTF-8 encoded dataset (<code>--dataset</code>), the HDF5 and JSON files are created, from the second time on Ludwig will load them instead of the dataset even if you specify the dataset (it looks in the same directory for files names in the same way but with a different extension), finally you can directly specify the HDF5 and JSON files.</p> <p>As the mapping from raw data to tensors depends on the type of feature that you specify in your configuration, if you change type (for instance from <code>sequence</code> to <code>text</code>) you also have to redo the preprocessing, which is achieved by deleting the HDF5 and JSON files. Alternatively you can skip saving the HDF5 and JSON files specifying <code>--skip_save_processed_input</code>.</p> <p>Splitting between train, validation and test set can be done in several ways. This allows for a few possible input data scenarios:</p> <ul> <li>one single UTF-8 encoded dataset file is provided (<code>-dataset</code>). In this case if the dataset contains a <code>split</code> column with values <code>0</code> for training, <code>1</code> for validation and <code>2</code> for test, this split will be used. If you want to ignore the split column and perform a random split, use a <code>force_split</code> argument in the configuration. In the case when there is no split column, a random <code>70-20-10</code> split will be performed. You can set the percentages and specify if you want stratified sampling in the configuration preprocessing section.</li> <li>you can provide separate UTF-8 encoded training, validation and test sets  (<code>--training_set</code>, <code>--validation_set</code>, <code>--test_set</code>).</li> <li>the HDF5 and JSON file indications specified in the case of a single dataset file apply also in the multiple files case, with the only difference that you need to specify only one JSON file (<code>--train_set_metadata_json</code>).</li> </ul> <p>The validation set is optional, but if absent the training will continue until the end of the training epochs, while when there's a validation set the default behavior is to perform early stopping after the validation measure does not improve for a certain amount of epochs. The test set is optional too.</p> <p>Other optional arguments are <code>--output_directory</code>, <code>--experiment_name</code> and <code>--model name</code>. By default the output directory is <code>./results</code>. That directory will contain a directory named <code>[experiment_name]_[model_name]_0</code> if model name and experiment name are specified. If the same combination of experiment and model name is used again, the integer at the end of the name will be increased. If neither of them is specified the directory will be named <code>run_0</code>. The directory will contain</p> <ul> <li><code>description.json</code> - a file containing a description of the training process with all the information to reproduce it.</li> <li><code>training_statistics.json</code> - a file containing records of all measures and losses for each epoch.</li> <li><code>model</code> - a directory containing model hyperparameters, weights, checkpoints and logs (for TensorBoard).</li> </ul> <p>The configuration can be provided either as a string (<code>--config_str</code>) or as YAML file (<code>--config</code>).</p> <p>Details on how to write your configuration are provided in the Configuration section.</p> <p>During training Ludwig saves two sets of weights for the model, one that is the weights at the end of the epoch where the best performance on the validation measure was achieved and one that is the weights at the end of the latest epoch. The reason for keeping the second set is to be able to resume training in case the training process gets interrupted somehow.</p> <p>To resume training using the latest weights and the whole history of progress so far you have to specify the <code>--model_resume_path</code> argument. You can avoid saving the latest weights and the overall progress so far by using the argument <code>--skip_save_progress</code>, but you will not be able to resume it afterwards.</p> <p>Another available option is to load a previously trained model as an initialization for a new training process. In this case Ludwig will start a new training process, without knowing any progress of the previous model, no training statistics, nor the number of epochs the model has been trained on so far.</p> <p>It's not resuming training, just initializing training with a previously trained model with the same configuration, and it is accomplished through the <code>--model_load_path</code> argument.</p> <p>You can specify a random seed to be used by the python environment, python random package, numpy and Torch with the <code>--random_seed</code> argument. This is useful for reproducibility.</p> <p>Be aware that due to asynchronicity in the Torch's GPU execution, when training on GPU results may not be reproducible.</p> <p>You can manage which GPUs on your machine are used with the <code>--gpus</code> argument, which accepts a string identical to the format of <code>CUDA_VISIBLE_DEVICES</code> environment variable, namely a list of integers separated by comma. You can also specify the maximum amount of GPU memory which will be allocated per device with <code>--gpu_memory_limit</code>. By default all of memory is allocated. If less than all of memory is allocated, Torch will need more GPU memory it will try to increase this amount.</p> <p>If parameter <code>--backend</code> is set, will use the given backend for distributed processing (Horovod or Ray).</p> <p>Finally the <code>--logging_level</code> argument lets you set the amount of logging that you want to see during training.</p> <p>Example:</p> <pre><code>ludwig train --dataset reuters-allcats.csv --config \"{input_features: [{name: text, type: text, encoder: {type: parallel_cnn}}], output_features: [{name: class, type: category}]}\"\n</code></pre>"},{"location":"user_guide/command_line_interface/#predict","title":"predict","text":"<p>This command lets you use a previously trained model to predict on new data. You can call it with:</p> <pre><code>ludwig predict [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.predict [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig predict [options]\n\nThis script loads a pretrained model and uses it to predict\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dataset DATASET     input data file path\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\n                        format of the input data\n  -s {training,validation,test,full}, --split {training,validation,test,full}\n                        the split to test the model on\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -od OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  -ssuo, --skip_save_unprocessed_output\n                        skips saving intermediate NPY output files\n  -sstp, --skip_save_predictions\n                        skips saving predictions CSV files\n  -bs BATCH_SIZE, --batch_size BATCH_SIZE\n                        size of batches\n  -g GPUS, --gpus GPUS  list of gpu to use\n  -gml GPU_MEMORY_LIMIT, --gpu_memory_limit GPU_MEMORY_LIMIT\n                        maximum memory in MB to allocate per GPU device\n  -dpt, --disable_parallel_threads\n                        disable Torch from using multithreading for\n                        reproducibility\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>The same distinction between UTF-8 encoded dataset files and HDF5 / JSON files explained in the train section also applies here. In either case, the JSON metadata file obtained during training is needed in order to map the new data into tensors. If the new data contains a split column, you can specify which split to use to calculate the predictions with the <code>--split</code> argument. By default it's <code>full</code> which means all the splits will be used.</p> <p>A model to load is needed, and you can specify its path with the <code>--model_path</code> argument. If you trained a model previously and got the results in, for instance, <code>./results/experiment_run_0</code>, you have to specify <code>./results/experiment_run_0/model</code> for using it to predict.</p> <p>You can specify an output directory with the argument <code>--output-directory</code>, by default it will be <code>./result_0</code>, with increasing numbers if a directory with the same name is present.</p> <p>The directory will contain a prediction CSV file and a probability CSV file for each output feature, together with raw NPY files containing raw tensors. You can specify not to save the raw NPY output files with the argument <code>skip_save_unprocessed_output</code>.</p> <p>A specific batch size for speeding up the prediction can be specified using the argument <code>--batch_size</code>.</p> <p>Finally the <code>--logging_level</code>, <code>--debug</code>, <code>--gpus</code>, <code>--gpu_memory_limit</code> and <code>--disable_parallel_threads</code>  related arguments behave exactly like described in the train command section.</p> <p>Example:</p> <pre><code>ludwig predict --dataset reuters-allcats.csv --model_path results/experiment_run_0/model/\n</code></pre>"},{"location":"user_guide/command_line_interface/#evaluate","title":"evaluate","text":"<p>This command lets you use a previously trained model to predict on new data and evaluate the performance of the prediction compared to ground truth. You can call it with:</p> <pre><code>ludwig evaluate [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.evaluate [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig evaluate [options]\n\nThis script loads a pretrained model and evaluates its performance by\ncomparing its predictions with ground truth.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dataset DATASET     input data file path\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\n                        format of the input data\n  -s {training,validation,test,full}, --split {training,validation,test,full}\n                        the split to test the model on\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -od OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  -ssuo, --skip_save_unprocessed_output\n                        skips saving intermediate NPY output files\n  -sses, --skip_save_eval_stats\n                        skips saving intermediate JSON eval statistics\n  -scp, --skip_collect_predictions\n                        skips collecting predictions\n  -scos, --skip_collect_overall_stats\n                        skips collecting overall stats\n  -bs BATCH_SIZE, --batch_size BATCH_SIZE\n                        size of batches\n  -g GPUS, --gpus GPUS  list of gpu to use\n  -gml GPU_MEMORY_LIMIT, --gpu_memory_limit GPU_MEMORY_LIMIT\n                        maximum memory in MB to allocate per GPU device\n  -dpt, --disable_parallel_threads\n                        disable Torch from using multithreading for\n                        reproducibility\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>All parameters are the same of predict and the behavior is the same. The only difference isthat <code>evaluate</code> requires the dataset to contain also columns with the same name of output features. This is needed because <code>evaluate</code> compares the predictions produced by the model with the ground truth and will save all those statistics in a <code>test_statistics.json</code> file in the result directory.</p> <p>Note that the data must contain columns for each output feature with ground truth output values in order to compute the performance statistics. If you receive an error regarding a missing output feature column in your data, it means that the data does not contain the columns for each output feature to use as ground truth.</p> <p>Example:</p> <pre><code>ludwig evaluate --dataset reuters-allcats.csv --model_path results/experiment_run_0/model/\n</code></pre>"},{"location":"user_guide/command_line_interface/#experiment","title":"experiment","text":"<p>This command combines training and evaluation into a single handy command. You can request a k-fold cross validation run by specifying the <code>--k_fold</code> parameter.</p> <p>You can call it with:</p> <pre><code>ludwig experiment [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.experiment [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig experiment [options]\n\nThis script trains and evaluates a model\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  --experiment_name EXPERIMENT_NAME\n                        experiment name\n  --model_name MODEL_NAME\n                        name for the model\n  --dataset DATASET     input data file path. If it has a split column, it\n                        will be used for splitting (0: train, 1: validation,\n                        2: test), otherwise the dataset will be randomly split\n  --training_set TRAINING_SET\n                        input train data file path\n  --validation_set VALIDATION_SET\n                        input validation data file path\n  --test_set TEST_SET   input test data file path\n  --training_set_metadata TRAINING_SET_METADATA\n                        input metadata JSON file path. An intermediate\n                        preprocessed  containing the mappings of the input\n                        file created the first time a file is used, in the\n                        same directory with the same name and a .json\n                        extension\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\n                        format of the input data\n  -es {training,validation,test,full}, --eval_split {training,validation,test,full}\n                        the split to evaluate the model on\n  -sspi, --skip_save_processed_input\n                        skips saving intermediate HDF5 and JSON files\n  -ssuo, --skip_save_unprocessed_output\n                        skips saving intermediate NPY output files\n  -kf K_FOLD, --k_fold K_FOLD\n                        number of folds for a k-fold cross validation run\n  -skfsi, --skip_save_k_fold_split_indices\n                        disables saving indices generated to split training\n                        data set for the k-fold cross validation run, but if\n                        it is not needed turning it off can slightly increase\n                        the overall speed\n  -c CONFIG, --config CONFIG\n                        Path to the YAML file containing the model configuration\n  -cs CONFIG_STR, --config_str CONFIG_STRING\n                        JSON or YAML serialized string of the model configuration. Ignores --config\n  -mlp MODEL_LOAD_PATH, --model_load_path MODEL_LOAD_PATH\n                        path of a pretrained model to load as initialization\n  -mrp MODEL_RESUME_PATH, --model_resume_path MODEL_RESUME_PATH\n                        path of the model directory to resume training of\n  -sstd, --skip_save_training_description\n                        disables saving the description JSON file\n  -ssts, --skip_save_training_statistics\n                        disables saving training statistics JSON file\n  -sstp, --skip_save_predictions\n                        skips saving test predictions CSV files\n  -sstes, --skip_save_eval_stats\n                        skips saving eval statistics JSON file\n  -ssm, --skip_save_model\n                        disables saving model weights and hyperparameters each\n                        time the model improves. By default Ludwig saves model\n                        weights after each epoch the validation metric\n                        improves, but if the model is really big that can be\n                        time consuming if you do not want to keep the weights\n                        and just find out what performance a model can get\n                        with a set of hyperparameters, use this parameter to\n                        skip it,but the model will not be loadable later on\n  -ssp, --skip_save_progress\n                        disables saving progress each epoch. By default Ludwig\n                        saves weights and stats after each epoch for enabling\n                        resuming of training, but if the model is really big\n                        that can be time consuming and will uses twice as much\n                        space, use this parameter to skip it, but training\n                        cannot be resumed later on\n  -ssl, --skip_save_log\n                        disables saving TensorBoard logs. By default Ludwig\n                        saves logs for the TensorBoard, but if it is not\n                        needed turning it off can slightly increase the\n                        overall speed\n  -rs RANDOM_SEED, --random_seed RANDOM_SEED\n                        a random seed that is going to be used anywhere there\n                        is a call to a random number generator: data\n                        splitting, parameter initialization and training set\n                        shuffling\n  -g GPUS [GPUS ...], --gpus GPUS [GPUS ...]\n                        list of GPUs to use\n  -gml GPU_MEMORY_LIMIT, --gpu_memory_limit GPU_MEMORY_LIMIT\n                        maximum memory in MB to allocate per GPU device\n  -dpt, --disable_parallel_threads\n                        disable Torch from using multithreading for\n                        reproducibility\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>The parameters combine parameters from both train and test so refer to those sections for an in depth explanation. The output directory will contain the outputs both commands produce.</p> <p>Example:</p> <pre><code>ludwig experiment --dataset reuters-allcats.csv --config \"{input_features: [{name: text, type: text, encoder: {type: parallel_cnn}}], output_features: [{name: class, type: category}]}\"\n</code></pre>"},{"location":"user_guide/command_line_interface/#hyperopt","title":"hyperopt","text":"<p>This command lets you perform an hyperparameter search with a given sampler and parameters. You can call it with:</p> <pre><code>ludwig hyperopt [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.hyperopt [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig hyperopt [options]\n\nThis script searches for optimal Hyperparameters\n\noptional arguments:\n  -h, --help            show this help message and exit\n-sshs, --skip_save_hyperopt_statistics\n                        skips saving hyperopt statistics file\n  --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  --experiment_name EXPERIMENT_NAME\n                        experiment name\n  --model_name MODEL_NAME\n                        name for the model\n  --dataset DATASET     input data file path. If it has a split column, it\n                        will be used for splitting (0: train, 1: validation,\n                        2: test), otherwise the dataset will be randomly split\n  --training_set TRAINING_SET\n                        input train data file path\n  --validation_set VALIDATION_SET\n                        input validation data file path\n  --test_set TEST_SET   input test data file path\n  --training_set_metadata TRAINING_SET_METADATA\n                        input metadata JSON file path. An intermediate\n                        preprocessed file containing the mappings of the input\n                        file created the first time a file is used, in the\n                        same directory with the same name and a .json\n                        extension\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\nformat of the input data\n  -sspi, --skip_save_processed_input\n                        skips saving intermediate HDF5 and JSON files\n  -c CONFIG, --config CONFIG\n                        Path to the YAML file containing the model configuration\n  -cs CONFIG_STR, --config_str CONFIG_STRING\n                        JSON or YAML serialized string of the model configuration. Ignores --config\n  -mlp MODEL_LOAD_PATH, --model_load_path MODEL_LOAD_PATH\n                        path of a pretrained model to load as initialization\n  -mrp MODEL_RESUME_PATH, --model_resume_path MODEL_RESUME_PATH\n                        path of the model directory to resume training of\n  -sstd, --skip_save_training_description\n                        disables saving the description JSON file\n  -ssts, --skip_save_training_statistics\n                        disables saving training statistics JSON file\n  -ssm, --skip_save_model\n                        disables saving weights each time the model improves.\n                        By default Ludwig saves weights after each epoch the\n                        validation metric improves, but if the model is really\n                        big that can be time consuming. If you do not want to\n                        keep the weights and just find out what performance\n                        can a model get with a set of hyperparameters, use\n                        this parameter to skip it\n  -ssp, --skip_save_progress\n                        disables saving weights after each epoch. By default\n                        ludwig saves weights after each epoch for enabling\n                        resuming of training, but if the model is really big\n                        that can be time consuming and will save twice as much\n                        space, use this parameter to skip it\n  -ssl, --skip_save_log\n                        disables saving TensorBoard logs. By default Ludwig\n                        saves logs for the TensorBoard, but if it is not\n                        needed turning it off can slightly increase the\n                        overall speed\n  -rs RANDOM_SEED, --random_seed RANDOM_SEED\n                        a random seed that is going to be used anywhere there\n                        is a call to a random number generator: data\n                        splitting, parameter initialization and training set\nshuffling\n  -g GPUS [GPUS ...], --gpus GPUS [GPUS ...]\nlist of gpus to use\n  -gml GPU_MEMORY_LIMIT, --gpu_memory_limit GPU_MEMORY_LIMIT\n                        maximum memory in MB to allocate per GPU device\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\nthe level of logging to use\n</code></pre> <p>The parameters combine parameters from both train and test so refer to those sections for an in depth explanation. The output directory will contain a <code>hyperopt_statistics.json</code> file that summarizes the results obtained.</p> <p>In order to perform an hyperparameter optimization, the <code>hyperopt</code> section needs to be provided within the configuration. In the <code>hyperopt</code> section you will be able to define what metric to optimize, what parameters, what sampler to use to optimize them and how to execute the optimization. For details on the <code>hyperopt</code> section see the detailed description in the Hyperparameter Optimization section.</p>"},{"location":"user_guide/command_line_interface/#serve","title":"serve","text":"<p>This command lets you load a pre-trained model and serve it on an http server.</p> <p>You can call it with:</p> <pre><code>ludwig serve [options]\n</code></pre> <p>or with</p> <pre><code>python -m ludwig.serve [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig serve [options]\n\nThis script serves a pretrained model\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n  -p PORT, --port PORT  port for server (default: 8000)\n  -H HOST, --host HOST  host for server (default: 0.0.0.0)\n</code></pre> <p>The most important argument is <code>--model_path</code> where you have to specify the path of the model to load.</p> <p>Once running, you can make a POST request on the <code>/predict</code> endpoint to run inference on the form data submitted.</p> <p>Note</p> <p><code>ludwig serve</code> will automatically use GPUs for serving, if avaiable to the machine-local torch environment.</p>"},{"location":"user_guide/command_line_interface/#example-curl","title":"Example curl","text":"<p>File</p> <p><code>curl http://0.0.0.0:8000/predict -X POST -F 'image_path=@path_to_image/example.png'</code></p> <p>Text</p> <p><code>curl http://0.0.0.0:8000/predict -X POST -F 'english_text=words to be translated'</code></p> <p>Both Text and File</p> <p><code>curl http://0.0.0.0:8000/predict -X POST -F 'text=mixed together with' -F 'image=@path_to_image/example.png'</code></p> <p>Batch prediction</p> <p>You can also make a POST request on the <code>/batch_predict</code> endpoint to run inference on multiple samples at once.</p> <p>Requests must be submitted as form data, with one of fields being <code>dataset</code>: a JSON encoded string representation of the data to be predicted.</p> <p>The <code>dataset</code> JSON string is expected to be in the Pandas \"split\" format to reduce payload size. This format divides the dataset into three parts:</p> <ol> <li>columns: <code>List[str]</code></li> <li>index (optional): <code>List[Union[str, int]]</code></li> <li>data: <code>List[List[object]]</code></li> </ol> <p>Additional form fields can be used to provide file resources like images that are referenced within the dataset.</p> <p>Batch prediction example:</p> <p><code>curl http://0.0.0.0:8000/batch_predict -X POST -F 'dataset={\"columns\": [\"a\", \"b\"], \"data\": [[1, 2], [3, 4]]}'</code></p>"},{"location":"user_guide/command_line_interface/#visualize","title":"visualize","text":"<p>This command lets you visualize training and prediction statistics, alongside with comparing different models performances and predictions. You can call it with:</p> <pre><code>ludwig visualize [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.visualize [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig visualize [options]\n\nThis script analyzes results and shows some nice plots.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -g GROUND_TRUTH, --ground_truth GROUND_TRUTH\n                        ground truth file\n  -gm GROUND_TRUTH_METADATA, --ground_truth_metadata GROUND_TRUTH_METADATA\n                        input metadata JSON file\n  -od OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\n                        directory where to save plots.If not specified, plots\n                        will be displayed in a window\n  -ff {pdf,png}, --file_format {pdf,png}\n                        file format of output plots\n  -v {binary_threshold_vs_metric,calibration_1_vs_all,calibration_multiclass,compare_classifiers_multiclass_multimetric,compare_classifiers_performance_changing_k,compare_classifiers_performance_from_pred,compare_classifiers_performance_from_prob,compare_classifiers_performance_subset,compare_classifiers_predictions,compare_classifiers_predictions_distribution,compare_performance,confidence_thresholding,confidence_thresholding_2thresholds_2d,confidence_thresholding_2thresholds_3d,confidence_thresholding_data_vs_acc,confidence_thresholding_data_vs_acc_subset,confidence_thresholding_data_vs_acc_subset_per_class,confusion_matrix,frequency_vs_f1,hyperopt_hiplot,hyperopt_report,learning_curves,roc_curves,roc_curves_from_test_statistics}, --visualization {binary_threshold_vs_metric,calibration_1_vs_all,calibration_multiclass,compare_classifiers_multiclass_multimetric,compare_classifiers_performance_changing_k,compare_classifiers_performance_from_pred,compare_classifiers_performance_from_prob,compare_classifiers_performance_subset,compare_classifiers_predictions,compare_classifiers_predictions_distribution,compare_performance,confidence_thresholding,confidence_thresholding_2thresholds_2d,confidence_thresholding_2thresholds_3d,confidence_thresholding_data_vs_acc,confidence_thresholding_data_vs_acc_subset,confidence_thresholding_data_vs_acc_subset_per_class,confusion_matrix,frequency_vs_f1,hyperopt_hiplot,hyperopt_report,learning_curves,roc_curves,roc_curves_from_test_statistics}\n                        type of visualization\n  -f OUTPUT_FEATURE_NAME, --output_feature_name OUTPUT_FEATURE_NAME\n                        name of the output feature to visualize\n  -gts GROUND_TRUTH_SPLIT, --ground_truth_split GROUND_TRUTH_SPLIT\n                        ground truth split - 0:train, 1:validation, 2:test\n                        split\n  -tf THRESHOLD_OUTPUT_FEATURE_NAMES [THRESHOLD_OUTPUT_FEATURE_NAMES ...], --threshold_output_feature_names THRESHOLD_OUTPUT_FEATURE_NAMES [THRESHOLD_OUTPUT_FEATURE_NAMES ...]\n                        names of output features for 2d threshold\n  -pred PREDICTIONS [PREDICTIONS ...], --predictions PREDICTIONS [PREDICTIONS ...]\n                        predictions files\n  -prob PROBABILITIES [PROBABILITIES ...], --probabilities PROBABILITIES [PROBABILITIES ...]\n                        probabilities files\n  -trs TRAINING_STATISTICS [TRAINING_STATISTICS ...], --training_statistics TRAINING_STATISTICS [TRAINING_STATISTICS ...]\n                        training stats files\n  -tes TEST_STATISTICS [TEST_STATISTICS ...], --test_statistics TEST_STATISTICS [TEST_STATISTICS ...]\n                        test stats files\n  -hs HYPEROPT_STATS_PATH, --hyperopt_stats_path HYPEROPT_STATS_PATH\n                        hyperopt stats file\n  -mn MODEL_NAMES [MODEL_NAMES ...], --model_names MODEL_NAMES [MODEL_NAMES ...]\n                        names of the models to use as labels\n  -tn TOP_N_CLASSES [TOP_N_CLASSES ...], --top_n_classes TOP_N_CLASSES [TOP_N_CLASSES ...]\n                        number of classes to plot\n  -k TOP_K, --top_k TOP_K\n                        number of elements in the ranklist to consider\n  -ll LABELS_LIMIT, --labels_limit LABELS_LIMIT\n                        maximum numbers of labels. If labels in dataset are\n                        higher than this number, \"rare\" label\n  -ss {ground_truth,predictions}, --subset {ground_truth,predictions}\n                        type of subset filtering\n  -n, --normalize       normalize rows in confusion matrix\n  -m METRICS [METRICS ...], --metrics METRICS [METRICS ...]\n                        metrics to display in threshold_vs_metric\n  -pl POSITIVE_LABEL, --positive_label POSITIVE_LABEL\n                        label of the positive class for the roc curve\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>As the <code>--visualization</code> parameters suggests, there is a vast number of visualizations readily available. Each of them requires a different subset of this command's arguments, so they will be described one by one in the Visualizations section.</p>"},{"location":"user_guide/command_line_interface/#init_config","title":"init_config","text":"<p>Initialize a user config from a dataset and targets.</p> <pre><code>usage: ludwig init_config [options]\n\nThis script initializes a valid config from a dataset.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DATASET, --dataset DATASET\n                        input data file path\n  -t TARGET, --target TARGET\n                        target(s) to predict as output features of the model\n  --time_limit_s TIME_LIMIT_S\n                        time limit to train the model in seconds when using hyperopt\n  --suggested SUGGESTED\n                        use suggested config from automl, otherwise only use inferred types and return a minimal config\n  --hyperopt HYPEROPT   include automl hyperopt config\n  --random_seed RANDOM_SEED\n                        seed for random number generators used in hyperopt to improve repeatability\n  --use_reference_config USE_REFERENCE_CONFIG\n                        refine hyperopt search space by setting first search point from stored reference model config\n  -o OUTPUT, --output OUTPUT\n                        output initialized YAML config path\n</code></pre>"},{"location":"user_guide/command_line_interface/#render_config","title":"render_config","text":"<p>Renders the fully populated config with all defaults set.</p> <pre><code>usage: ludwig render_config [options]\n\nThis script renders the full config from a user config.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -o OUTPUT, --output OUTPUT\n                        output rendered YAML config path\n</code></pre>"},{"location":"user_guide/command_line_interface/#collect_summary","title":"collect_summary","text":"<p>This command loads a pretrained model and prints names of weights and layers activations to use with <code>collect_weights</code> or <code>collect_activations</code>.</p> <pre><code>ludwig collect_summary [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.collect names [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig collect_summary [options]\n\nThis script loads a pretrained model and print names of weights and layer activations.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre>"},{"location":"user_guide/command_line_interface/#collect_weights","title":"collect_weights","text":"<p>This command lets you load a pre-trained model and collect the tensors with a specific name in order to save them in a NPY format. This may be useful in order to visualize the learned weights (for instance collecting embedding matrices) and for some post-hoc analyses. You can call it with:</p> <pre><code>ludwig collect_weights [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.collect weights [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig collect_weights [options]\n\nThis script loads a pretrained model and uses it collect weights.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -t TENSORS [TENSORS ...], --tensors TENSORS [TENSORS ...]\n                        tensors to collect\n  -od OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>The three most important arguments are <code>--model_path</code> where you have to specify the path of the model to load, <code>--tensors</code> that lets you specify a list of tensor names in the Torch graph that contain the weights you want to collect, and finally <code>--output_directory</code> that lets you specify where the NPY files (one for each tensor name specified) will be saved.</p> <p>In order to figure out the names of the tensors containing the weights you want to collect, use the <code>collect_summary</code> command.</p>"},{"location":"user_guide/command_line_interface/#collect_activations","title":"collect_activations","text":"<p>This command lets you load a pre-trained model and input data and collects the values of activations contained in tensors with a specific name in order to save them in a NPY format.</p> <p>This may be useful in order to visualize the activations (for instance collecting the last layer's activations as embeddings representations of the input datapoint) and for some post-hoc analyses.</p> <p>You can call it with:</p> <pre><code>ludwig collect_activations [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.collect activations [options]\n</code></pre> <p>from within Ludwig's main directory.</p> <p>These are the available arguments:</p> <pre><code>usage: ludwig collect_activations [options]\n\nThis script loads a pretrained model and uses it collect tensors for each\ndatapoint in the dataset.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dataset  DATASET    filepath for input dataset\n  --data_format DATA_FORMAT  format of the dataset.  Valid values are auto,\n                        csv, excel, feature, fwf, hdf5, html, tables, json,\n                        json, jsonl, parquet, pickle, sas, spss, stata, tsv\n  -s {training,validation,test,full}, --split {training,validation,test,full}\n                        the split to test the model on\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -lyr LAYER [LAYER ..], --layers LAYER [LAYER ..]\n                        layers to collect\n  -od OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\n                        directory that contains the results\n  -bs BATCH_SIZE, --batch_size BATCH_SIZE\n                        size of batches\n  -g GPUS, --gpus GPUS  list of gpu to use\n  -gml GPU_MEMORY, --gpu_memory_limit GPU_MEMORY\n                        maximum memory in MB of gpu memory to allocate per\n                        GPU device\n  -dpt, --disable_parallel_threads\n                        disable Torch from using multithreading\n                        for reproducibility\n  -b BACKEND, --backend BACKEND\n                        specifies backend to use for parallel / distributed execution,\n                        defaults to local execution or Horovod if called using horovodrun\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>The data related and runtime related arguments (GPUs, batch size, etc.) are the same as the ones used in predict, you can refer to that section for an explanation.</p> <p>The collect-specific arguments, <code>--model_path</code>, <code>--tensors</code> and <code>--output_directory</code>, are the same used in collect_weights, you can refer to that section for an explanation.</p>"},{"location":"user_guide/command_line_interface/#export_torchscript","title":"export_torchscript","text":"<p>Exports a pre-trained model to Torch's <code>torchscript</code> format.</p> <pre><code>ludwig export_torchscript [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.export torchscript [options]\n</code></pre> <p>These are the available arguments:</p> <pre><code>usage: ludwig export_torchscript [options]\n\nThis script loads a pretrained model and saves it as torchscript.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -mo, --model_only     Script and export the model only.\n  -d DEVICE, --device DEVICE\n                        Device to use for torchscript tracing (e.g. \"cuda\" or \"cpu\"). Ideally, this is the same as the device used\n                        when the model is loaded.\n  -op OUTPUT_PATH, --output_path OUTPUT_PATH\n                        path where to save the export model. If not specified, defaults to model_path.\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>For more information, see TorchScript Export</p>"},{"location":"user_guide/command_line_interface/#export_neuropod","title":"export_neuropod","text":"<p>A Ludwig model can be exported as a Neuropod, a mechanism that allows it to be executed in a framework agnostic way.</p> <p>In order to export a Ludwig model as a Neuropod, first make sure the <code>neuropod</code> package is installed in your environment together with the appropriate backend (only use Python 3.7+), then run the following command:</p> <pre><code>ludwig export_neuropod [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.export neuropod [options]\n</code></pre> <p>These are the available arguments:</p> <pre><code>usage: ludwig export_neuropod [options]\n\nThis script loads a pretrained model and uses it collect weights.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        model to load\n  -mn MODEL_NAME, --model_name MODEL_NAME\n                        model name\n  -od OUTPUT_PATH, --output_path OUTPUT_PATH\n                        path where to save the export model\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre> <p>This functionality has been tested with <code>neuropod==0.2.0</code>.</p>"},{"location":"user_guide/command_line_interface/#export_mlflow","title":"export_mlflow","text":"<p>A Ludwig model can be exported as an mlflow.pyfunc model, which allows it to be executed in a framework agnostic way.</p> <p>There are two ways to export a Ludwig model to MLflow:</p> <ol> <li>Convert a saved model directory on disk to the MLflow format on disk.</li> <li>Register a saved model directory on disk or in an existing MLflow experiment to an MLflow model registry.</li> </ol> <p>For the first approach, you only need to provide the location of the saved Ludwig model locally and the location where the model should be written to on local disk:</p> <pre><code>ludwig export_mlflow --model_path /saved/ludwig/model --output_path /exported/mlflow/model\n</code></pre> <p>For the second, you will need to provide a registered model name used by the model registry:</p> <pre><code>ludwig export_mlflow --model_path /saved/ludwig/model --output_path relative/model/path --registered_model_name my_ludwig_model\n</code></pre>"},{"location":"user_guide/command_line_interface/#preprocess","title":"preprocess","text":"<p>Preprocess data and saves it into HDF5 and JSON format. The preprocessed files can be then used for performing training, prediction and evaluation. The advantage is that, being the data already preprocessed, if multiple models have to be trained on the same data, the preprocessed files act as a cache to avoid performing preprocessing multiple times.</p> <pre><code>ludwig preprocess [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.preprocess [options]\n</code></pre> <p>These are the available arguments:</p> <pre><code>usage: ludwig preprocess [options]\n\nThis script preprocess a dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --dataset DATASET     input data file path. If it has a split column, it\n                        will be used for splitting (0: train, 1: validation,\n                        2: test), otherwise the dataset will be randomly split\n  --training_set TRAINING_SET\n                        input train data file path\n  --validation_set VALIDATION_SET\n                        input validation data file path\n  --test_set TEST_SET   input test data file path\n  --training_set_metadata TRAINING_SET_METADATA\n                        input metadata JSON file path. An intermediate\n                        preprocessed  containing the mappings of the input\n                        file created the first time a file is used, in the\n                        same directory with the same name and a .json\n                        extension\n  --data_format {auto,csv,excel,feather,fwf,hdf5,htmltables,json,jsonl,parquet,pickle,sas,spss,stata,tsv}\n                        format of the input data\n  -pc PREPROCESSING_CONFIG, --preprocessing_config PREPROCESSING_CONFIG\n                        preprocessing config. Uses the same format of config,\n                        but ignores encoder specific parameters, decoder\n                        specific parameters, combiner and training parameters\n  -pcf PREPROCESSING_CONFIG_FILE, --preprocessing_config_file PREPROCESSING_CONFIG_FILE\n                        YAML file describing the preprocessing. Ignores\n                        --preprocessing_config.Uses the same format of config,\n                        but ignores encoder specific parameters, decoder\n                        specific parameters, combiner and training parameters\n  -rs RANDOM_SEED, --random_seed RANDOM_SEED\n                        a random seed that is going to be used anywhere there\n                        is a call to a random number generator: data\n                        splitting, parameter initialization and training set\n                        shuffling\n  -dbg, --debug         enables debugging mode\n  -l {critical,error,warning,info,debug,notset}, --logging_level {critical,error,warning,info,debug,notset}\n                        the level of logging to use\n</code></pre>"},{"location":"user_guide/command_line_interface/#synthesize_dataset","title":"synthesize_dataset","text":"<p>Creates synthetic data for testing purposes depending on the feature list parameters provided in YAML format.</p> <pre><code>ludwig synthesize_dataset [options]\n</code></pre> <p>or with:</p> <pre><code>python -m ludwig.data.dataset_synthesizer [options]\n</code></pre> <p>These are the available arguments:</p> <pre><code>usage: ludwig synthesize_dataset [options]\n\nThis script generates a synthetic dataset.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -od OUTPUT_PATH, --output_path OUTPUT_PATH\n                        output CSV file path\n  -d DATASET_SIZE, --dataset_size DATASET_SIZE\n                        size of the dataset\n  -f FEATURES, --features FEATURES\n                        list of features to generate in YAML format. Provide a\n                        list containing one dictionary for each feature, each\n                        dictionary must include a name, a type and can include\n                        some generation parameters depending on the type\n\nProcess finished with exit code 0\n</code></pre> <p>Example:</p> <pre><code>ludwig synthesize_dataset --features=\"[ \\\n  {name: text, type: text}, \\\n  {name: category, type: category}, \\\n  {name: number, type: number}, \\\n  {name: binary, type: binary}, \\\n  {name: set, type: set}, \\\n  {name: bag, type: bag}, \\\n  {name: sequence, type: sequence}, \\\n  {name: timeseries, type: timeseries}, \\\n  {name: date, type: date}, \\\n  {name: h3, type: h3}, \\\n  {name: vector, type: vector}, \\\n  {name: image, type: image} \\\n]\" --dataset_size=10 --output_path=synthetic_dataset.csv\n</code></pre> <p>The available parameters depend on the feature type.</p> <p>binary</p> <ul> <li><code>prob</code> (float, default: <code>0.5</code>): probability of generating <code>true</code>.</li> <li><code>cycle</code> (boolean, default: <code>false</code>): cycle through values instead of sampling.</li> </ul> <p>number</p> <ul> <li><code>min</code> (float, default: <code>0</code>): minimum value of the range of values to generate.</li> <li><code>max</code> (float, default: <code>1</code>): maximum value of the range of values to generate.</li> </ul> <p>category</p> <ul> <li><code>vocab_size</code> (int, default: <code>10</code>): size of the vocabulary to sample from.</li> <li><code>cycle</code> (boolean, default: <code>false</code>): cycle through values instead of sampling.</li> </ul> <p>sequence</p> <ul> <li><code>vocab_size</code> (int, default: <code>10</code>): size of the vocabulary to sample from.</li> <li><code>max_len</code> (int, default: <code>10</code>): maximum length of the generated sequence.</li> <li><code>min_len</code> (int, default: <code>null</code>): if <code>null</code> all sequences will be of size <code>max_len</code>. If a value is provided, the length will be randomly determined between <code>min_len</code> and <code>max_len</code>.</li> </ul> <p>set</p> <ul> <li><code>vocab_size</code> (int, default: <code>10</code>): size of the vocabulary to sample from.</li> <li><code>max_len</code> (int, default: <code>10</code>): maximum length of the generated set.</li> </ul> <p>bag</p> <ul> <li><code>vocab_size</code> (int, default: <code>10</code>): size of the vocabulary to sample from.</li> <li><code>max_len</code> (int, default: <code>10</code>): maximum length of the generated set.</li> </ul> <p>text</p> <ul> <li><code>vocab_size</code> (int, default: <code>10</code>): size of the vocabulary to sample from.</li> <li><code>max_len</code> (int, default: <code>10</code>): maximum length of the generated sequence, lengths will be randomly sampled between <code>max_len - 20%</code> and <code>max_len</code>.</li> </ul> <p>timeseries</p> <ul> <li><code>max_len</code> (int, default: <code>10</code>): maximum length of the generated sequence.</li> <li><code>min</code> (float, default: <code>0</code>): minimum value of the range of values to generate.</li> <li><code>max</code> (float, default: <code>1</code>): maximum value of the range of values to generate.</li> </ul> <p>audio</p> <ul> <li><code>destination_folder</code> (str): folder where the generated audio files will be saved.</li> <li><code>preprocessing: {audio_file_length_limit_in_s}</code> (int, default: <code>1</code>): length of the generated audio in seconds.</li> </ul> <p>image</p> <ul> <li><code>destination_folder</code> (str): folder where the generated image files will be saved.</li> <li><code>preprocessing: {height}</code> (int, default: <code>28</code>): height of the generated image in pixels.</li> <li><code>preprocessing: {width}</code> (int, default: <code>28</code>): width of the generated image in pixels.</li> <li><code>preprocessing: {num_channels}</code> (int, default: <code>1</code>): number of channels of the generated images. Valid values are <code>1</code>, <code>3</code>, <code>4</code>.</li> <li><code>preprocessing: {infer_image_dimensions}</code> (boolean, default: <code>true</code>): whether to transform differently-sized images to the same width/height dimensions. Target dimensions are inferred by taking the average dimensions of the first <code>infer_image_sample_size</code> images, then applying <code>infer_image_max_height</code> and <code>infer_image_max_width</code>. This parameter has no effect if explicit <code>width</code> and <code>height</code> are specified.</li> <li><code>preprocessing: {infer_image_sample_size}</code> (int, default <code>100</code>): sample size of <code>infer_image_dimensions</code>.</li> <li><code>preprocessing: {infer_image_max_height}</code> (int, default <code>256</code>): maximum height of an image transformed using <code>infer_image_dimensions</code>.</li> <li><code>preprocessing: {infer_image_max_width}</code> (int, default <code>256</code>): maximum width of an image transformed using <code>infer_image_dimensions</code>.</li> </ul> <p>date</p> <p>No parameters.</p> <p>h3</p> <p>No parameters.</p> <p>vector</p> <ul> <li><code>vector_size</code> (int, default: <code>10</code>): size of the vectors to generate.</li> </ul>"},{"location":"user_guide/gpus/","title":"GPUs","text":"<p>Ludwig will automatically detect and run on GPU hardware when available. To run on multiple GPUs or a cluster of GPU machines, see Distributed Training.</p> <p>GPUs will dramatically improve the speed of training using the default ECD architecture. Some features types, nmely text and images, de facto require GPUs to train in a reasonable amount of time. That said, if you are only using tabular features (binary, category, number) with the default concat combiner, or are training a GBM model, you may not notice a difference without GPUs, so the utility will vary by use case.</p>"},{"location":"user_guide/gpus/#running-on-apple-metal","title":"Running on Apple Metal","text":"<p>Ludwig supports experimental support for Apple Metal GPUs with the Metal Performance Shaders (MPS) library. To try it out, set the <code>LUDWIG_ENABLE_MPS</code> environment variable when training:</p> <pre><code>LUDWIG_ENABLE_MPS=1 PYTORCH_ENABLE_MPS_FALLBACK=1 ludwig train ...\n</code></pre> <p>It is also recommended to set <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code> as well, as not all operations used by Ludwig are supported by MPS.</p> <p>Empircally, we've observed significant speedups using MPS when training on larger text and image models. However, we also observed performance decreases using smaller tabular models, which is why we do not enable this feature by default. We recommend trying with and without to see which gives the best performance for your use case.</p>"},{"location":"user_guide/gpus/#tips","title":"Tips","text":""},{"location":"user_guide/gpus/#avoiding-cuda-ooms","title":"Avoiding CUDA OOMs","text":"<p>GPUs can easily run out of memory when training on large models. To avoid these types of errors, we recommend trying the following in order:</p> <ul> <li>Setting <code>trainer.batch_size=auto</code> in the config (this is the current default).</li> <li>Manually setting <code>trainer.max_batch_size</code>.</li> <li>Trying a smaller model architecture.</li> <li>Splitting the model across multiple GPUs the Fully Sharded Data Parallel strategy.</li> </ul>"},{"location":"user_guide/gpus/#disabling-gpus","title":"Disabling GPUs","text":"<p>You can disable GPU acceleration during training by setting <code>CUDA_VISIBLE_DEVICES=\"\"</code> in the environment. This can be useful for debugging runtime failures specific to your GPU hardware.</p>"},{"location":"user_guide/how_ludwig_works/","title":"How Ludwig Works","text":""},{"location":"user_guide/how_ludwig_works/#configuration","title":"Configuration","text":"<p>Ludwig provides an expressive declarative configuration system for how users construct their ML pipeline, like data preprocessing, model architecting, backend infrastructure, the training loop, hyperparameter optimization, and more.</p> <pre><code>input_features:\n-\nname: title\ntype: text\nencoder: type: rnn\ncell: lstm\nnum_layers: 2\nstate_size: 128\npreprocessing:\ntokenizer: space_punct\n-\nname: author\ntype: category\nencoder: embedding_size: 128\npreprocessing:\nmost_common: 10000\n-\nname: description\ntype: text\nencoder: type: bert\n-\nname: cover\ntype: image\nencoder: type: resnet\nnum_layers: 18\n\noutput_features:\n-\nname: genre\ntype: set\n-\nname: price\ntype: number\npreprocessing:\nnormalization: zscore\n\ntrainer:\nepochs: 50\nbatch_size: 256\noptimizer:\ntype: adam\nbeat1: 0.9\nlearning_rate: 0.001\n\nbackend:\ntype: local\ncache_format: parquet\n\nhyperopt:\nmetric: f1\nsampler: random\nparameters:\ntitle.encoder.num_layers:\nlower: 1\nupper: 5\ntraining.learning_rate:\nvalues: [0.01, 0.003, 0.001]\n</code></pre> <p>See Ludwig configurations for an in-depth reference.</p>"},{"location":"user_guide/how_ludwig_works/#data-type-abstractions","title":"Data type abstractions","text":"<p>Every feature in Ludwig is described by a specific data type. Each data type maps to a specific set of modules that handle preprocessing, encoding, decoding, and post-processing for that type. Vice versa, every module (preprocessor, encoder, decoder) is registered to a specific set of data types that the module supports.</p> <p>Binary Number Category Bag Set Sequence Text Vector Audio Date H3 Image Timeseries</p> <p>Read more about Ludwig's supported feature types.</p>"},{"location":"user_guide/how_ludwig_works/#ecd-architecture","title":"ECD Architecture","text":"<p>Ludwig\u2019s core modeling architecture is referred to as ECD (encoder-combiner-decoder). Multiple input features are encoded and fed through the Combiner model that operates on encoded inputs to combine them. On the output side, the combiner model's outputs are fed to decoders for each output feature for predictions and post-processing. Find out more about Ludwig's Combiner models like TabNet, Transformer, and Concat (Wide and Deep learning).</p> <p>Visualized, the ECD architecture looks like a butterfly and sometimes we refer to it as the \u201cbutterfly architecture\u201d.</p> <p></p> <p>ECD flexibly handles many different combinations of input and output data types, making the tool well-suited for many different applications.</p> <p></p> <p>Take a look at Examples to see how you can use Ludwig for several many different applications.</p>"},{"location":"user_guide/how_ludwig_works/#distributed-training-data-processing-and-hyperparameter-search-with-ray","title":"Distributed training, data processing, and hyperparameter search with Ray","text":"<p>Ludwig on Ray is a new backend introduced in v0.4 that enables users can scale their training process from running on their local laptop, to running in the cloud on a GPU instance, to scaling across hundreds of machines in parallel, all without changing a single line of code.</p> <p></p> <p>By integrating with Ray, Ludwig is able to provide a unified way for doing distributed training:</p> <ul> <li>Ray enables you to provision a cluster of machines in a single command through its cluster launcher.</li> <li>Horovod on Ray enables you to do distributed training without needing to configure MPI in your environment.</li> <li>Dask on Ray enables you to process large datasets that don\u2019t fit in memory on a single machine.</li> <li>Ray Tune enables you to easily run distributed hyperparameter search across many machines in parallel.</li> </ul>"},{"location":"user_guide/hyperopt/","title":"Hyperparameter Optimization","text":"<p>Ludwig supports hyperparameter optimization using Ray Tune or a local executor.</p> <p>The hyperparameter optimization strategy is specified as part of the Ludwig configuration and run using the <code>ludwig hyperopt</code> command. Every parameter within the config can be tuned using hyperopt.</p>"},{"location":"user_guide/hyperopt/#hyperopt-configuration","title":"Hyperopt Configuration","text":"<p>Most parameters or nested parameters of a Ludwig configuration may be optimized, including <code>input_features</code>, <code>output_features</code>, <code>combiner</code>, <code>preprocessing</code>, <code>trainer</code> and <code>defaults</code>.  Supported types are <code>float</code>, <code>int</code> and <code>category</code>.</p> <p>To enable hyperparameter optimization, add the <code>hyperopt</code> dictionary at the top level of your config.yaml. The <code>hyperopt</code> section declares which parameters to optimize, the search strategy, and the optimization goal.</p> config.yaml<pre><code>hyperopt:\nparameters:\ntitle.num_filters:\nspace: choice\ncategories: [128, 256, 512]\ntraining.learning_rate:\nspace: loguniform\nlower: 0.0001\nupper: 0.1\ncombiner.num_fc_layers:\nspace: randint\nlower: 2\nupper: 6\ngoal: minimize\nmetric: loss\n</code></pre>"},{"location":"user_guide/hyperopt/#default-hyperopt-parameters","title":"Default Hyperopt Parameters","text":"<p>In addition to defining hyperopt parameters for individual input or output features (like the <code>title</code> feature in the example above), default parameters can be specified for entire feature types (for example, the encoder to use for all text features in your dataset). These parameters will follow the same convention as the defaults section of the Ludwig config. This is particularly helpful in cases where a dataset has a large number of features and you don't want to define parameters for each feature individually.</p> <p>Default hyperopt parameters are defined using the following keywords in order separated by the <code>.</code> delimiter:</p> <ul> <li><code>defaults</code>: The defaults keyword used to indicate a feature-level parameter</li> <li><code>feature_type</code>: Any input or output feature type that Ludwig supports. This can be one of text, numeric, category, etc. See the full list of support feature types here</li> <li><code>subsection</code>: One of <code>preprocessing</code>, <code>encoder</code>, <code>decoder</code> or <code>loss</code>, the 4 sections that can be modified via the Ludwig defaults section</li> <li><code>parameter</code>: A valid parameter belonging to the <code>subsection</code>. For e.g., <code>most_common</code> is a valid parameter for the <code>preprocessing</code> sub-section for <code>text</code> feature types</li> </ul> <p>For each hyperopt trial, a value will be sampled from the parameter space and applied to either input features (<code>preprocessing</code> or <code>encoder</code> related parameters) or output features (<code>decoder</code> or <code>loss</code> related parameters) of that feature type. Additionally, parameters defined for individual features (like <code>title.preprocessing.most_common</code>) will take precedence over default parameters (like <code>defaults.text.preprocessing.most_common</code>) if they share the same feature type and parameter and both parameters are defined in the Ludwig hyperopt config.</p> config.yaml<pre><code>...\nhyperopt:\nparameters:\ntitle.num_filters:\nspace: choice\ncategories: [128, 256, 512]\ndefaults.text.preprocessing.most_common:\nspace: choice\ncategories: [100, 500, 1000]\ngoal: minimize\nmetric: loss\n...\n</code></pre> <p>In this example, <code>defaults.text.preprocessing.most_common</code> is a default parameter. Here:</p> <ul> <li><code>defaults</code> helps denote a default hyperopt parameter</li> <li><code>text</code> refers to the group of text input features since it is a <code>preprocessing</code> releated parameter</li> <li><code>preprocessing</code> refers to the text preprocessing sub-section within Ludwig's default section. This means that this parameter will modify preprocessing for all text input features</li> <li><code>most_common</code> is the parameter within <code>preprocessing</code> that we want to modify for all text input features</li> </ul>"},{"location":"user_guide/hyperopt/#nested-ludwig-config-parameters","title":"Nested Ludwig Config Parameters","text":"<p>Ludwig also extends the range of hyperopt parameters to support parameter choices that consist of partial or complete blocks of Ludwig config sections. This allows users to search over a set of Ludwig configs, as opposed to needing to specify config params individually and search over all combinations of parameters.</p> <p>To provide a parameter that represents a subsection of the Ludwig config, the <code>.</code> key name can be used.</p> <p>For example, one can define a hyperopt search space like the one below and sample partial Ludwig configs:</p> <pre><code>hyperopt:\nparameters:\n.:  space: choice\ncategories: -   combiner: # Ludwig config subsection 1\ntype: tabnet\ntrainer:\nlearning_rate: 0.001\nbatch_size: 64\n-   combiner: # Ludwig config subsection 2\ntype: concat\ntrainer:\nbatch_size: 256\ntrainer.decay_rate:\nspace: loguniform\nlower: 0.001\nupper: 0.1\n</code></pre> <p>The <code>.</code> parameter defines the nested hyperopt parameter with two choices. These will be sampled and used to update the Ludwig config for each trial based on which of the two choices is picked.</p> <p>This config above will create hyperopt samples that look like the following:</p> <pre><code># Trial 1\ncombiner:\ntype: tabnet\ntrainer: learning_rate: 0.001\nbatch_size: 64\ndecay_rate: 0.02\n\n# Trial 2\ncombiner:\ntype: tabnet\ntrainer: learning_rate: 0.001\nbatch_size: 64\ndecay_rate: 0.001\n\n# Trial 3\ncombiner:\ntype: concat\ntrainer: batch_size: 64\ndecay_rate: 0.001\n</code></pre>"},{"location":"user_guide/hyperopt/#running-hyperparameter-optimization","title":"Running Hyperparameter Optimization","text":"<p>Use the <code>ludwig hyperopt</code> command to run hyperparameter optimization.</p> <pre><code>ludwig hyperopt --dataset reuters-allcats.csv --config hyperopt_config.yaml\n</code></pre> <p>For a complete reference of hyperparameter search and execution options, see the Hyperopt page of the Ludwig configuration guide.</p>"},{"location":"user_guide/integrations/","title":"Third-Party Integrations","text":""},{"location":"user_guide/integrations/#using-integrations","title":"Using Integrations","text":"<p>Ludwig provides an extendable interface to integrate with third-party systems. To activate a particular integration, simply insert its flag into the command line. Each integration may have specific requirements and use cases.</p>"},{"location":"user_guide/integrations/#available-integrations","title":"Available integrations","text":""},{"location":"user_guide/integrations/#mlflow","title":"MLFlow","text":"<p><code>--mlflow</code> - logs training metrics, hyperopt parameters, output artifacts, and trained models to MLflow. Set the environment variable <code>MLFLOW_TRACKING_URI</code> to log results to a remote tracking server.</p>"},{"location":"user_guide/integrations/#weights-biases","title":"Weights &amp; Biases","text":"<p><code>--wandb</code> - logs training metrics, configuration parameters, environment details, and trained model to Weights &amp; Biases. For more details, refer to W&amp;B Quickstart.</p>"},{"location":"user_guide/integrations/#comet-ml","title":"Comet ML","text":"<p><code>--comet</code> - logs training metrics, environment details, test results, visualizations, and more to Comet.ML. Requires a free account. For more details, see Comet's Running Ludwig with Comet.</p>"},{"location":"user_guide/integrations/#aim","title":"Aim","text":"<p><code>--aim</code> - complete experimentation trackings with configuration, metadata, hyperparameters, losses and terminal logs. In order to see and end to end Aim-Ludwig training and tracking example please refer to our demo. For more details about Aim refer to the documentation.</p>"},{"location":"user_guide/integrations/#add-more-integrations","title":"Add more integrations","text":"<p>For more information about integration contributions, please see the Developer Guide.</p>"},{"location":"user_guide/model_export/","title":"Model Export","text":""},{"location":"user_guide/model_export/#exporting-ludwig-models","title":"Exporting Ludwig Models","text":"<p>There are a number of ways to export models in Ludwig.</p>"},{"location":"user_guide/model_export/#torchscript-export","title":"TorchScript Export","text":"<p>A subset of Ludwig Models can be exported to Torchscript end-to-end. This means that, in addition to the model itself, the preprocessing and postprocessing steps can be exported to TorchScript as well, ensuring that the model can be used for inference in a production environment out-of-the-box.</p> <p>To get started, simply run the <code>export_torchscript</code> command:</p> <pre><code>ludwig export_torchscript -m=results/experiment_run/model\n</code></pre> <p>As long as <code>--model_only</code> is not specified, then three files are output by this command. The most important are <code>inference_preprocessor.pt</code>, <code>inference_predictor_&lt;DEVICE&gt;.pt</code>, and <code>inference_postprocessor.pt</code>.</p> <p>The <code>inference_preprocessor.pt</code> file contains the preprocessor, which is a <code>torch.nn.Module</code> that takes in a dictionary of raw data and outputs a dictionary of tensors. The <code>inference_predictor_&lt;DEVICE&gt;.pt</code> file contains the predictor, which is a <code>torch.nn.Module</code> that takes in a dictionary of tensors and outputs a dictionary of tensors. The <code>inference_postprocessor.pt</code> file contains the postprocessor, which is a <code>torch.nn.Module</code> that takes in a dictionary of tensors and outputs a dictionary of postprocessed data containing the same keys as the Ludwig <code>predict</code> command DataFrame output. These files can each be loaded separately (and on separate devices) to run inference in a staged manner. This can be particularly useful with a tool like NVIDIA Triton, which can be used to manage each stage of the pipeline independently.</p> <p>You can get started using these modules immediately by loading them with <code>torch.jit.load</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; preprocessor = torch.jit.load(\"model/inference_preprocessor.pt\")\n&gt;&gt;&gt; predictor = torch.jit.load(\"model/inference_predictor_cpu.pt\")\n&gt;&gt;&gt; postprocessor = torch.jit.load(\"model/inference_postprocessor.pt\")\n</code></pre> <p>Once you've done this, you can pass in raw data in the form of a dictionary and get back predictions in the form of a dictionary:</p> <pre><code>&gt;&gt;&gt; raw_data = {\n    'text_feature': ['This is a test.'],\n    'category_feature': ['cat1', 'cat2'],\n    'numerical_feature': [1.0, 2.0],\n    'vector_feature': [[1.0, 2.0], [3.0, 4.0]],\n}\n&gt;&gt;&gt; preprocessed_data = preprocessor(raw_data)\n&gt;&gt;&gt; predictions = predictor(preprocessed_data)\n&gt;&gt;&gt; postprocessed_data = postprocessor(predictions)\n&gt;&gt;&gt; postprocessed_data\n{\n    'probabilities': [[0.8, 0.2]],\n    'predictions': ['class1', 'class2'],\n}\n</code></pre> <p>If you have your data stored in a DataFrame, you can use the <code>to_inference_module_input_from_dataframe</code> function to convert it to the correct format:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from ludwig.utils.inference_utils import to_inference_module_input_from_dataframe\n\n&gt;&gt;&gt; df = pd.read_csv('test.csv')\n&gt;&gt;&gt; raw_data = to_inference_module_input_from_dataframe(df)\n&gt;&gt;&gt; raw_data\n{\n    'text_feature': ['This is a test.'],\n    'category_feature': ['cat1', 'cat2'],\n    'numerical_feature': [1.0, 2.0],\n    'vector_feature': [[1.0, 2.0], [3.0, 4.0]],\n}\n</code></pre>"},{"location":"user_guide/model_export/#using-the-inferencemodule-class","title":"Using the InferenceModule class","text":"<p>For convenience, we've provided a wrapper class called <code>InferenceModule</code> that can be used to load and use the exported model. The <code>InferenceModule</code> class is a subclass of <code>torch.nn.Module</code> and can be used in the same way. The <code>InferenceModule</code> class can be used to load the preprocessor, predictor, and postprocessor in a single step:</p> <pre><code>&gt;&gt;&gt; from ludwig.models.inference import InferenceModule\n&gt;&gt;&gt; inference_module = InferenceModule.from_directory('model/')\n&gt;&gt;&gt; raw_data = {...}\n&gt;&gt;&gt; preprocessed_data = inference_module.preprocessor_forward(raw_data)\n&gt;&gt;&gt; predictions = inference_module.predictor_forward(preprocessed_data)\n&gt;&gt;&gt; postprocessed_data = inference_module.postprocessor_forward(predictions)\n</code></pre> <p>You can also call the preprocessor, predictor, and postprocessor separately:</p> <pre><code>&gt;&gt;&gt; inference_module.preprocessor_forward(raw_data)\n{...}\n</code></pre> <p>You can also use the <code>InferenceModule.predict</code> method to input a DataFrame and output a DataFrame, similar to how you would with the <code>LudwigModel.predict</code> method:</p> <pre><code>&gt;&gt;&gt; input_df = pd.read_csv('test.csv')\n&gt;&gt;&gt; output_df = inference_module.predict(input_df)\n</code></pre> <p>Finally, you can convert the <code>InferenceModule</code> to TorchScript if you need a monolithic artifact for inference. Note that you can still call the <code>InferenceModule.preprocessor_forward</code>, <code>InferenceModule.predictor_forward</code>, and <code>InferenceModule.postprocessor_forward</code> methods, but <code>InferenceModule.predict</code> will no longer work after this conversion.</p> <pre><code>&gt;&gt;&gt; scripted_module = torch.jit.script(inference_module)\n&gt;&gt;&gt; raw_data = {...}\n&gt;&gt;&gt; scripted_module(raw_data)\n{...}\n&gt;&gt;&gt; input_df = pd.read_csv('test.csv')\n&gt;&gt;&gt; scripted_module.predict(input_df)  # Will fail\n</code></pre>"},{"location":"user_guide/model_export/#current-limitations","title":"Current Limitations","text":"<p>TorchScript only implements a subset of Python libraries. This means that there are a few of Ludwig's preprocessing steps that are not supported.</p>"},{"location":"user_guide/model_export/#image-and-audio-features","title":"Image and Audio Features","text":"<p>If you are using the preprocessor module directly or using the <code>forward</code> method of the <code>InferenceModule</code> module, loading <code>image</code> and <code>audio</code> files from filepaths is not supported. Instead, one has to load the data as either a batched tensor or a List of tensors ahead of passing it in to the module.</p> <p>If you are using the <code>predict</code> method of the <code>InferenceModule</code> module, then filepath strings will be loaded for you.</p>"},{"location":"user_guide/model_export/#date-features","title":"Date Features","text":"<p>TorchScript does not implement the <code>datetime</code> module, which means that <code>date</code> features cannot be parsed from string.</p> <p>If you are using the preprocessor module directly or using the <code>forward</code> method of the <code>InferenceModule</code> module, a vector of integers representing the date must be passed in. Given a <code>datetime_obj</code>, the following code can be used to generate the vector:</p> <pre><code>def create_vector_from_datetime_obj(datetime_obj):\n    yearday = datetime_obj.toordinal() - date(datetime_obj.year, 1, 1).toordinal() + 1\n\n    midnight = datetime_obj.replace(hour=0, minute=0, second=0, microsecond=0)\n    second_of_day = (datetime_obj - midnight).seconds\n\n    return [\n        datetime_obj.year,\n        datetime_obj.month,\n        datetime_obj.day,\n        datetime_obj.weekday(),\n        yearday,\n        datetime_obj.hour,\n        datetime_obj.minute,\n        datetime_obj.second,\n        second_of_day,\n    ]\n</code></pre> <p>This function is also available in the <code>ludwig.utils.date_utils</code> module for convenience.</p> <p>If you are using the <code>predict</code> method of the <code>InferenceModule</code> module, then date strings will be featurized for you.</p>"},{"location":"user_guide/model_export/#gpu-support","title":"GPU Support","text":"<p>Input features fed in as strings can only be preprocessed on CPU. In Ludwig, these would be the following features:</p> <ul> <li><code>binary</code></li> <li><code>category</code></li> <li><code>set</code></li> <li><code>sequence</code></li> <li><code>text</code></li> <li><code>vector</code></li> <li><code>bag</code></li> <li><code>timeseries</code></li> </ul> <p>If you are using the preprocessor module directly or using the <code>forward</code> method of the <code>InferenceModule</code> module, then the user can pass in <code>binary</code>, <code>vector</code>, and <code>time series</code> features as tensors to preprocess them on the GPU.</p> <p>If you are using the <code>predict</code> method of the <code>InferenceModule</code> module, then all of the above features will be cast first to string and then preprocessed on CPU.</p>"},{"location":"user_guide/model_export/#nan-handling","title":"NaN Handling","text":"<p>For the majority of features, NaNs are handled by the preprocessor in the same way that they are handled by the original Ludwig Model. The exceptions are the following features:</p> <ul> <li><code>image</code></li> <li><code>audio</code></li> <li><code>date</code></li> <li><code>vector</code></li> </ul>"},{"location":"user_guide/model_export/#huggingface-models","title":"HuggingFace Models","text":"<p>HuggingFace models are not yet supported for TorchScript export, though we are working on it!</p>"},{"location":"user_guide/model_export/#triton-export","title":"Triton Export","text":"<p>Coming soon...</p>"},{"location":"user_guide/model_export/#neuropod-export","title":"Neuropod Export","text":"<p>Coming soon...</p>"},{"location":"user_guide/model_export/#mlflow-export","title":"MLFlow Export","text":""},{"location":"user_guide/serving/","title":"Serving","text":""},{"location":"user_guide/serving/#serving-ludwig-models","title":"Serving Ludwig Models","text":"<p>Ludwig models can be served using the serve command.</p> <pre><code>ludwig serve --model_path=/path/to/model\n</code></pre> <p>The command will spawn a Rest API using the FastAPI library.</p> <p>This API has two endpoints: <code>predict</code> and <code>predict_batch</code>. <code>predict</code> should be used to obtain predictions for individual examples, while <code>predict_batch</code> should be used to obtain predictions for an a batch of examples.</p> <p>Inputs sent to the REST API should be consistent with the feature names and types used to train the model. The output structure from the REST API depends on the model's output features and their data types.</p>"},{"location":"user_guide/serving/#rest-endpoints","title":"REST Endpoints","text":""},{"location":"user_guide/serving/#predict","title":"predict","text":""},{"location":"user_guide/serving/#input-format","title":"Input format","text":"<p>For each input of the model, the predict endpoint expects a field with a name. For instance, a model trained with an input text field named <code>english_text</code> would expect a POST like:</p> <pre><code>curl http://0.0.0.0:8000/predict -X POST -F 'english_text=words to be translated'\n</code></pre> <p>If the model was trained with an input image field, it will instead expects a POST with a file, like:</p> <pre><code>curl http://0.0.0.0:8000/predict -X POST -F 'image=@path_to_image/example.png'\n</code></pre> <p>A model with both a text and an image field will expect a POST like:</p> <pre><code>curl http://0.0.0.0:8000/predict -X POST -F 'text=mixed together with' -F 'image=@path_to_image/example.png'\n</code></pre>"},{"location":"user_guide/serving/#output-format","title":"Output format","text":"<p>The response is a JSON dictionary with keys prefixed by the names of the model's output features.</p> <p>For binary outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"NAME_predictions\": false,\n\"NAME_probabilities_False\": 0.76,\n\"NAME_probabilities_True\": 0.24,\n\"NAME_probability\": 0.76\n}\n</code></pre> <p>For number outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\"NAME_predictions\": 0.381}\n</code></pre> <p>For categorical outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"NAME_predictions\": \"CLASSNAMEK\",\n\"NAME_probability\": 0.62,\n\"NAME_probabilities_CLASSNAME1\": 0.099,\n\"NAME_probabilities_CLASSNAME2\": 0.095,\n...\n\"NAME_probabilities_CLASSNAMEN\": 0.077\n}\n</code></pre> <p>For set outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"NAME_predictions\":[\n\"CLASSNAMEI\",\n\"CLASSNAMEJ\",\n\"CLASSNAMEK\"\n],\n\"NAME_probabilities_CLASSNAME1\":0.490,\n\"NAME_probabilities_CLASSNAME2\":0.245,\n...\n\"NAME_probabilities_CLASSNAMEN\":0.341,\n\"NAME_probability\":[\n0.53,\n0.62,\n0.95\n]\n}\n</code></pre> <p>For sequence outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"NAME_predictions\":[\n\"TOKEN1\",\n\"TOKEN2\",\n\"TOKEN3\"\n],\n\"NAME_last_predictions\": \"TOKEN3\",\n\"NAME_probabilities\":[\n0.106,\n0.122,\n0.118,\n0.133\n],\n\"NAME_probability\": -6.4765729904174805\n}\n</code></pre> <p>For text outputs, the JSON structure returned by the REST API is the same as for sequences.</p>"},{"location":"user_guide/serving/#batch_predict","title":"batch_predict","text":""},{"location":"user_guide/serving/#input-format_1","title":"Input format","text":"<p>You can also make a POST request on the /batch_predict endpoint to run inference on multiple samples at once.</p> <p>Requests must be submitted as form data, with one of fields being <code>dataset</code>: a JSON encoded string representation of the data to be predicted.</p> <p>The dataset JSON string is expected to be in the Pandas <code>split</code> format to reduce payload size. This format divides the dataset into three parts:</p> <ul> <li><code>columns</code>: <code>List[str]</code></li> <li><code>index</code> (optional): <code>List[Union[str, int]]</code></li> <li><code>data</code>: <code>List[List[object]]</code></li> </ul> <p>Additional form fields can be used to provide file resources like images that are referenced within the dataset.</p> <p>An example of batch prediction:</p> <pre><code>curl http://0.0.0.0:8000/batch_predict -X POST -F 'dataset={\"columns\": [\"a\", \"b\"], \"data\": [[1, 2], [3, 4]]}'\n</code></pre>"},{"location":"user_guide/serving/#output-format_1","title":"Output format","text":"<p>The response is a JSON dictionary with keys prefixed by the names of the model's output features.</p> <p>For binary outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"index\": [0, 1],\n\"columns\": [\n\"NAME_predictions\",\n\"NAME_probabilities_False\",\n\"NAME_probabilities_True\",\n\"NAME_probability\"\n],\n\"data\": [\n[false, 0.768, 0.231, 0.768],\n[true, 0.372, 0.627, 0.627]\n]\n}\n</code></pre> <p>For number outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\"index\":[0, 1],\"columns\":[\"NAME_predictions\"],\"data\":[[0.381],[0.202]]}\n</code></pre> <p>For categorical outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"index\": [0, 1],\n\"columns\": [\n\"NAME_predictions\",\n\"NAME_probabilities_CLASSNAME1\",\n\"NAME_probabilities_CLASSNAME2\",\n...\n\"NAME_probabilities_CLASSNAMEN\",\n\"NAME_probability\"\n],\n\"data\": [\n[\"CLASSNAMEK\", 0.099, 0.095, ... 0.077, 0.623],\n[\"CLASSNAMEK\", 0.092, 0.061, ... 0.084, 0.541]\n]\n}\n</code></pre> <p>For set outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"index\": [0, 1],\n\"columns\": [\n\"NAME_predictions\",\n\"NAME_probabilities_CLASSNAME1\",\n\"NAME_probabilities_CLASSNAME2\",\n...\n\"NAME_probabilities_CLASSNAMEK\",\n\"NAME_probability\"\n],\n\"data\": [\n[\n[\"CLASSNAMEI\", \"CLASSNAMEJ\", \"CLASSNAMEK\"],\n0.490,\n0.453,\n...\n0.500,\n[0.53, 0.62, 0.95]\n],\n[\n[\"CLASSNAMEM\", \"CLASSNAMEN\", \"CLASSNAMEO\"],\n0.481,\n0.466,\n...\n0.485,\n[0.63, 0.72, 0.81]\n]\n]\n}\n</code></pre> <p>For sequence outputs, the JSON structure returned by the REST API is the following:</p> <pre><code>{\n\"index\": [0, 1],\n\"columns\": [\n\"NAME_predictions\",\n\"NAME_last_predictions\",\n\"NAME_probabilities\",\n\"NAME_probability\"\n],\n\"data\": [\n[\n[\"TOKEN1\", \"TOKEN1\", \"TOKEN1\"],\n\"TOKEN3\",\n[0.106, 0.122, \u2026 0.083],\n-6.476\n],\n[\n[\"TOKEN4\", \"TOKEN5\", \"TOKEN6\"],\n\"TOKEN6\",\n[0.108, 0.127, \u2026 0.083],\n-6.482\n]\n]\n}\n</code></pre> <p>For text outputs, the JSON structure returned by the REST API is the same as for sequences.</p>"},{"location":"user_guide/visualizations/","title":"Visualizations","text":""},{"location":"user_guide/visualizations/#visualize-command","title":"Visualize Command","text":"<p>Several visualizations can be obtained from the result files from both <code>train</code>, <code>predict</code> and <code>experiment</code> by using the <code>ludwig visualize</code> command. The command has several parameters, but not all the visualizations use all of them. Let's first present the parameters of the general script, and then, for each available visualization, we will discuss about the specific parameters needed and what visualization they produce.</p> <pre><code>usage: ludwig visualize [options]\n\nThis script analyzes results and shows some nice plots.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -g, --ground_truth GROUND_TRUTH\n                        ground truth file\n  -gm, --ground_truth_metadata GROUND_TRUTH_METADATA\n                        input metadata JSON file\n  -sf, --split_file SPLIT_FILE\n                        file containing split values used in conjunction with\n                        ground truth file.\n  -od, --output_directory OUTPUT_DIRECTORY\n                        directory where to save plots.  If not specified, plots\n                        will be displayed in a window\n  -ff, --file_format {pdf,png}\n                        file format of output plots\n  -v, --visualization {\n      binary_threshold_vs_metric,\n      calibration_1_vs_all,\n      calibration_multiclass,\n      compare_classifiers_multiclass_multimetric,\n      compare_classifiers_performance_changing_k,\n      compare_classifiers_performance_from_pred,\n      compare_classifiers_performance_from_prob\n      compare_classifiers_performance_subset,\n      compare_classifiers_predictions,\n      compare_classifiers_predictions_distribution,\n      compare_performance,confidence_thresholding,\n      confidence_thresholding_2thresholds_2d,\n      confidence_thresholding_2thresholds_3d,\n      confidence_thresholding_data_vs_acc,\n      confidence_thresholding_data_vs_acc_subset,\n      confidence_thresholding_data_vs_acc_subset_per_class,\n      confusion_matrix,\n      frequency_vs_f1,\n      hyperopt_hiplot,\n      hyperopt_report,\n      learning_curves,\n      roc_curves,\n      roc_curves_from_test_statistics\n  },\n                        The type of visualization to generate\n  -ofn, --output_feature_name OUTPUT_FEATURE_NAME\n                        name of the output feature to visualize\n  -gts, --ground_truth_split GROUND_TRUTH_SPLIT\n                        ground truth split - 0:train, 1:validation, 2:test split\n  -tf, --threshold_output_feature_names THRESHOLD_OUTPUT_FEATURE_NAMES [THRESHOLD_OUTPUT_FEATURE_NAMES ...]\n                        names of output features for 2d threshold\n  -pred, --predictions PREDICTIONS [PREDICTIONS ...]\n                        predictions files\n  -prob, --probabilities PROBABILITIES [PROBABILITIES ...]\n                        probabilities files\n  -trs, --training_statistics TRAINING_STATISTICS [TRAINING_STATISTICS ...]\n                        training stats files\n  -tes, --test_statistics TEST_STATISTICS [TEST_STATISTICS ...]\n                        test stats files\n  -hs, --hyperopt_stats_path HYPEROPT_STATS_PATH\n                        hyperopt stats file\n  -mn, --model_names MODEL_NAMES [MODEL_NAMES ...]\n                        names of the models to use as labels\n  -tn, --top_n_classes TOP_N_CLASSES [TOP_N_CLASSES ...]\n                        number of classes to plot\n  -k, --top_k TOP_K\n                        number of elements in the ranklist to consider\n  -ll, --labels_limit LABELS_LIMIT\n                        maximum numbers of labels. Encoded numeric label values in\n                        dataset that are higher than labels_limit are considered to\n                        be \"rare\" labels\n  -ss, --subset {ground_truth,predictions}\n                        type of subset filtering\n  -n, --normalize       normalize rows in confusion matrix\n  -m, --metrics METRICS [METRICS ...]\n                        metrics to dispay in threshold_vs_metric\n  -pl, --positive_label POSITIVE_LABEL\n                        label of the positive class for the roc curve\n  -l, --logging_level {critical, error, warning, info, debug, notset}\n                        the level of logging to use\n</code></pre> <p>Some additional information on the parameters:</p> <ul> <li>The list parameters are all aligned.  In other words, <code>predictions</code>, <code>probabilities</code>, <code>training_statistics</code>, <code>test_statistics</code> and <code>model_names</code> are parallel arrays, and the nth entry of <code>model_names</code> should be the name of the model corresponding to the nth entry of <code>predictions</code>.</li> <li><code>ground_truth</code> and <code>ground_truth_metadata</code> are respectively the <code>HDF5</code> and <code>JSON</code> file obtained during training preprocessing. If you plan to use visualizations do not use <code>--skip_save_preprocessing</code> when training. Those files contain the train/test split performed at preprocessing time.</li> <li><code>output_feature_name</code> is the output feature to use for creating the visualization.</li> </ul> <p>Other parameters will be detailed for each visualization as different ones use them differently.</p>"},{"location":"user_guide/visualizations/#examples","title":"Examples","text":"<p>Example commands to generate the visualizations are based on running two experiments and comparing them. The experiments themselves are run with the following:</p> <pre><code>ludwig experiment --experiment_name titanic --model_name Model1 --dataset train.csv -cf titanic_model1.yaml\nludwig experiment --experiment_name titanic --model_name Model2 --dataset train.csv -cf titanic_model2.yaml\n</code></pre> <p>To run these examples, you need to download the Titanic Kaggle competition dataset to get <code>train.csv</code>.  Note that the example images associated with each visualization below were generated using a different dataset. The two models are defined with <code>titanic_model1.yaml</code></p> <pre><code>input_features:\n-\nname: Pclass\ntype: category\n-\nname: Sex\ntype: category\n-\nname: Age\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: SibSp\ntype: number\n-\nname: Parch\ntype: number\n-\nname: Fare\ntype: number\npreprocessing:\nmissing_value_strategy: fill_with_mean\n-\nname: Embarked\ntype: category\n\noutput_features:\n-\nname: Survived\ntype: binary\n</code></pre> <p>and with <code>titanic_model2.yaml</code>:</p> <pre><code>input_features:\n-\nname: Pclass\ntype: category\n-\nname: Sex\ntype: category\n-\nname: SibSp\ntype: number\n-\nname: Parch\ntype: number\n-\nname: Embarked\ntype: category\n\noutput_features:\n-\nname: Survived\ntype: binary\n</code></pre>"},{"location":"user_guide/visualizations/#learning-curves","title":"Learning Curves","text":""},{"location":"user_guide/visualizations/#learning_curves","title":"learning_curves","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>training_statistics</code></li> <li><code>model_names</code></li> </ul> <p>For each model (in the aligned lists of <code>training_statistics</code> and <code>model_names</code>) and for each output feature and metric of the model, it produces a line plot showing how that metric changed over the course of the epochs of training on the training and validation sets.  If <code>output_feature_name</code> is not specified, then all output features are plotted.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization learning_curves \\\n  --output_feature_name Survived \\\n  --training_statistics results/titanic_Model1_0/training_statistics.json \\\n       results/titanic_Model2_0/training_statistics.json \\\n  --model_names Model1 Model2\n</code></pre> <p></p> <p></p>"},{"location":"user_guide/visualizations/#confusion-matrix","title":"Confusion Matrix","text":""},{"location":"user_guide/visualizations/#confusion_matrix","title":"confusion_matrix","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>normalize</code></li> </ul> <p>For each model (in the aligned lists of <code>test_statistics</code> and <code>model_names</code>) it produces a heatmap of the confusion matrix in the predictions for each field that has a confusion matrix in <code>test_statistics</code>. The value of <code>top_n_classes</code> limits the heatmap to the <code>n</code> most frequent classes.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization confusion_matrix \\\n  --ground_truth_metadata results/titanic_Model1_0/model/train_set_metadata.json \\\n  --test_statistics results/titanic_Model1_0/test_statistics.json \\\n  --top_n_classes 2\n</code></pre> <p></p> <p>The second plot produced is a bar chart showing the entropy of each class, ranked from most entropic to least entropic.</p> <p></p>"},{"location":"user_guide/visualizations/#compare-performance","title":"Compare Performance","text":""},{"location":"user_guide/visualizations/#compare_performance","title":"compare_performance","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> </ul> <p>For each model (in the aligned lists of <code>test_statistics</code> and <code>model_names</code>) it produces bars in a bar plot, one for each overall metric available in the <code>test_statistics</code> file for the specified <code>output_feature_name</code>.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_performance \\\n  --output_feature_name Survived \\\n  --test_statistics results/titanic_Model1_0/test_statistics.json \\\n       results/titanic_Model2_0/test_statistics.json \\\n  --model_names Model1 Model2\n</code></pre> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_performance_from_prob","title":"compare_classifiers_performance_from_prob","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must be the name of category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces bars in a bar plot, one for each overall metric computed from the probabilities of predictions for the specified <code>output_feature_name</code>.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_classifiers_performance_from_prob \\\n  --ground_truth train.hdf5 \\\n  --output_feature_name Survived \\\n  --probabilities results/titanic_Model1_0/Survived_probabilities.csv \\\n        results/titanic_Model2_0/Survived_probabilities.csv \\\n  --model_names Model1 Model2\n</code></pre> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_performance_from_pred","title":"compare_classifiers_performance_from_pred","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>predictions</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>predictions</code> and <code>model_names</code>) it produces bars in a bar plot, one for each overall metric computed on the fly from the predictions for the specified <code>output_feature_name</code>.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_classifiers_performance_from_pred \\\n  --ground_truth train.hdf5 \\\n  --ground_truth_metadata train.json \\\n  --output_feature_name Survived \\\n  --predictions results/titanic_Model1_0/Survived_predictions.csv \\\n        results/titanic_Model2_0/Survived_predictions.csv \\\n  --model_names Model1 Model2\n</code></pre> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_performance_subset","title":"compare_classifiers_performance_subset","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>labels_limit</code></li> <li><code>subset</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>predictions</code> and <code>model_names</code>) it produces bars in a bar plot, one for each overall metric computed on the fly from the probabilities predictions for the specified <code>output_feature_name</code>, considering only a subset of the full training set. The way the subset is obtained is using the <code>top_n_classes</code> and <code>subset</code> parameters.</p> <p>If the values of <code>subset</code> is <code>ground_truth</code>, then only datapoints where the ground truth class is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_classifiers_performance_subset \\\n  --ground_truth train.hdf5 \\\n  --ground_truth_metadata train.json \\\n  --output_feature_name Survived \\\n  --probabilities results/titanic_Model1_0/Survived_probabilities.csv \\\n           results/titanic_Model2_0/Survived_probabilities.csv \\\n  --model_names Model1 Model2 \\\n  --top_n_classes 2 \\\n  --subset ground_truth\n</code></pre> <p></p> <p>If the values of <code>subset</code> is <code>predictions</code>, then only datapoints where the model predicts a class that is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed for each model.</p> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_performance_changing_k","title":"compare_classifiers_performance_changing_k","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_k</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces a line plot that shows the Hits@K metric (that counts a prediction as correct if the model produces it among the first <code>k</code>) while changing <code>k</code> from 1 to <code>top_k</code> for the specified <code>output_feature_name</code>.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_classifiers_performance_changing_k \\\n  --ground_truth train.hdf5 \\\n  --output_feature_name Survived \\\n  --probabilities results/titanic_Model1_0/Survived_probabilities.csv \\\n         results/titanic_Model2_0/Survived_probabilities.csv \\\n  --model_names Model1 Model2 \\\n  --top_k 5\n</code></pre> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_multiclass_multimetric","title":"compare_classifiers_multiclass_multimetric","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>test_statistics</code> and <code>model_names</code>) it produces four plots that show the precision, recall and F1 of the model on several classes for the specified <code>output_feature_name</code>.</p> <p>The first one shows the metrics on the <code>n</code> most frequent classes.</p> <p></p> <p>The second one shows the metrics on the <code>n</code> classes where the model performs the best.</p> <p></p> <p>The third one shows the metrics on the <code>n</code> classes where the model performs the worst.</p> <p></p> <p>The fourth one shows the metrics on all the classes, sorted by their frequency. This will become unreadable if the number of classes is too high.</p> <p></p>"},{"location":"user_guide/visualizations/#compare-classifier-predictions","title":"Compare Classifier Predictions","text":""},{"location":"user_guide/visualizations/#compare_classifiers_predictions","title":"compare_classifiers_predictions","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>predictions</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature and there must be two and only two models (in the aligned lists of <code>predictions</code> and <code>model_names</code>). This visualization produces a pie chart comparing the predictions of the two models for the specified <code>output_feature_name</code>.</p> <p>Example command:</p> <pre><code>ludwig visualize --visualization compare_classifiers_predictions \\\n  --ground_truth train.hdf5 \\\n  --output_feature_name Survived \\\n  --predictions results/titanic_Model1_0/Survived_predictions.csv \\\n          results/titanic_Model2_0/Survived_predictions.csv \\\n  --model_names Model1 Model2\n</code></pre> <p></p>"},{"location":"user_guide/visualizations/#compare_classifiers_predictions_distribution","title":"compare_classifiers_predictions_distribution","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>predictions</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. This visualization produces a radar plot comparing the distributions of predictions of the models for the first 10 classes of the specified <code>output_feature_name</code>.</p> <p></p>"},{"location":"user_guide/visualizations/#confidence-thresholding","title":"Confidence Thresholding","text":""},{"location":"user_guide/visualizations/#confidence_thresholding","title":"confidence_thresholding","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces a pair of lines indicating the accuracy of the model and the data coverage while increasing a threshold (x axis) on the probabilities of predictions for the specified <code>output_feature_name</code>.</p> <p></p>"},{"location":"user_guide/visualizations/#confidence_thresholding_data_vs_acc","title":"confidence_thresholding_data_vs_acc","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces a line indicating the accuracy of the model and the data coverage while increasing a threshold on the probabilities of predictions for the specified <code>output_feature_name</code>. The difference with <code>confidence_thresholding</code> is that it uses two axes instead of three, not visualizing the threshold and having coverage as x axis instead of the threshold.</p> <p></p>"},{"location":"user_guide/visualizations/#confidence_thresholding_data_vs_acc_subset","title":"confidence_thresholding_data_vs_acc_subset","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>labels_limit</code></li> <li><code>subset</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces a line indicating the accuracy of the model and the data coverage while increasing a threshold on the probabilities of predictions for the specified <code>output_feature_name</code>, considering only a subset of the full training set. The way the subset is obtained is using the <code>top_n_classes</code> and <code>subset</code> parameters. The difference with <code>confidence_thresholding</code> is that it uses two axes instead of three, not visualizing the threshold and having coverage as x axis instead of the threshold.</p> <p>If the values of <code>subset</code> is <code>ground_truth</code>, then only datapoints where the ground truth class is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed. If the values of <code>subset</code> is <code>predictions</code>, then only datapoints where the model predicts a class that is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed for each model.</p> <p></p>"},{"location":"user_guide/visualizations/#confidence_thresholding_data_vs_acc_subset_per_class","title":"confidence_thresholding_data_vs_acc_subset_per_class","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>labels_limit</code></li> <li><code>subset</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>) it produces a line indicating the accuracy of the model and the data coverage while increasing a threshold on the probabilities of predictions for the specified <code>output_feature_name</code>, considering only a subset of the full training set. The way the subset is obtained is using the <code>top_n_classes</code> and <code>subset</code> parameters. The difference with <code>confidence_thresholding</code> is that it uses two axes instead of three, not visualizing the threshold and having coverage as x axis instead of the threshold.</p> <p>If the values of <code>subset</code> is <code>ground_truth</code>, then only datapoints where the ground truth class is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed. If the values of <code>subset</code> is <code>predictions</code>, then only datapoints where the model predicts a class that is within the top <code>n</code> most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed for each model.</p> <p>The difference with <code>confidence_thresholding_data_vs_acc_subset</code> is that it produces one plot per class within the <code>top_n_classes</code>.</p> <p></p> <p></p>"},{"location":"user_guide/visualizations/#confidence_thresholding_2thresholds_2d","title":"confidence_thresholding_2thresholds_2d","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>ground_truth_split</code></li> <li><code>threshold_output_feature_names</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>threshold_output_feature_names</code> need to be exactly two, either category or binary. <code>probabilities</code> need to be exactly two, aligned with <code>threshold_output_feature_names</code>. <code>model_names</code> has to be exactly one. Three plots are produced.</p> <p>The first plot shows several semi transparent lines. They summarize the 3d surfaces displayed by <code>confidence_thresholding_2thresholds_3d</code> that have thresholds on the confidence of the predictions of the two <code>threshold_output_feature_names</code> as x and y axes and either the data coverage percentage or the accuracy as z axis. Each line represents a slice of the data coverage surface projected onto the accuracy surface.</p> <p></p> <p>The second plot shows the max of all the lines displayed in the first plot.</p> <p></p> <p>The third plot shows the max line and the values of the thresholds that obtained a specific data coverage vs accuracy pair of values.</p> <p></p>"},{"location":"user_guide/visualizations/#confidence_thresholding_2thresholds_3d","title":"confidence_thresholding_2thresholds_3d","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>ground_truth_split</code></li> <li><code>threshold_output_feature_names</code></li> <li><code>probabilities</code></li> <li><code>labels_limit</code></li> </ul> <p><code>threshold_output_feature_names</code> need to be exactly two, either category or binary. <code>probabilities</code> need to be exactly two, aligned with <code>threshold_output_feature_names</code>. The plot shows the 3d surfaces displayed by <code>confidence_thresholding_2thresholds_3d</code> that have thresholds on the confidence of the predictions of the two <code>threshold_output_feature_names</code> as x and y axes and either the data coverage percentage or the accuracy as z axis.</p> <p></p>"},{"location":"user_guide/visualizations/#binary-threshold-vs-metric","title":"Binary Threshold vs. Metric","text":""},{"location":"user_guide/visualizations/#binary_threshold_vs_metric","title":"binary_threshold_vs_metric","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>metrics</code></li> <li><code>positive_label</code></li> </ul> <p><code>output_feature_name</code> can be a category or binary feature. For each metric specified in <code>metrics</code> (options are <code>f1</code>, <code>precision</code>, <code>recall</code>, <code>accuracy</code>), this visualization produces a line chart plotting a threshold on the confidence of the model against the metric for the specified <code>output_feature_name</code>. If <code>output_feature_name</code> is a category feature, <code>positive_label</code> indicates which class is to be considered the positive class, all others will be considered negative. <code>positive_label</code> must be an integer, to find the integer label associated with a class check the <code>ground_truth_metadata</code> JSON file.</p> <p></p>"},{"location":"user_guide/visualizations/#roc-curves","title":"ROC Curves","text":""},{"location":"user_guide/visualizations/#roc_curves","title":"roc_curves","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>positive_label</code></li> </ul> <p><code>output_feature_name</code> can be a category or binary feature. This visualization produces a line chart plotting the roc curves for the specified <code>output_feature_name</code>. If <code>output_feature_name</code> is a category feature, <code>positive_label</code> indicates which class is considered the positive class, all others will be considered negative. <code>positive_label</code> must be an integer, to find the integer label associated with a class check the <code>ground_truth_metadata</code> JSON file.</p> <p></p>"},{"location":"user_guide/visualizations/#roc_curves_from_test_statistics","title":"roc_curves_from_test_statistics","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> </ul> <p><code>output_feature_name</code> must name a binary feature. This visualization produces a line chart plotting the roc curves for the specified <code>output_feature_name</code>.</p> <p></p>"},{"location":"user_guide/visualizations/#precision-recall-curves","title":"Precision Recall Curves","text":""},{"location":"user_guide/visualizations/#precision_recall_curves","title":"precision_recall_curves","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>positive_label</code></li> </ul> <p><code>output_feature_name</code> can be a category or binary feature. This visualization produces a line chart plotting the precision recall curves for the specified <code>output_feature_name</code>. If <code>output_feature_name</code> is a category feature, <code>positive_label</code> indicates which class is considered the positive class, all others will be considered negative. <code>positive_label</code> must be an integer, to find the integer label associated with a class check the <code>ground_truth_metadata</code> JSON file.</p> <p></p>"},{"location":"user_guide/visualizations/#precision_recall_curves_from_test_statistics","title":"precision_recall_curves_from_test_statistics","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> </ul> <p><code>output_feature_name</code> must name a binary feature. This visualization produces a line chart plotting the precision recall curves for the specified <code>output_feature_name</code>.</p> <p></p>"},{"location":"user_guide/visualizations/#calibration-plot","title":"Calibration Plot","text":""},{"location":"user_guide/visualizations/#calibration_1_vs_all","title":"calibration_1_vs_all","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category or binary feature. For each class or each of the <code>n</code> most frequent classes if <code>top_n_classes</code> is specified, it produces two plots computed from the probabilities of predictions for the specified <code>output_feature_name</code>.</p> <p>The first plot is a calibration curve that shows the calibration of the predictions considering the current class to be the true one and all others to be a false one, drawing one line for each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>).</p> <p></p> <p>The second plot shows the distributions of the predictions considering the current class to be the true one and all others to be a false one, drawing the distribution for each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>).</p> <p></p>"},{"location":"user_guide/visualizations/#calibration_multiclass","title":"calibration_multiclass","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth</code></li> <li><code>split_file</code></li> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>ground_truth_split</code></li> <li><code>probabilities</code></li> <li><code>model_names</code></li> <li><code>labels_limit</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each class, produces two plots computed from the probabilities of predictions for the specified <code>output_feature_name</code>.</p> <p>The first plot is a calibration curve that shows the calibration of the predictions considering all classes, drawing one line for each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>).</p> <p></p> <p>The second plot shows a bar plot of the Brier score (which calculates how calibrated are the probabilities of the predictions of a model), drawing one bar for each model (in the aligned lists of <code>probabilities</code> and <code>model_names</code>).</p> <p></p>"},{"location":"user_guide/visualizations/#class-frequency-vs-f1-score","title":"Class Frequency vs. F1 score","text":""},{"location":"user_guide/visualizations/#frequency_vs_f1","title":"frequency_vs_f1","text":"<p>Parameters for this visualization:</p> <ul> <li><code>ground_truth_metadata</code></li> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>output_feature_name</code></li> <li><code>test_statistics</code></li> <li><code>model_names</code></li> <li><code>top_n_classes</code></li> </ul> <p><code>output_feature_name</code> must name a category feature. For each model (in the aligned lists of <code>test_statistics</code> and <code>model_names</code>), produces two plots statistics of predictions for the specified <code>output_feature_name</code>.</p> <p>Generates plots for <code>top_n_classes</code>.  The first plot is a line plot with one x axis representing the different classes and two vertical axes colored in orange and blue respectively. The orange one is the frequency of the class and an orange line is plotted to show the trend. The blue one is the F1 score for that class and a blue line is plotted to show the trend. The classes on the x axis are sorted by F1 score.</p> <p></p> <p>The second plot has the same structure as the first one, but the axes are flipped and the classes on the x axis are sorted by frequency.</p> <p></p>"},{"location":"user_guide/visualizations/#hyperparameter-optimization-visualization","title":"Hyperparameter optimization visualization","text":"<p>The examples of the hyperparameter visualizations shown here are obtained by running a random search with 100 samples on the ATIS dataset used for classifying intents given user utterances.</p>"},{"location":"user_guide/visualizations/#hyperopt_report","title":"hyperopt_report","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>hyperopt_stats_path</code></li> </ul> <p>The visualization creates one plot for each hyperparameter in the file at <code>hyperopt_stats_path</code> vs the metric hyperopt was optimized for (for e.g., loss), plus an additional one containing a pair plot of hyperparameters interactions.</p> <p>Each plot will show the distribution of the parameters with respect to the metric to optimize. For <code>float</code> and <code>int</code> parameters a scatter plot is used, while for <code>category</code> parameters a Raincloud plot is used instead. Raincloud plots can visualize raw data, probability density, and key summary statistics such as median, mean, and relevant confidence intervals with minimal redundancy.</p>"},{"location":"user_guide/visualizations/#float-parameter-hyperopt-plot","title":"Float parameter hyperopt plot","text":""},{"location":"user_guide/visualizations/#integer-parameter-hyperopt-plot","title":"Integer parameter hyperopt plot","text":""},{"location":"user_guide/visualizations/#category-parameter-hyperopt-plot","title":"Category parameter hyperopt plot","text":"<p>Note</p> <p>For <code>category</code> type parameters, raincloud plots are only created if there are enough hyperopt trials trained so that there are 2 or more trials per parameter value. Otherwise, a stripplot (a type of categorical scatterplot) is created.</p> <p>The pair plot shows a heatmap of how the values of pairs of hyperparameters correlate with the metric to optimize.</p> <p></p>"},{"location":"user_guide/visualizations/#hyperopt_hiplot","title":"hyperopt_hiplot","text":"<p>Parameters for this visualization:</p> <ul> <li><code>output_directory</code></li> <li><code>file_format</code></li> <li><code>hyperopt_stats_path</code></li> </ul> <p>The visualization creates an interactive HTML page visualizing all the results from the hyperparameter optimization at once using a parallel coordinate plot.</p> <p>Note</p> <p>This plot is only created if there is more than one parameter in the hyperopt parameter space.</p> <p></p>"},{"location":"user_guide/visualizations/#tensorboard","title":"Tensorboard","text":"<p>Users can visualize raw training metrics on Tensorboard with:</p> <pre><code>tensorboard --logdir &lt;/path/to/model&gt;/log\n</code></pre>"},{"location":"user_guide/what_is_ludwig/","title":"What is Ludwig?","text":""},{"location":"user_guide/what_is_ludwig/#introduction","title":"Introduction","text":"<p>Ludwig is an open-source, declarative machine learning framework that makes it easy to define deep learning pipelines with a simple and flexible data-driven configuration system. Ludwig is suitable for a wide variety of AI tasks, and is hosted by the Linux Foundation AI &amp; Data.</p> <p>Ludwig enables you to apply state-of-the-art tabular, natural language processing, and computer vision models to your existing data and put them into production with just a few short commands.</p> CLIPythondata.csvconfig.yaml <pre><code>ludwig train --config config.yaml --dataset data.csv\nludwig predict --model_path results/experiment_run/model --dataset test.csv\n</code></pre> <pre><code>from ludwig.api import LudwigModel\nimport pandas as pd\n\n# train a model\nconfig = {\n    \"input_features\": [\n        {\n            \"name\": \"sepal_length_cm\",\n            \"type\": \"number\"\n        },\n        {\n            \"name\": \"sepal_width_cm\",\n            \"type\": \"number\"\n        },\n        {\n            \"name\": \"petal_length_cm\",\n            \"type\": \"number\"\n        },\n        {\n            \"name\": \"petal_width_cm\",\n            \"type\": \"number\"\n        }\n    ],\n    \"output_features\": [\n        {\n            \"name\": \"class\",\n            \"type\": \"category\"\n        }\n    ]\n}\nmodel = LudwigModel(config)\ndata = pd.read_csv(\"data.csv\")\ntrain_stats, _, model_dir = model.train(data)\n\n# or load a model\nmodel = LudwigModel.load(model_dir)\n\n# obtain predictions\npredictions = model.predict(data)\n</code></pre> <pre><code>sepal_length_cm,sepal_width_cm,petal_length_cm,petal_width_cm\n4.9,3.0,1.4,0.2\n4.7,3.2,1.3,0.2\n4.6,3.1,1.5,0.2\n5.0,3.6,1.4,0.2\n5.4,3.9,1.7,0.4\n4.6,3.4,1.4,0.3\n5.0,3.4,1.5,0.2\n4.4,2.9,1.4,0.2\n4.9,3.1,1.5,0.1\n</code></pre> <pre><code>input_features:\n- name: sepal_length_cm\ntype: number\n- name: sepal_width_cm\ntype: number\n- name: petal_length_cm\ntype: number\n- name: petal_width_cm\ntype: number\noutput_features:\n- name: class\ntype: category\n</code></pre> <p>Ludwig makes this possible through its declarative approach to structuring machine learning pipelines. Instead of writing code for your model, training loop, preprocessing, postprocessing, evaluation and hyperparameter optimization, you only need to declare the schema of your data with a simple YAML configuration:</p> <pre><code>input_features:\n- name: title\ntype: text\n- name: author\ntype: category\n- name: description\ntype: text\n- name: cover\ntype: image\n\noutput_features:\n- name: genre\ntype: set\n- name: price\ntype: number\n</code></pre> <p>Starting from a simple config like the one above, any and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements:</p> <pre><code>input_features:\n- name: title\ntype: text\nencoder: rnn\ncell: lstm\nnum_layers: 2\nstate_size: 128\npreprocessing:\ntokenizer: space_punct\n- name: author\ntype: category\nembedding_size: 128\npreprocessing:\nmost_common: 10000\n- name: description\ntype: text\nencoder: bert\n- name: cover\ntype: image\nencoder: resnet\nnum_layers: 18\n\noutput_features:\n- name: genre\ntype: set\n- name: price\ntype: number\npreprocessing:\nnormalization: zscore\n\ntrainer:\nepochs: 50\nbatch_size: 256\noptimizer:\ntype: adam\nbeat1: 0.9\nlearning_rate: 0.001\n\nbackend:\ntype: local\ncache_format: parquet\n\nhyperopt:\nmetric: f1\nsampler: random\nparameters:\ntitle.num_layers:\nlower: 1\nupper: 5\ntraining.learning_rate:\nvalues: [0.01, 0.003, 0.001]\n</code></pre> <p>Ludwig is a single framework that guides you through machine learning end-to-end; from experimenting with different training recipes, exploring state-of-the-art model architectures, to scaling up to large out-of-memory datasets and multi-node clusters, and finally serving the best model in production.</p>"},{"location":"user_guide/what_is_ludwig/#why-declarative-machine-learning-systems","title":"Why Declarative Machine Learning Systems","text":"<p>Ludwig\u2019s declarative approach to machine learning provides the simplicity of an AutoML solution with the flexibility of writing your own PyTorch code. This is achieved by creating an extensible, declarative configuration with optional parameters for every aspect of the pipeline.</p>"},{"location":"user_guide/what_is_ludwig/#multi-modal-multi-task-learning-out-of-the-box","title":"Multi-modal, multi-task learning out-of-the-box","text":"<p>Mix and match tabular data, text, images, and even audio into complex model configurations without writing code.</p>"},{"location":"user_guide/what_is_ludwig/#fully-customizable-and-extensible","title":"Fully customizable and extensible","text":"<p>Every part of the model and training process can be controlled through a simple configuration interface.</p>"},{"location":"user_guide/what_is_ludwig/#minimal-machine-learning-boilerplate","title":"Minimal machine learning boilerplate","text":"<p>Engineering complexity of deep learning is handled out of the box, enabling research scientists to focus on building models at the highest level of abstraction.</p> <p>Data preprocessing, hyperparameter optimization, device management, and distributed training for newly registered <code>torch.nn.Module</code> models come completely free.</p>"},{"location":"user_guide/what_is_ludwig/#why-ludwig","title":"Why Ludwig","text":"<p>Ludwig\u2019s declarative programming model allows for key features such as:</p>"},{"location":"user_guide/what_is_ludwig/#highly-configurable-data-preprocessing-modeling-and-metrics","title":"Highly configurable data preprocessing, modeling, and metrics","text":"<p>Any and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements.</p> <p>For details on what can be configured, check out Ludwig Configuration docs.</p>"},{"location":"user_guide/what_is_ludwig/#integration-with-any-structured-data-source","title":"Integration with any structured data source","text":"<p>If it can be read into a SQL table or Pandas DataFrame, Ludwig can train a model on it.</p>"},{"location":"user_guide/what_is_ludwig/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>Perform a variety of hyperparameter search algorithms locally or across many workers in parallel using Ray Tune.</p>"},{"location":"user_guide/what_is_ludwig/#rich-model-exporting-and-tracking","title":"Rich model exporting and tracking","text":"<p>Automatically track all trials and metrics with tools like Tensorboard, Comet ML, Weights &amp; Biases, and MLflow.</p>"},{"location":"user_guide/what_is_ludwig/#automatically-scale-training-to-multi-gpu-multi-node-clusters","title":"Automatically scale training to multi-GPU, multi-node clusters","text":"<p>Go from training on your local machine to the cloud without code or config changes.</p>"},{"location":"user_guide/what_is_ludwig/#easily-build-your-benchmarks","title":"Easily build your benchmarks","text":"<p>Creating a state-of-the-art baseline and comapring it with a new model is a simple config change.</p>"},{"location":"user_guide/what_is_ludwig/#easily-apply-new-architectures-to-multiple-problems-and-datasets","title":"Easily apply new architectures to multiple problems and datasets","text":"<p>Apply new models across the extensive set of tasks and datasets that Ludwig supports. Ludwig includes a full benchmarking toolkit accessible to any user, for running experiments with multiple models across multiple datasets with just a simple configuration.</p>"},{"location":"user_guide/what_is_ludwig/#low-code-interface-for-state-of-the-art-models-including-pre-trained-huggingface-transformers","title":"Low-code interface for state-of-the-art models, including pre-trained Huggingface Transformers","text":"<p>Ludwig also natively integrates with pre-trained models, such as the ones available in Huggingface Transformers. Users can choose from a vast collection of state-of-the-art pre-trained PyTorch models to use without needing to write any code at all. For example, training a BERT-based sentiment analysis model with Ludwig is as simple as:</p> <pre><code>ludwig train --dataset sst5 -\u2013config_str \u201c{input_features: [{name: sentence, type: text, encoder: bert}], output_features: [{name: label, type: category}]}\u201d\n</code></pre>"},{"location":"user_guide/what_is_ludwig/#low-code-interface-for-automl","title":"Low-code interface for AutoML","text":"<p>Ludwig AutoML allows users to obtain trained models by providing just a dataset, the target column, and a time budget.</p> <pre><code>auto_train_results = ludwig.automl.auto_train(dataset=my_dataset_df, target=target_column_name, time_limit_s=7200)\n</code></pre>"},{"location":"user_guide/what_is_ludwig/#easy-productionisation","title":"Easy productionisation","text":"<p>Ludwig makes it easy to serve deep learning models, including on GPUs. Launch a REST API for your trained Ludwig model.</p> <pre><code>ludwig serve --model_path=/path/to/model\n</code></pre> <p>Ludwig supports exporting models to efficient Torschscript bundles.</p> <pre><code>ludwig export_torchscript -\u2013model_path=/path/to/model\n</code></pre>"},{"location":"user_guide/api/LudwigModel/","title":"LudwigModel","text":""},{"location":"user_guide/api/LudwigModel/#ludwigmodel-class-source","title":"LudwigModel class [source]","text":"<pre><code>ludwig.api.LudwigModel(\n  config,\n  logging_level=40,\n  backend=None,\n  gpus=None,\n  gpu_memory_limit=None,\n  allow_parallel_threads=True,\n  callbacks=None\n)\n</code></pre> <p>Class that allows access to high level Ludwig functionalities.</p> <p>Inputs</p> <ul> <li>config (Union[str, dict]): in-memory representation of     config or string path to a YAML config file.</li> <li>logging_level (int): Log level that will be sent to stderr.</li> <li>backend (Union[Backend, str]): <code>Backend</code> or string name of backend to use to execute preprocessing / training steps.</li> <li>gpus (Union[str, int, List[int]], default: <code>None</code>): GPUs to use (it uses the same syntax of CUDA_VISIBLE_DEVICES)</li> <li>gpu_memory_limit (float: default: <code>None</code>): maximum memory fraction [0, 1] allowed to allocate per GPU device.</li> <li>allow_parallel_threads (bool, default: <code>True</code>): allow Torch to use multithreading parallelism to improve performance at the cost of determinism.</li> </ul> <p>Example usage:</p> <pre><code>from ludwig.api import LudwigModel\n</code></pre> <p>Train a model:</p> <pre><code>config = {...}\nludwig_model = LudwigModel(config)\ntrain_stats, _, _ = ludwig_model.train(dataset=file_path)\n</code></pre> <p>or</p> <pre><code>train_stats, _, _ = ludwig_model.train(dataset=dataframe)\n</code></pre> <p>If you have already trained a model you can load it and use it to predict</p> <pre><code>ludwig_model = LudwigModel.load(model_dir)\n</code></pre> <p>Predict:</p> <pre><code>predictions, _ = ludwig_model.predict(dataset=file_path)\n</code></pre> <p>or</p> <pre><code>predictions, _ = ludwig_model.predict(dataset=dataframe)\n</code></pre> <p>Evaluation:</p> <pre><code>eval_stats, _, _ = ludwig_model.evaluate(dataset=file_path)\n</code></pre> <p>or</p> <pre><code>eval_stats, _, _ = ludwig_model.evaluate(dataset=dataframe)\n</code></pre> <p>PublicAPI: This API is stable across Ludwig releases.</p>"},{"location":"user_guide/api/LudwigModel/#ludwigmodel-methods","title":"LudwigModel methods","text":""},{"location":"user_guide/api/LudwigModel/#collect_activations","title":"collect_activations","text":"<pre><code>collect_activations(\n  layer_names,\n  dataset,\n  data_format=None,\n  split='full',\n  batch_size=128,\n  debug=False\n)\n</code></pre> <p>Loads a pre-trained model model and input data to collect the values of the activations contained in the tensors.</p> <p>Inputs</p> <ul> <li>layer_names (list): list of strings for layer names in the model to collect activations.</li> <li>dataset (Union[str, Dict[str, list], pandas.DataFrame]): source containing the data to make predictions.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>. :param: split: (str, default= <code>'full'</code>): if the input dataset contains a split column, this parameter indicates which split of the data to use. Possible values are <code>'full'</code>, <code>'training'</code>, <code>'validation'</code>, <code>'test'</code>.</li> <li>batch_size (int, default: 128): size of batch to use when making predictions.</li> </ul> <p>Return</p> <ul> <li>return (list): list of collected tensors.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#collect_weights","title":"collect_weights","text":"<pre><code>collect_weights(\n  tensor_names=None\n)\n</code></pre> <p>Load a pre-trained model and collect the tensors with a specific name.</p> <p>Inputs</p> <ul> <li>tensor_names (list, default: <code>None</code>): List of tensor names to collect weights</li> </ul> <p>Return</p> <ul> <li>return (list): List of tensors</li> </ul>"},{"location":"user_guide/api/LudwigModel/#create_model","title":"create_model","text":"<pre><code>create_model(\n  config_obj,\n  random_seed=42\n)\n</code></pre> <p>Instantiates BaseModel object.</p> <p>Inputs</p> <ul> <li>config_obj (Config): Ludwig config object</li> <li>random_seed (int, default: ludwig default random seed): Random seed used for weights initialization, splits and any other random function.</li> </ul> <p>Return</p> <ul> <li>return (ludwig.models.BaseModel): Instance of the Ludwig model object.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#evaluate","title":"evaluate","text":"<pre><code>ludwig.evaluate(\n  dataset=None,\n  data_format=None,\n  split='full',\n  batch_size=None,\n  skip_save_unprocessed_output=True,\n  skip_save_predictions=True,\n  skip_save_eval_stats=True,\n  collect_predictions=False,\n  collect_overall_stats=False,\n  output_directory='results',\n  return_type=&lt;class 'pandas.core.frame.DataFrame'&gt;\n)\n</code></pre> <p>This function is used to predict the output variables given the input variables using the trained model and compute test statistics like performance measures, confusion matrices and the like.</p> <p>Inputs</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame]): source containing the entire dataset to be evaluated.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>. :param: split: (str, default= <code>'full'</code>): if the input dataset contains a split column, this parameter indicates which split of the data to use. Possible values are <code>'full'</code>, <code>'training'</code>, <code>'validation'</code>, <code>'test'</code>.</li> <li>batch_size (int, default: None): size of batch to use when making predictions. Defaults to model config eval_batch_size</li> <li>skip_save_unprocessed_output (bool, default: <code>True</code>): if this parameter is <code>False</code>, predictions and their probabilities are saved in both raw unprocessed numpy files containing tensors and as postprocessed CSV files (one for each output feature). If this parameter is <code>True</code>, only the CSV ones are saved and the numpy ones are skipped.</li> <li>skip_save_predictions (bool, default: <code>True</code>): skips saving test predictions CSV files.</li> <li>skip_save_eval_stats (bool, default: <code>True</code>): skips saving test statistics JSON file.</li> <li>collect_predictions (bool, default: <code>False</code>): if <code>True</code> collects post-processed predictions during eval.</li> <li>collect_overall_stats (bool, default: False): if <code>True</code> collects overall stats during eval.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>return_type (Union[str, dict, pd.DataFrame], default: pandas.DataFrame): indicates the format to of the returned predictions.</li> </ul> <p>Return</p> <ul> <li>return (<code>evaluation_statistics</code>, <code>predictions</code>, <code>output_directory</code>): <code>evaluation_statistics</code> dictionary containing evaluation performance     statistics, <code>postprocess_predictions</code> contains predicted values, <code>output_directory</code> is location where results are stored.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#experiment","title":"experiment","text":"<pre><code>experiment(\n  dataset=None,\n  training_set=None,\n  validation_set=None,\n  test_set=None,\n  training_set_metadata=None,\n  data_format=None,\n  experiment_name='experiment',\n  model_name='run',\n  model_load_path=None,\n  model_resume_path=None,\n  eval_split='test',\n  skip_save_training_description=False,\n  skip_save_training_statistics=False,\n  skip_save_model=False,\n  skip_save_progress=False,\n  skip_save_log=False,\n  skip_save_processed_input=False,\n  skip_save_unprocessed_output=False,\n  skip_save_predictions=False,\n  skip_save_eval_stats=False,\n  skip_collect_predictions=False,\n  skip_collect_overall_stats=False,\n  output_directory='results',\n  random_seed=42\n)\n</code></pre> <p>Trains a model on a dataset's training and validation splits and uses it to predict on the test split. It saves the trained model and the statistics of training and testing.</p> <p>Inputs</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing the entire dataset to be used in the experiment. If it has a split column, it will be used for splitting (0 for train, 1 for validation, 2 for test), otherwise the dataset will be randomly split.</li> <li>training_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing training data.</li> <li>validation_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing validation data.</li> <li>test_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing test data.</li> <li>training_set_metadata (Union[str, dict], default: <code>None</code>): metadata JSON file or loaded metadata.  Intermediate preprocessed structure containing the mappings of the input dataset created the first time an input file is used in the same directory with the same name and a '.meta.json' extension.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>.</li> <li>experiment_name (str, default: <code>'experiment'</code>): name for the experiment.</li> <li>model_name (str, default: <code>'run'</code>): name of the model that is being used.</li> <li>model_load_path (str, default: <code>None</code>): if this is specified the loaded model will be used as initialization (useful for transfer learning).</li> <li>model_resume_path (str, default: <code>None</code>): resumes training of the model from the path specified. The config is restored. In addition to config, training statistics and loss for epoch and the state of the optimizer are restored such that training can be effectively continued from a previously interrupted training process.</li> <li>eval_split (str, default: <code>test</code>): split on which to perform evaluation. Valid values are <code>training</code>, <code>validation</code> and <code>test</code>.</li> <li>skip_save_training_description (bool, default: <code>False</code>): disables saving the description JSON file.</li> <li>skip_save_training_statistics (bool, default: <code>False</code>): disables saving training statistics JSON file.</li> <li>skip_save_model (bool, default: <code>False</code>): disables saving model weights and hyperparameters each time the model improves. By default Ludwig saves model weights after each epoch the validation metric improves, but if the model is really big that can be time consuming. If you do not want to keep the weights and just find out what performance a model can get with a set of hyperparameters, use this parameter to skip it, but the model will not be loadable later on and the returned model will have the weights obtained at the end of training, instead of the weights of the epoch with the best validation performance.</li> <li>skip_save_progress (bool, default: <code>False</code>): disables saving progress each epoch. By default Ludwig saves weights and stats after each epoch for enabling resuming of training, but if the model is really big that can be time consuming and will uses twice as much space, use this parameter to skip it, but training cannot be resumed later on.</li> <li>skip_save_log (bool, default: <code>False</code>): disables saving TensorBoard logs. By default Ludwig saves logs for the TensorBoard, but if it is not needed turning it off can slightly increase the overall speed.</li> <li>skip_save_processed_input (bool, default: <code>False</code>): if input dataset is provided it is preprocessed and cached by saving an HDF5 and JSON files to avoid running the preprocessing again. If this parameter is <code>False</code>, the HDF5 and JSON file are not saved.</li> <li>skip_save_unprocessed_output (bool, default: <code>False</code>): by default predictions and their probabilities are saved in both raw unprocessed numpy files containing tensors and as postprocessed CSV files (one for each output feature). If this parameter is True, only the CSV ones are saved and the numpy ones are skipped.</li> <li>skip_save_predictions (bool, default: <code>False</code>): skips saving test predictions CSV files</li> <li>skip_save_eval_stats (bool, default: <code>False</code>): skips saving test statistics JSON file</li> <li>skip_collect_predictions (bool, default: <code>False</code>): skips collecting post-processed predictions during eval.</li> <li>skip_collect_overall_stats (bool, default: <code>False</code>): skips collecting overall stats during eval.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>random_seed (int: default: 42): random seed used for weights initialization, splits and any other random function.</li> </ul> <p>Return</p> <ul> <li>return (Tuple[dict, dict, tuple, str)): <code>(evaluation_statistics, training_statistics, preprocessed_data, output_directory)</code> <code>evaluation_statistics</code> dictionary with evaluation performance     statistics on the test_set, <code>training_statistics</code> is a nested dictionary of dataset -&gt; feature_name -&gt; metric_name -&gt; List of metrics.     Each metric corresponds to each training checkpoint. <code>preprocessed_data</code> tuple containing preprocessed <code>(training_set, validation_set, test_set)</code>, <code>output_directory</code> filepath string to where results are stored.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#load","title":"load","text":"<pre><code>load(\n  model_dir,\n  logging_level=40,\n  backend=None,\n  gpus=None,\n  gpu_memory_limit=None,\n  allow_parallel_threads=True,\n  callbacks=None\n)\n</code></pre> <p>This function allows for loading pretrained models.</p> <p>Inputs</p> <ul> <li>model_dir (str): path to the directory containing the model.    If the model was trained by the <code>train</code> or <code>experiment</code> command,    the model is in <code>results_dir/experiment_dir/model</code>.</li> <li>logging_level (int, default: 40): log level that will be sent to stderr.</li> <li>backend (Union[Backend, str]): <code>Backend</code> or string name of backend to use to execute preprocessing / training steps.</li> <li>gpus (Union[str, int, List[int]], default: <code>None</code>): GPUs to use (it uses the same syntax of CUDA_VISIBLE_DEVICES)</li> <li>gpu_memory_limit (float: default: <code>None</code>): maximum memory fraction [0, 1] allowed to allocate per GPU device.</li> <li>allow_parallel_threads (bool, default: <code>True</code>): allow Torch to use multithreading parallelism to improve performance at the cost of determinism.</li> <li>callbacks (list, default: <code>None</code>): a list of <code>ludwig.callbacks.Callback</code> objects that provide hooks into the Ludwig pipeline.</li> </ul> <p>Return</p> <ul> <li>return (LudwigModel): a LudwigModel object</li> </ul> <p>Example usage</p> <pre><code>ludwig_model = LudwigModel.load(model_dir)\n</code></pre>"},{"location":"user_guide/api/LudwigModel/#load_weights","title":"load_weights","text":"<pre><code>load_weights(\n  model_dir\n)\n</code></pre> <p>Loads weights from a pre-trained model.</p> <p>Inputs</p> <ul> <li>model_dir (str): filepath string to location of a pre-trained model</li> </ul> <p>Return</p> <ul> <li>return ( <code>Non):</code>None`</li> </ul> <p>Example usage</p> <pre><code>ludwig_model.load_weights(model_dir)\n</code></pre>"},{"location":"user_guide/api/LudwigModel/#predict","title":"predict","text":"<pre><code>ludwig.predict(\n  dataset=None,\n  data_format=None,\n  split='full',\n  batch_size=128,\n  skip_save_unprocessed_output=True,\n  skip_save_predictions=True,\n  output_directory='results',\n  return_type=&lt;class 'pandas.core.frame.DataFrame'&gt;,\n  callbacks=None\n)\n</code></pre> <p>Using a trained model, make predictions from the provided dataset.</p> <p>Inputs</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame]): source containing the entire dataset to be evaluated.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>. :param: split: (str, default= <code>'full'</code>): if the input dataset contains a split column, this parameter indicates which split of the data to use. Possible values are <code>'full'</code>, <code>'training'</code>, <code>'validation'</code>, <code>'test'</code>.</li> <li>batch_size (int, default: 128): size of batch to use when making predictions.</li> <li>skip_save_unprocessed_output (bool, default: <code>True</code>): if this parameter is <code>False</code>, predictions and their probabilities are saved in both raw unprocessed numpy files containing tensors and as postprocessed CSV files (one for each output feature). If this parameter is <code>True</code>, only the CSV ones are saved and the numpy ones are skipped.</li> <li>skip_save_predictions (bool, default: <code>True</code>): skips saving test predictions CSV files.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>return_type (Union[str, dict, pandas.DataFrame], default: pd.DataFrame): indicates the format of the returned predictions.</li> <li>callbacks (Optional[List[Callback]], default: None): optional list of callbacks to use during this predict operation. Any callbacks already registered to the model will be preserved.</li> </ul> <p>Return</p> <ul> <li>return (Tuple[Union[dict, pd.DataFrame], str]) <code>(predictions, output_directory):</code> <code>predictions</code> predictions from the provided dataset, <code>output_directory</code> filepath string to where data was stored.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#preprocess","title":"preprocess","text":"<pre><code>preprocess(\n  dataset=None,\n  training_set=None,\n  validation_set=None,\n  test_set=None,\n  training_set_metadata=None,\n  data_format=None,\n  skip_save_processed_input=True,\n  random_seed=42\n)\n</code></pre> <p>This function is used to preprocess data.</p> <p>Args:</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>):     source containing the entire dataset to be used in the experiment.     If it has a split column, it will be used for splitting     (0 for train, 1 for validation, 2 for test),     otherwise the dataset will be randomly split.</li> <li>training_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>):     source containing training data.</li> <li>validation_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>):     source containing validation data.</li> <li>test_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>):     source containing test data.</li> <li>training_set_metadata (Union[str, dict], default: <code>None</code>):     metadata JSON file or loaded metadata. Intermediate preprocessed structure containing the mappings of the input     dataset created the first time an input file is used in the same     directory with the same name and a '.meta.json' extension.</li> <li>data_format (str, default: <code>None</code>): format to interpret data     sources. Will be inferred automatically if not specified.  Valid     formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>,     <code>'feather'</code>, <code>'fwf'</code>,     <code>'hdf5'</code> (cache file produced during previous training),     <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>),     <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>,     <code>'pickle'</code> (pickled Pandas DataFrame),     <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>.</li> <li>skip_save_processed_input (bool, default: <code>False</code>): if input     dataset is provided it is preprocessed and cached by saving an HDF5     and JSON files to avoid running the preprocessing again. If this     parameter is <code>False</code>, the HDF5 and JSON file are not saved.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that     will contain the training statistics, TensorBoard logs, the saved     model and the training progress files.</li> <li>random_seed (int, default: <code>42</code>): a random seed that will be     used anywhere there is a call to a random number generator: data     splitting, parameter initialization and training set shuffling</li> </ul> <p>Returns:</p> <ul> <li>__:return__: (PreprocessedDataset) data structure containing     <code>(proc_training_set, proc_validation_set, proc_test_set, training_set_metadata)</code>.</li> </ul> <p>Raises:</p> <ul> <li>RuntimeError: An error occured while preprocessing the data. Examples include training dataset     being empty after preprocessing, lazy loading not being supported with RayBackend, etc.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#save","title":"save","text":"<pre><code>save(\n  save_path\n)\n</code></pre> <p>This function allows to save models on disk.</p> <p>Inputs</p> <ul> <li> save_path (str): path to the directory where the model is     going to be saved. Both a JSON file containing the model     architecture hyperparameters and checkpoints files containing     model weights will be saved.</li> </ul> <p>Return</p> <ul> <li>return (None): <code>None</code></li> </ul> <p>Example usage</p> <pre><code>ludwig_model.save(save_path)\n</code></pre>"},{"location":"user_guide/api/LudwigModel/#save_config","title":"save_config","text":"<pre><code>save_config(\n  save_path\n)\n</code></pre> <p>Save config to specified location.</p> <p>Inputs</p> <ul> <li>save_path (str): filepath string to save config as a JSON file.</li> </ul> <p>Return</p> <ul> <li>return ( <code>None):</code>None`</li> </ul>"},{"location":"user_guide/api/LudwigModel/#save_torchscript","title":"save_torchscript","text":"<pre><code>save_torchscript(\n  save_path,\n  model_only=False,\n  device=None\n)\n</code></pre> <p>Saves the Torchscript model to disk.</p> <p>save_path (str): The path to the directory where the model will be saved. model_only (bool, optional): If True, only the ECD model will be converted to Torchscript. Else, the     preprocessing and postprocessing steps will also be converted to Torchscript. device (TorchDevice, optional): If None, the model will be converted to Torchscript on the same device to     ensure maximum model parity.</p>"},{"location":"user_guide/api/LudwigModel/#set_logging_level","title":"set_logging_level","text":"<pre><code>set_logging_level(\n  logging_level\n)\n</code></pre> <p>Sets level for log messages.</p> <p>Inputs</p> <ul> <li>logging_level (int): Set/Update the logging level. Use logging constants like <code>logging.DEBUG</code> , <code>logging.INFO</code> and <code>logging.ERROR</code>.</li> </ul> <p>Return</p> <ul> <li>return ( <code>None):</code>None`</li> </ul>"},{"location":"user_guide/api/LudwigModel/#to_torchscript","title":"to_torchscript","text":"<pre><code>to_torchscript(\n  model_only=False,\n  device=None\n)\n</code></pre> <p>Converts the trained model to Torchscript.</p> <p>Args: model_only (bool, optional): If True, only the ECD model will be converted to Torchscript. Else, preprocessing and postprocessing steps will also be converted to Torchscript. device (TorchDevice, optional): If None, the model will be converted to Torchscript on the same device to ensure maximum model parity. Returns: A torch.jit.ScriptModule that can be used to predict on a dictionary of inputs.</p>"},{"location":"user_guide/api/LudwigModel/#train","title":"train","text":"<pre><code>train(\n  dataset=None,\n  training_set=None,\n  validation_set=None,\n  test_set=None,\n  training_set_metadata=None,\n  data_format=None,\n  experiment_name='api_experiment',\n  model_name='run',\n  model_resume_path=None,\n  skip_save_training_description=False,\n  skip_save_training_statistics=False,\n  skip_save_model=False,\n  skip_save_progress=False,\n  skip_save_log=False,\n  skip_save_processed_input=False,\n  output_directory='results',\n  random_seed=42\n)\n</code></pre> <p>This function is used to perform a full training of the model on the specified dataset.</p> <p>During training if the skip parameters are False the model and statistics will be saved in a directory <code>[output_dir]/[experiment_name]_[model_name]_n</code> where all variables are resolved to user specified ones and <code>n</code> is an increasing number starting from 0 used to differentiate among repeated runs.</p> <p>Inputs</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing the entire dataset to be used in the experiment. If it has a split column, it will be used for splitting (0 for train, 1 for validation, 2 for test), otherwise the dataset will be randomly split.</li> <li>training_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing training data.</li> <li>validation_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing validation data.</li> <li>test_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing test data.</li> <li>training_set_metadata (Union[str, dict], default: <code>None</code>): metadata JSON file or loaded metadata. Intermediate preprocessed structure containing the mappings of the input dataset created the first time an input file is used in the same directory with the same name and a '.meta.json' extension.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>.</li> <li>experiment_name (str, default: <code>'experiment'</code>): name for the experiment.</li> <li>model_name (str, default: <code>'run'</code>): name of the model that is being used.</li> <li>model_resume_path (str, default: <code>None</code>): resumes training of the model from the path specified. The config is restored. In addition to config, training statistics, loss for each epoch and the state of the optimizer are restored such that training can be effectively continued from a previously interrupted training process.</li> <li>skip_save_training_description (bool, default: <code>False</code>): disables saving the description JSON file.</li> <li>skip_save_training_statistics (bool, default: <code>False</code>): disables saving training statistics JSON file.</li> <li>skip_save_model (bool, default: <code>False</code>): disables saving model weights and hyperparameters each time the model improves. By default Ludwig saves model weights after each epoch the validation metric improves, but if the model is really big that can be time consuming. If you do not want to keep the weights and just find out what performance a model can get with a set of hyperparameters, use this parameter to skip it, but the model will not be loadable later on and the returned model will have the weights obtained at the end of training, instead of the weights of the epoch with the best validation performance.</li> <li>skip_save_progress (bool, default: <code>False</code>): disables saving progress each epoch. By default Ludwig saves weights and stats after each epoch for enabling resuming of training, but if the model is really big that can be time consuming and will uses twice as much space, use this parameter to skip it, but training cannot be resumed later on.</li> <li>skip_save_log (bool, default: <code>False</code>): disables saving TensorBoard logs. By default Ludwig saves logs for the TensorBoard, but if it is not needed turning it off can slightly increase the overall speed.</li> <li>skip_save_processed_input (bool, default: <code>False</code>): if input dataset is provided it is preprocessed and cached by saving an HDF5 and JSON files to avoid running the preprocessing again. If this parameter is <code>False</code>, the HDF5 and JSON file are not saved.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>random_seed (int, default: <code>42</code>): a random seed that will be    used anywhere there is a call to a random number generator: data    splitting, parameter initialization and training set shuffling</li> </ul> <p>Return</p> <ul> <li>return (Tuple[Dict, Union[Dict, pd.DataFrame], str]): tuple containing <code>(training_statistics, preprocessed_data, output_directory)</code>. <code>training_statistics</code> is a nested dictionary of dataset -&gt; feature_name -&gt; metric_name -&gt; List of metrics.     Each metric corresponds to each training checkpoint. <code>preprocessed_data</code> is the tuple containing these three data sets <code>(training_set, validation_set, test_set)</code>. <code>output_directory</code> filepath to where training results are stored.</li> </ul>"},{"location":"user_guide/api/LudwigModel/#train_online","title":"train_online","text":"<pre><code>train_online(\n  dataset,\n  training_set_metadata=None,\n  data_format='auto',\n  random_seed=42\n)\n</code></pre> <p>Performs one epoch of training of the model on <code>dataset</code>.</p> <p>Inputs</p> <ul> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing the entire dataset to be used in the experiment. If it has a split column, it will be used for splitting (0 for train, 1 for validation, 2 for test), otherwise the dataset will be randomly split.</li> <li>training_set_metadata (Union[str, dict], default: <code>None</code>): metadata JSON file or loaded metadata.  Intermediate preprocessed structure containing the mappings of the input dataset created the first time an input file is used in the same directory with the same name and a '.meta.json' extension.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>.</li> <li>random_seed (int, default: <code>42</code>): a random seed that is going to be    used anywhere there is a call to a random number generator: data    splitting, parameter initialization and training set shuffling</li> </ul> <p>Return</p> <ul> <li>return (None): <code>None</code></li> </ul>"},{"location":"user_guide/api/LudwigModel/#module-functions","title":"Module functions","text":""},{"location":"user_guide/api/LudwigModel/#kfold_cross_validate","title":"kfold_cross_validate","text":"<pre><code>ludwig.api.kfold_cross_validate(\n  num_folds,\n  config,\n  dataset=None,\n  data_format=None,\n  skip_save_training_description=False,\n  skip_save_training_statistics=False,\n  skip_save_model=False,\n  skip_save_progress=False,\n  skip_save_log=False,\n  skip_save_processed_input=False,\n  skip_save_predictions=False,\n  skip_save_eval_stats=False,\n  skip_collect_predictions=False,\n  skip_collect_overall_stats=False,\n  output_directory='results',\n  random_seed=42,\n  gpus=None,\n  gpu_memory_limit=None,\n  allow_parallel_threads=True,\n  backend=None,\n  logging_level=20\n)\n</code></pre> <p>Performs k-fold cross validation and returns result data structures.</p> <p>Inputs</p> <ul> <li>num_folds (int): number of folds to create for the cross-validation</li> <li>config (Union[dict, str]): model specification    required to build a model. Parameter may be a dictionary or string    specifying the file path to a yaml configuration file.  Refer to the    User Guide    for details.</li> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing the entire dataset to be used for k_fold processing.</li> <li>data_format (str, default: <code>None</code>): format to interpret data     sources. Will be inferred automatically if not specified.  Valid     formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>,     <code>'fwf'</code>,     <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>,     <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>,     <code>'stata'</code>, <code>'tsv'</code>.  Currently <code>hdf5</code> format is not supported for     k_fold cross validation.</li> <li>skip_save_training_description (bool, default: <code>False</code>): disables     saving the description JSON file.</li> <li>skip_save_training_statistics (bool, default: <code>False</code>): disables     saving training statistics JSON file.</li> <li>skip_save_model (bool, default: <code>False</code>): disables saving model weights and hyperparameters each time the model improves. By default Ludwig saves model weights after each epoch the validation metric improves, but if the model is really big that can be time consuming. If you do not want to keep the weights and just find out what performance a model can get with a set of hyperparameters, use this parameter to skip it, but the model will not be loadable later on and the returned model will have the weights obtained at the end of training, instead of the weights of the epoch with the best validation performance.</li> <li>skip_save_progress (bool, default: <code>False</code>): disables saving    progress each epoch. By default Ludwig saves weights and stats    after each epoch for enabling resuming of training, but if    the model is really big that can be time consuming and will uses    twice as much space, use this parameter to skip it, but training    cannot be resumed later on.</li> <li>skip_save_log (bool, default: <code>False</code>): disables saving TensorBoard    logs. By default Ludwig saves logs for the TensorBoard, but if it    is not needed turning it off can slightly increase the    overall speed.</li> <li>skip_save_processed_input (bool, default: <code>False</code>): if input dataset is provided it is preprocessed and cached by saving an HDF5 and JSON files to avoid running the preprocessing again. If this parameter is <code>False</code>, the HDF5 and JSON file are not saved.</li> <li>skip_save_predictions (bool, default: <code>False</code>): skips saving test     predictions CSV files.</li> <li>skip_save_eval_stats (bool, default: <code>False</code>): skips saving test     statistics JSON file.</li> <li>skip_collect_predictions (bool, default: <code>False</code>): skips collecting     post-processed predictions during eval.</li> <li>skip_collect_overall_stats (bool, default: <code>False</code>): skips collecting     overall stats during eval.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>random_seed (int, default: <code>42</code>): Random seed     used for weights initialization,    splits and any other random function.</li> <li>gpus (list, default: <code>None</code>): list of GPUs that are available     for training.</li> <li>gpu_memory_limit (float: default: <code>None</code>): maximum memory fraction     [0, 1] allowed to allocate per GPU device.</li> <li>allow_parallel_threads (bool, default: <code>True</code>): allow Torch to     use multithreading parallelism    to improve performance at the cost of determinism.</li> <li>backend (Union[Backend, str]): <code>Backend</code> or string name     of backend to use to execute preprocessing / training steps.</li> <li>logging_level (int, default: INFO): log level to send to stderr.</li> </ul> <p>Return</p> <ul> <li>return (tuple(kfold_cv_statistics, kfold_split_indices), dict): a tuple of     dictionaries <code>kfold_cv_statistics</code>: contains metrics from cv run.      <code>kfold_split_indices</code>: indices to split training data into      training fold and test fold.</li> </ul> <p>PublicAPI: This API is stable across Ludwig releases.</p>"},{"location":"user_guide/api/LudwigModel/#hyperopt","title":"hyperopt","text":"<pre><code>ludwig.hyperopt.run.hyperopt(\n  config,\n  dataset=None,\n  training_set=None,\n  validation_set=None,\n  test_set=None,\n  training_set_metadata=None,\n  data_format=None,\n  experiment_name='hyperopt',\n  model_name='run',\n  resume=None,\n  skip_save_training_description=False,\n  skip_save_training_statistics=False,\n  skip_save_model=False,\n  skip_save_progress=False,\n  skip_save_log=False,\n  skip_save_processed_input=True,\n  skip_save_unprocessed_output=False,\n  skip_save_predictions=False,\n  skip_save_eval_stats=False,\n  skip_save_hyperopt_statistics=False,\n  output_directory='results',\n  gpus=None,\n  gpu_memory_limit=None,\n  allow_parallel_threads=True,\n  callbacks=None,\n  tune_callbacks=None,\n  backend=None,\n  random_seed=42,\n  hyperopt_log_verbosity=3\n)\n</code></pre> <p>This method performs an hyperparameter optimization.</p> <p>Inputs</p> <ul> <li>config (Union[str, dict]): config which defines the different parameters of the model, features, preprocessing and training.  If <code>str</code>, filepath to yaml configuration file.</li> <li>dataset (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing the entire dataset to be used in the experiment. If it has a split column, it will be used for splitting (0 for train, 1 for validation, 2 for test), otherwise the dataset will be randomly split.</li> <li>training_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing training data.</li> <li>validation_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing validation data.</li> <li>test_set (Union[str, dict, pandas.DataFrame], default: <code>None</code>): source containing test data.</li> <li>training_set_metadata (Union[str, dict], default: <code>None</code>): metadata JSON file or loaded metadata.  Intermediate preprocessed structure containing the mappings of the input dataset created the first time an input file is used in the same directory with the same name and a '.meta.json' extension.</li> <li>data_format (str, default: <code>None</code>): format to interpret data sources. Will be inferred automatically if not specified.  Valid formats are <code>'auto'</code>, <code>'csv'</code>, <code>'df'</code>, <code>'dict'</code>, <code>'excel'</code>, <code>'feather'</code>, <code>'fwf'</code>, <code>'hdf5'</code> (cache file produced during previous training), <code>'html'</code> (file containing a single HTML <code>&lt;table&gt;</code>), <code>'json'</code>, <code>'jsonl'</code>, <code>'parquet'</code>, <code>'pickle'</code> (pickled Pandas DataFrame), <code>'sas'</code>, <code>'spss'</code>, <code>'stata'</code>, <code>'tsv'</code>.</li> <li>experiment_name (str, default: <code>'experiment'</code>): name for the experiment.</li> <li>model_name (str, default: <code>'run'</code>): name of the model that is being used.</li> <li>resume (bool): If true, continue hyperopt from the state of the previous run in the output directory with the same experiment name. If false, will create new trials, ignoring any previous state, even if they exist in the output_directory. By default, will attempt to resume if there is already an existing experiment with the same name, and will create new trials if not.</li> <li>skip_save_training_description (bool, default: <code>False</code>): disables saving the description JSON file.</li> <li>skip_save_training_statistics (bool, default: <code>False</code>): disables saving training statistics JSON file.</li> <li>skip_save_model (bool, default: <code>False</code>): disables saving model weights and hyperparameters each time the model improves. By default Ludwig saves model weights after each epoch the validation metric improves, but if the model is really big that can be time consuming. If you do not want to keep the weights and just find out what performance a model can get with a set of hyperparameters, use this parameter to skip it, but the model will not be loadable later on and the returned model will have the weights obtained at the end of training, instead of the weights of the epoch with the best validation performance.</li> <li>skip_save_progress (bool, default: <code>False</code>): disables saving progress each epoch. By default Ludwig saves weights and stats after each epoch for enabling resuming of training, but if the model is really big that can be time consuming and will uses twice as much space, use this parameter to skip it, but training cannot be resumed later on.</li> <li>skip_save_log (bool, default: <code>False</code>): disables saving TensorBoard logs. By default Ludwig saves logs for the TensorBoard, but if it is not needed turning it off can slightly increase the overall speed.</li> <li>skip_save_processed_input (bool, default: <code>False</code>): if input dataset is provided it is preprocessed and cached by saving an HDF5 and JSON files to avoid running the preprocessing again. If this parameter is <code>False</code>, the HDF5 and JSON file are not saved.</li> <li>skip_save_unprocessed_output (bool, default: <code>False</code>): by default predictions and their probabilities are saved in both raw unprocessed numpy files containing tensors and as postprocessed CSV files (one for each output feature). If this parameter is True, only the CSV ones are saved and the numpy ones are skipped.</li> <li>skip_save_predictions (bool, default: <code>False</code>): skips saving test predictions CSV files.</li> <li>skip_save_eval_stats (bool, default: <code>False</code>): skips saving test statistics JSON file.</li> <li>skip_save_hyperopt_statistics (bool, default: <code>False</code>): skips saving hyperopt stats file.</li> <li>output_directory (str, default: <code>'results'</code>): the directory that will contain the training statistics, TensorBoard logs, the saved model and the training progress files.</li> <li>gpus (list, default: <code>None</code>): list of GPUs that are available for training.</li> <li>gpu_memory_limit (float: default: <code>None</code>): maximum memory fraction [0, 1] allowed to allocate per GPU device.</li> <li>allow_parallel_threads (bool, default: <code>True</code>): allow PyTorch to use multithreading parallelism to improve performance at the cost of determinism.</li> <li>callbacks (list, default: <code>None</code>): a list of <code>ludwig.callbacks.Callback</code> objects that provide hooks into the Ludwig pipeline.</li> <li>backend (Union[Backend, str]): <code>Backend</code> or string name of backend to use to execute preprocessing / training steps.</li> <li>random_seed (int: default: 42): random seed used for weights initialization, splits and any other random function.</li> <li>hyperopt_log_verbosity (int: default: 3): controls verbosity of ray tune log messages.  Valid values: 0 = silent, 1 = only status updates, 2 = status and brief trial results, 3 = status and detailed trial results.</li> </ul> <p>Return</p> <ul> <li>return (List[dict]): List of results for each trial, ordered by descending performance on the target metric.</li> </ul>"},{"location":"user_guide/api/visualization/","title":"Visualization","text":""},{"location":"user_guide/api/visualization/#module-functions","title":"Module functions","text":""},{"location":"user_guide/api/visualization/#learning_curves","title":"learning_curves","text":"<pre><code>ludwig.visualize.learning_curves(\n  train_stats_per_model,\n  output_feature_name=None,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  callbacks=None\n)\n</code></pre> <p>Show how model metrics change over training and validation data epochs.</p> <p>For each model and for each output feature and metric of the model, it produces a line plot showing how that metric changed over the course of the epochs of training on the training and validation sets.</p> <p>Inputs</p> <ul> <li>train_stats_per_model (List[dict]): list containing dictionary of training statistics per model.</li> <li>output_feature_name (Union[str, <code>None</code>], default: <code>None</code>): name of the output feature to use for the visualization.  If <code>None</code>, use all output features.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>callbacks (list, default: <code>None</code>): a list of <code>ludwig.callbacks.Callback</code> objects that provide hooks into the Ludwig pipeline.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_performance","title":"compare_performance","text":"<pre><code>ludwig.visualize.compare_performance(\n  test_stats_per_model,\n  output_feature_name=None,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Produces model comparison barplot visualization for each overall metric.</p> <p>For each model (in the aligned lists of test_statistics and model_names) it produces bars in a bar plot, one for each overall metric available in the test_statistics file for the specified output_feature_name.</p> <p>Inputs</p> <ul> <li>test_stats_per_model (List[dict]): dictionary containing evaluation performance statistics.</li> <li>output_feature_name (Union[str, <code>None</code>], default: <code>None</code>): name of the output feature to use for the visualization.  If <code>None</code>, use all output features.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>Example usage:</p> <pre><code>model_a = LudwigModel(config)\nmodel_a.train(dataset)\na_evaluation_stats, _, _ = model_a.evaluate(eval_set)\nmodel_b = LudwigModel.load(\"path/to/model/\")\nb_evaluation_stats, _, _ = model_b.evaluate(eval_set)\ncompare_performance([a_evaluation_stats, b_evaluation_stats], model_names=[\"A\", \"B\"])\n</code></pre> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_performance_from_prob","title":"compare_classifiers_performance_from_prob","text":"<pre><code>ludwig.visualize.compare_classifiers_performance_from_prob(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit=0,\n  top_n_classes=3,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Produces model comparison barplot visualization from probabilities.</p> <p>For each model it produces bars in a bar plot, one for each overall metric computed on the fly from the probabilities of predictions for the specified <code>model_names</code>.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[np.ndarray]): path to experiment probabilities file</li> <li>ground_truth (pd.Series): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>top_n_classes (List[int]): list containing the number of classes to plot.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_performance_from_pred","title":"compare_classifiers_performance_from_pred","text":"<pre><code>ludwig.visualize.compare_classifiers_performance_from_pred(\n  predictions_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Produces model comparison barplot visualization from predictions.</p> <p>For each model it produces bars in a bar plot, one for each overall metric computed on the fly from the predictions for the specified <code>model_names</code>.</p> <p>Inputs</p> <ul> <li>predictions_per_model (List[str]): path to experiment predictions file.</li> <li>ground_truth (pd.Series): ground truth values</li> <li>metadata (dict): feature metadata dictionary.</li> <li>output_feature_name (str): name of the output feature to visualize.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_performance_subset","title":"compare_classifiers_performance_subset","text":"<pre><code>ludwig.visualize.compare_classifiers_performance_subset(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  labels_limit,\n  subset,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Produces model comparison barplot visualization from train subset.</p> <p>For each model  it produces bars in a bar plot, one for each overall metric computed on the fly from the probabilities predictions for the specified <code>model_names</code>, considering only a subset of the full training set. The way the subset is obtained is using the <code>top_n_classes</code> and <code>subset</code> parameters.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model    probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>top_n_classes (List[int]): list containing the number of classes    to plot.</li> <li>labels_limit (int): upper limit on the numeric encoded label value.    Encoded numeric label values in dataset that are higher than    <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>subset (str): string specifying type of subset filtering.  Valid    values are <code>ground_truth</code> or <code>predictions</code>.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or    list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save    plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots -    <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use    metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_performance_changing_k","title":"compare_classifiers_performance_changing_k","text":"<pre><code>ludwig.visualize.compare_classifiers_performance_changing_k(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  top_k,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Produce lineplot that show Hits@K metric while k goes from 1 to <code>top_k</code>.</p> <p>For each model it produces a line plot that shows the Hits@K metric (that counts a prediction as correct if the model produces it among the first k) while changing k from 1 to top_k for the specified <code>output_feature_name</code>.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>top_k (int): number of elements in the ranklist to consider.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_multiclass_multimetric","title":"compare_classifiers_multiclass_multimetric","text":"<pre><code>ludwig.visualize.compare_classifiers_multiclass_multimetric(\n  test_stats_per_model,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show the precision, recall and F1 of the model for the specified output_feature_name.</p> <p>For each model it produces four plots that show the precision, recall and F1 of the model on several classes for the specified output_feature_name.</p> <p>Inputs</p> <ul> <li>test_stats_per_model (List[dict]): list containing dictionary of evaluation performance statistics</li> <li>metadata (dict): intermediate preprocess structure created during training containing the mappings of the input dataset.</li> <li>output_feature_name (Union[str, <code>None</code>]): name of the output feature to use for the visualization.  If <code>None</code>, use all output features.</li> <li>top_n_classes (List[int]): list containing the number of classes to plot.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#compare_classifiers_predictions","title":"compare_classifiers_predictions","text":"<pre><code>ludwig.visualize.compare_classifiers_predictions(\n  predictions_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show two models comparison of their output_feature_name predictions.</p> <p>Inputs</p> <ul> <li>predictions_per_model (List[list]): list containing the model predictions for the specified output_feature_name.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confidence_thresholding_2thresholds_2d","title":"confidence_thresholding_2thresholds_2d","text":"<pre><code>ludwig.visualize.confidence_thresholding_2thresholds_2d(\n  probabilities_per_model,\n  ground_truths,\n  metadata,\n  threshold_output_feature_names,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show confidence threshold data vs accuracy for two output feature names.</p> <p>The first plot shows several semi transparent lines. They summarize the 3d surfaces displayed by confidence_thresholding_2thresholds_3d that have thresholds on the confidence of the predictions of the two <code>threshold_output_feature_names</code>  as x and y axes and either the data coverage percentage or the accuracy as z axis. Each line represents a slice of the data coverage  surface projected onto the accuracy surface.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[List[np.array], List[pd.Series]]): containing ground truth data</li> <li>metadata (dict): feature metadata dictionary</li> <li>threshold_output_feature_names (List[str]): List containing two output feature names for visualization.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confidence_thresholding_2thresholds_3d","title":"confidence_thresholding_2thresholds_3d","text":"<pre><code>ludwig.visualize.confidence_thresholding_2thresholds_3d(\n  probabilities_per_model,\n  ground_truths,\n  metadata,\n  threshold_output_feature_names,\n  labels_limit,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show 3d confidence threshold data vs accuracy for two output feature names.</p> <p>The plot shows the 3d surfaces displayed by confidence_thresholding_2thresholds_3d that have thresholds on the confidence of the predictions of the two <code>threshold_output_feature_names</code> as x and y axes and either the data coverage percentage or the accuracy as z axis.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[List[np.array], List[pd.Series]]): containing ground truth data</li> <li>metadata (dict): feature metadata dictionary</li> <li>threshold_output_feature_names (List[str]): List containing two output feature names for visualization.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confidence_thresholding","title":"confidence_thresholding","text":"<pre><code>ludwig.visualize.confidence_thresholding(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show models accuracy and data coverage while increasing treshold.</p> <p>For each model it produces a pair of lines indicating the accuracy of the model and the data coverage while increasing a threshold (x axis) on the probabilities of predictions for the specified output_feature_name.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confidence_thresholding_data_vs_acc","title":"confidence_thresholding_data_vs_acc","text":"<pre><code>ludwig.visualize.confidence_thresholding_data_vs_acc(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show models comparison of confidence threshold data vs accuracy.</p> <p>For each model it produces a line indicating the accuracy of the model and the data coverage while increasing a threshold on the probabilities of predictions for the specified output_feature_name. The difference with confidence_thresholding is that it uses two axes instead of three, not visualizing the threshold and having coverage as x axis instead of the threshold.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confidence_thresholding_data_vs_acc_subset","title":"confidence_thresholding_data_vs_acc_subset","text":"<pre><code>ludwig.visualize.confidence_thresholding_data_vs_acc_subset(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  labels_limit,\n  subset,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show models comparison of confidence threshold data vs accuracy on a subset of data.</p> <p>For each model it produces a line indicating the accuracy of the model and the data coverage while increasing a threshold on the probabilities of predictions for the specified output_feature_name, considering only a subset of the full training set. The way the subset is obtained is using the <code>top_n_classes</code> and subset parameters. The difference with confidence_thresholding is that it uses two axes instead of three, not visualizing the threshold and having coverage as x axis instead of the threshold.</p> <p>If the values of subset is <code>ground_truth</code>, then only datapoints where the ground truth class is within the top n most frequent ones will be considered  as test set, and the percentage of datapoints that have been kept  from the original set will be displayed. If the values of subset is <code>predictions</code>, then only datapoints where the the model predicts a class that is within the top n most frequent ones will be considered as test set, and the percentage of datapoints that have been kept from the original set will be displayed for each model.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>top_n_classes (List[int]): list containing the number of classes to plot.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>subset (str): string specifying type of subset filtering.  Valid values are <code>ground_truth</code> or <code>predictions</code>.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#binary_threshold_vs_metric","title":"binary_threshold_vs_metric","text":"<pre><code>ludwig.visualize.binary_threshold_vs_metric(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  metrics,\n  positive_label=1,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show confidence of the model against metric for the specified output_feature_name.</p> <p>For each metric specified in metrics (options are <code>f1</code>, <code>precision</code>, <code>recall</code>, <code>accuracy</code>), this visualization produces a line chart plotting a threshold on  the confidence of the model against the metric for the specified output_feature_name.  If output_feature_name is a category feature, positive_label, which is specified as the numeric encoded value, indicates the class to be considered positive class and all others will be considered negative. To figure out the association between classes and numeric encoded values check the ground_truth_metadata JSON file.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>metrics (List[str]): metrics to display (<code>'f1'</code>, <code>'precision'</code>, <code>'recall'</code>, <code>'accuracy'</code>).</li> <li>positive_label (int, default: <code>1</code>): numeric encoded value for the positive class.</li> <li>model_names (List[str], default: <code>None</code>): list of the names of the models to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (<code>None): (</code>None`)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#roc_curves","title":"roc_curves","text":"<pre><code>ludwig.visualize.roc_curves(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  positive_label=1,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show the roc curves for output features in the specified models.</p> <p>This visualization produces a line chart plotting the roc curves for the specified output feature name. If output feature name is a category feature, <code>positive_label</code> indicates which is the class to be considered positive class and all the others will be considered negative. <code>positive_label</code> is the encoded numeric value for category classes. The numeric value can be determined by association between classes and integers captured in the training metadata JSON file.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>positive_label (int, default: <code>1</code>): numeric encoded value for the positive class.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#roc_curves_from_test_statistics","title":"roc_curves_from_test_statistics","text":"<pre><code>ludwig.visualize.roc_curves_from_test_statistics(\n  test_stats_per_model,\n  output_feature_name,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show the roc curves for the specified models output binary <code>output_feature_name</code>.</p> <p>This visualization uses <code>output_feature_name</code>, <code>test_stats_per_model</code> and <code>model_names</code> parameters. <code>output_feature_name</code> needs to be binary feature. This visualization produces a line chart plotting the roc curves for the specified <code>output_feature_name</code>.</p> <p>Inputs</p> <ul> <li>test_stats_per_model (List[dict]): dictionary containing evaluation performance statistics.</li> <li>output_feature_name (str): name of the output feature to use for the visualization.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#calibration_1_vs_all","title":"calibration_1_vs_all","text":"<pre><code>ludwig.visualize.calibration_1_vs_all(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show models probability of predictions for the specified output_feature_name.</p> <p>For each class or each of the k most frequent classes if top_k is specified,  it produces two plots computed on the fly from the probabilities  of predictions for the specified output_feature_name.</p> <p>The first plot is a calibration curve that shows the calibration of the predictions considering the current class to be the true one and all others  to be a false one, drawing one line for each model (in the aligned  lists of probabilities and model_names).</p> <p>The second plot shows the distributions of the predictions considering the  current class to be the true one and all others to be a false one, drawing the distribution for each model (in the aligned lists of probabilities and model_names).</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>top_n_classes (list): List containing the number of classes to plot.</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (List[str], default: <code>None</code>): list of the names of the models to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>String</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#calibration_multiclass","title":"calibration_multiclass","text":"<pre><code>ludwig.visualize.calibration_multiclass(\n  probabilities_per_model,\n  ground_truth,\n  metadata,\n  output_feature_name,\n  labels_limit,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf',\n  ground_truth_apply_idx=True\n)\n</code></pre> <p>Show models probability of predictions for each class of the specified output_feature_name.</p> <p>Inputs</p> <ul> <li>probabilities_per_model (List[numpy.array]): list of model probabilities.</li> <li>ground_truth (Union[pd.Series, np.ndarray]): ground truth values</li> <li>metadata (dict): feature metadata dictionary</li> <li>output_feature_name (str): output feature name</li> <li>labels_limit (int): upper limit on the numeric encoded label value. Encoded numeric label values in dataset that are higher than <code>labels_limit</code> are considered to be \"rare\" labels.</li> <li>model_names (List[str], default: <code>None</code>): list of the names of the models to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> <li>ground_truth_apply_idx (bool, default: <code>True</code>): whether to use metadata['str2idx'] in np.vectorize</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#confusion_matrix","title":"confusion_matrix","text":"<pre><code>ludwig.visualize.confusion_matrix(\n  test_stats_per_model,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  normalize,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show confusion matrix in the models predictions for each <code>output_feature_name</code>.</p> <p>For each model (in the aligned lists of test_statistics and model_names) it  produces a heatmap of the confusion matrix in the predictions for each  output_feature_name that has a confusion matrix in test_statistics. The value of <code>top_n_classes</code> limits the heatmap to the n most frequent classes.</p> <p>Inputs</p> <ul> <li>test_stats_per_model (List[dict]): dictionary containing evaluation   performance statistics.</li> <li>metadata (dict): intermediate preprocess structure created during training containing the mappings of the input dataset.</li> <li>output_feature_name (Union[str, <code>None</code>]): name of the output feature to use for the visualization.  If <code>None</code>, use all output features.</li> <li>top_n_classes (List[int]): number of top classes or list containing the number of top classes to plot.</li> <li>normalize (bool): flag to normalize rows in confusion matrix.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#frequency_vs_f1","title":"frequency_vs_f1","text":"<pre><code>ludwig.visualize.frequency_vs_f1(\n  test_stats_per_model,\n  metadata,\n  output_feature_name,\n  top_n_classes,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Show prediction statistics for the specified <code>output_feature_name</code> for each model.</p> <p>For each model (in the aligned lists of <code>test_stats_per_model</code> and <code>model_names</code>), produces two plots statistics of predictions for the specified <code>output_feature_name</code>.</p> <p>The first plot is a line plot with one x axis representing the different classes and two vertical axes colored in orange and blue respectively. The orange one is the frequency of the class and an orange line is plotted to show the trend. The blue one is the F1 score for that class and a blue line is plotted to show the trend. The classes on the x axis are sorted by f1 score.</p> <p>The second plot has the same structure of the first one, but the axes are flipped and the classes on the x axis are sorted by frequency.</p> <p>Inputs</p> <ul> <li>test_stats_per_model (List[dict]): dictionary containing evaluation performance statistics.</li> <li>metadata (dict): intermediate preprocess structure created during training containing the mappings of the input dataset.</li> <li>output_feature_name (Union[str, <code>None</code>]): name of the output feature to use for the visualization.  If <code>None</code>, use all output features.</li> <li>top_n_classes (List[int]): number of top classes or list containing the number of top classes to plot.</li> <li>model_names (Union[str, List[str]], default: <code>None</code>): model name or list of the model names to use as labels.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#hyperopt_report","title":"hyperopt_report","text":"<pre><code>ludwig.visualize.hyperopt_report(\n  hyperopt_stats_path,\n  output_directory=None,\n  file_format='pdf'\n)\n</code></pre> <p>Produces a report about hyperparameter optimization creating one graph per hyperparameter to show the distribution of results and one additional graph of pairwise hyperparameters interactions.</p> <p>Inputs</p> <ul> <li>hyperopt_stats_path (str): path to the hyperopt results JSON file.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window.</li> <li>file_format (str, default: <code>'pdf'</code>): file format of output plots - <code>'pdf'</code> or <code>'png'</code>.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/api/visualization/#hyperopt_hiplot","title":"hyperopt_hiplot","text":"<pre><code>ludwig.visualize.hyperopt_hiplot(\n  hyperopt_stats_path,\n  output_directory=None\n)\n</code></pre> <p>Produces a parallel coordinate plot about hyperparameter optimization creating one HTML file and optionally a CSV file to be read by hiplot.</p> <p>Inputs</p> <ul> <li>hyperopt_stats_path (str): path to the hyperopt results JSON file.</li> <li>output_directory (str, default: <code>None</code>): directory where to save plots. If not specified, plots will be displayed in a window.</li> </ul> <p>Return</p> <ul> <li>return (Non): (None)</li> </ul> <p>DeveloperAPI: This API may change across minor Ludwig releases.</p>"},{"location":"user_guide/datasets/data_postprocessing/","title":"Data Postprocessing","text":"<p>The JSON metadata file obtained during preprocessing is also used for postprocessing: Ludwig models return output predictions and, depending on their datatype they are mapped back into raw data.</p> <p>Number and timeseries do not require additional transformations and are returned as they are, directly from the model.</p> <p>Category, set, sequence, and text features are represented in the model as integers. These predictions are mapped back into the original tokens / names using the <code>idx2str</code> in the JSON file.</p> <p>Users running <code>experiment</code> or <code>predict</code> will find multiple prediction results files: 1) a CSV file for each output containing the mapped predictions, 2) a probability CSV file containing the probability of that prediction, 3) a probabilities CSV file containing the probabilities for all alternatives (for instance, the probabilities of all the categories in case of a categorical feature).</p> <p>Users will also get the raw unmapped predictions from the model as NPY files. If you don't need them users can use the <code>--skip_save_unprocessed_output</code> argument.</p>"},{"location":"user_guide/datasets/data_preprocessing/","title":"Data Preprocessing","text":""},{"location":"user_guide/datasets/data_preprocessing/#overview","title":"Overview","text":"<p>Ludwig data preprocessing performs a few different operations on the incoming dataset:</p> <ol> <li>Computing metadata like vocabulary, vocabulary size, and sequence lengths. This allows Ludwig to create    dictionaries like <code>idx2str</code> or <code>str2idx</code> to map between raw data values to tensor values.</li> <li>Handling missing values any rows/examples that have missing feature values are filled in with constants or other    example-derived values (see Type-Global Preprocesing Configuration).</li> <li>(optional) Splitting dataset into train, validation, and test based on splitting percentages, or using explicitly    specified splits.</li> <li>(optional) Balancing data which can be useful for datasets with heavily underrepresented or overrepresented    classes.</li> </ol> <p>Data preprocessing maps raw data to two files: 1) an processed dataset file containing tensors (HDF5 when running locally, Parquet when running on Ray) and 2) a JSON file of metadata. The processed dataset and metadata files are saved in the cache directory (defaults to the same directory as the input dataset), unless <code>--skip_save_processed_input</code> is used. The two files will serve as a cache to help avoid performing the same preprocessing again for subsequent experiments, which can be time consuming.</p> <p>The preprocessing process is highly customizable via the Type-Global Preprocessing Section of the Ludwig config. The basic assumption is always that all data is UTF-8 encoded and contains one row for each example and one column for each feature.</p> <p>It's helpful to assign types to each feature. Some types assume a specific format, and different types will have different ways of mapping raw data into tensors. From v0.5, users also have the option to rely on Ludwig AutoML to assign types automatically.</p>"},{"location":"user_guide/datasets/data_preprocessing/#preprocessing-for-different-data-types","title":"Preprocessing for different data types","text":"<p>Each datatype is preprocessed in a different way, using different parameters and different tokenizers. Details on how to set those parameters for each feature type and for each specific feature is described in the Configuration - Defaults - Type-Global Preprocessing section.</p>"},{"location":"user_guide/datasets/data_preprocessing/#binary-features","title":"Binary features","text":"<p><code>Binary</code> features are directly transformed into a binary valued vector of length <code>n</code> (where <code>n</code> is the size of the dataset) and added to the processed dataset with a key that reflects the name of column in the dataset. No additional information about them is available in the JSON metadata file.</p>"},{"location":"user_guide/datasets/data_preprocessing/#number-features","title":"Number features","text":"<p><code>Number</code> features are directly transformed into a float valued vector of length <code>n</code> (where <code>n</code> is the size of the dataset) and added to the processed dataset with a key that reflects the name of column in the dataset. No additional information about them is available in the JSON metadata file.</p>"},{"location":"user_guide/datasets/data_preprocessing/#category-features","title":"Category features","text":"<p><code>Category</code> features are transformed into an integer valued vector of size <code>n</code> (where <code>n</code> is the size of the dataset) and added to the processed dataset with a key that reflects the name of column in the dataset.</p> <p>The way categories are mapped into integers consists of first collecting a dictionary of all the unique category strings present in the column of the dataset, then rank them by frequency and then assign them an increasing integer ID from the most frequent to the most rare (with 0 being assigned to a <code>&lt;UNK&gt;</code> token).  The column name is added to the JSON file, with an associated dictionary containing:</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the size of the set of all tokens (<code>vocab_size</code>)</li> <li>additional preprocessing information (by default how to fill missing values    and what token to use to fill missing values)</li> </ol>"},{"location":"user_guide/datasets/data_preprocessing/#set-features","title":"Set features","text":"<p><code>Set</code> features are transformed into a binary (int8 actually) valued matrix of size <code>n x l</code> (where <code>n</code> is the size of the dataset and <code>l</code> is the minimum of the size of the biggest set and a <code>max_size</code> parameter) and added to processed dataset with a key that reflects the name of column in the dataset.</p> <p>The way sets are mapped into integers consists in first using a tokenizer to map from strings to sequences of set items (by default this is done by splitting on spaces).  Then a dictionary of all the different set item strings present in the column of the dataset is collected, then they are ranked by frequency and an increasing integer ID is assigned to them from the most frequent to the most rare (with 0 being assigned to <code>&lt;PAD&gt;</code> used for padding and 1 assigned to <code>&lt;UNK&gt;</code> item).  The column name is added to the JSON file, with an associated dictionary containing:</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the maximum size of all sets (<code>max_set_size</code>)</li> <li>additional preprocessing information (by default how to fill missing values    and what token to use to fill missing values)</li> </ol>"},{"location":"user_guide/datasets/data_preprocessing/#bag-features","title":"Bag features","text":"<p><code>Bag</code> features are treated in the same way of set features, with the only difference being that the matrix had float values (frequencies).</p>"},{"location":"user_guide/datasets/data_preprocessing/#sequence-features","title":"Sequence Features","text":"<p>Sequence features by default are managed by <code>space</code> tokenizers. This splits the content of the feature value into a list of strings using space.</p> before tokenizer after tokenizer \"token3 token4 token2\" [token3, token4, token2] \"token3 token1\" [token3, token1] <p>Computing metadata: A list <code>idx2str</code> and two dictionaries <code>str2idx</code> and <code>str2freq</code> are created containing all the tokens in all the lists obtained by splitting all the rows of the column and an integer id is assigned to each of them (in order of frequency).</p> <pre><code>{\n\"column_name\": {\n\"idx2str\": [\n\"&lt;PAD&gt;\",\n\"&lt;UNK&gt;\",\n\"token3\",\n\"token2\",\n\"token4\",\n\"token1\"\n],\n\"str2idx\": {\n\"&lt;PAD&gt;\": 0,\n\"&lt;UNK&gt;\": 1,\n\"token3\": 2,\n\"token2\": 3,\n\"token4\": 4,\n\"token1\": 5\n},\n\"str2freq\": {\n\"&lt;PAD&gt;\":  0,\n\"&lt;UNK&gt;\":  0,\n\"token3\": 2,\n\"token2\": 1,\n\"token4\": 1,\n\"token1\": 1\n}\n}\n}\n</code></pre> <p>Finally, a numpy matrix is created with sizes <code>n x l</code> where <code>n</code> is the number of rows in the column and <code>l</code> is the minimum of the longest tokenized list and a <code>max_length</code> parameter that can be set. All sequences shorter than <code>l</code> are right-padded to the <code>max_length</code> (though this behavior may also be modified through a parameter).</p> after tokenizer numpy matrix [token3, token4, token2] 2 4 3 [token3, token1] 2 5 0 <p>The final result matrix is saved in the processed dataset with the name of the original column in the dataset as key, while the mapping from token to integer ID (and its inverse mapping) is saved in the JSON file.</p> <p>A frequency-ordered vocabulary dictionary is created which maps tokens to integer IDs. Special symbols like <code>&lt;PAD&gt;</code>, <code>&lt;START&gt;</code>, <code>&lt;STOP&gt;</code>, and <code>&lt;UNK&gt;</code> have specific indices. By default, we use <code>[0, 1, 2, 3]</code>, but these can be overridden manually.</p> <p>If a <code>huggingface</code> encoder is specified, then that encoder's special symbol indices will be used instead.</p> <p>The computed metadata includes:</p> <ol> <li>the mapping from integer to string (<code>idx2str</code>)</li> <li>the mapping from string to id (<code>str2idx</code>)</li> <li>the mapping from string to frequency (<code>str2freq</code>)</li> <li>the maximum length of all sequences (<code>max_sequence_length</code>)</li> <li>additional preprocessing information (by default how to fill missing values    and what token to use to fill missing values)</li> </ol>"},{"location":"user_guide/datasets/data_preprocessing/#text-features","title":"Text features","text":"<p><code>Text</code> features are treated in the same way of sequence features, with a couple differences. Two different tokenizations happen, one that splits at every character and one that uses a custom tokenizer. Two different keys are added to the processed dataset file, one for the matrix of characters and one for the matrix of symbols.</p> <p>The same thing happens in the JSON file, where there are two sets of dictionaries, one for mapping characters to integers (and the inverse) and symbols to integers (and their inverse).</p> <p>If a <code>huggingface</code> encoder is specified, then that encoder's tokenizer will be used for the symbol-based tokenizer.</p> <p>In the configuration users can specify which level of representation to use: the character level or the symbol level.</p>"},{"location":"user_guide/datasets/data_preprocessing/#timeseries-features","title":"Timeseries features","text":"<p><code>Timeseries</code> features are treated in the same way of sequence features, with the only difference being that the matrix in the processed dataset file does not have integer values, but float values. The JSON file has no additional mapping information.</p>"},{"location":"user_guide/datasets/data_preprocessing/#image-features","title":"Image features","text":"<p><code>Image</code> features are transformed into a int8 valued tensor of size <code>n x h x w x c</code> (where <code>n</code> is the size of the dataset and <code>h x w</code> is a specific resizing of the image that can be set, and <code>c</code> is the number of color channels) and added to processed dataset with a key that reflects the name of column in the dataset.</p> <p>The column name is added to the JSON file, with an associated dictionary containing preprocessing information about the sizes of the resizing.</p>"},{"location":"user_guide/datasets/dataset_zoo/","title":"Dataset Zoo","text":"<p>The Ludwig Dataset Zoo provides datasets that can be directly plugged into a Ludwig model.</p> <p>The simplest way to use a dataset is to reference it as a URI when specifying the training set:</p> <pre><code>ludwig train --dataset ludwig://reuters ...\n</code></pre> <p>Any Ludwig dataset can be specified as a URI of the form <code>ludwig://&lt;dataset&gt;</code>.</p> <p>Datasets can also be programatically imported and loaded into a Pandas DataFrame using the <code>.load()</code> method:</p> <pre><code>from ludwig.datasets import reuters\n\n# Loads into single dataframe with a 'split' column:\ndataset_df = reuters.load()\n\n# Loads into split dataframes:\ntrain_df, test_df, _ = reuters.load(split=True)\n</code></pre> <p>The <code>ludwig.datasets</code> API also provides functions to list, describe, and get datasets.  For example:</p> <pre><code>import ludwig.datasets\n\n# Gets a list of all available dataset names.\ndataset_names = ludwig.datasets.list_datasets()\n\n# Prints the description of the titanic dataset.\nprint(ludwig.datasets.describe_dataset(\"titanic\"))\n\ntitanic = ludwig.datasets.get_dataset(\"titanic\")\n\n# Loads into single dataframe with a 'split' column:\ndataset_df = titanic.load()\n\n# Loads into split dataframes:\ntrain_df, test_df, _ = titanic.load(split=True)\n</code></pre>"},{"location":"user_guide/datasets/dataset_zoo/#kaggle-datasets","title":"Kaggle Datasets","text":"<p>Some datasets are hosted on Kaggle and require a kaggle account. To use these, you'll need to set up Kaggle credentials in your environment. If the dataset is part of a Kaggle competition, you'll need to accept the terms on the competition page.</p> <p>To check programmatically, datasets have an <code>.is_kaggle_dataset</code> property.</p>"},{"location":"user_guide/datasets/dataset_zoo/#downloading-processing-and-exporting","title":"Downloading, Processing, and Exporting","text":"<p>Datasets are first downloaded into <code>LUDWIG_CACHE</code>, which may be set as an environment variable and defaults to <code>$HOME/.ludwig_cache</code>.</p> <p>Datasets are automatically loaded, processed, and re-saved as parquet files in the cache.</p> <p>To export the processed dataset, including any files it depends on, use the <code>.export(output_directory)</code> method. This is recommended if the dataset contains media files like images or audio files. File paths are relative to the working directory of the training process.</p> <pre><code>from ludwig.datasets import twitter_bots\n\n# Exports twitter bots dataset and image files to the current working directory.\ntwitter_bots.export(\".\")\n</code></pre>"},{"location":"user_guide/datasets/dataset_zoo/#end-to-end-example","title":"End-to-end Example","text":"<p>Here's an end-to-end example of training a model using the MNIST dataset:</p> <pre><code>from ludwig.api import LudwigModel\nfrom ludwig.datasets import mnist\n\n# Initializes a Ludwig model\nconfig = {\n    \"input_features\": [{\"name\": \"image_path\", \"type\": \"image\"}],\n    \"output_features\": [{\"name\": \"label\", \"type\": \"category\"}],\n}\nmodel = LudwigModel(config)\n\n# Loads and splits MNIST dataset\ntraining_set, test_set, _ = mnist.load(split=True)\n\n# Exports the mnist image files to the current working directory.\nmnist.export(\".\")\n\n# Runs model training\ntrain_stats, _, _ = model.train(training_set=training_set, test_set=test_set, model_name=\"mnist_model\")\n</code></pre>"},{"location":"user_guide/datasets/dataset_zoo/#dataset-splits","title":"Dataset Splits","text":"<p>All datasets in the dataset zoo are provided with a default train/validation/test split. When loading with <code>split=False</code>, the default split will be returned (and is guaranteed to be the same every time). With <code>split=True</code>, Ludwig will randomly re-split the dataset.</p> <p>Note</p> <p>Some benchmark or contest datasets are released with held-out test set labels. In other words, the train and validation splits have labels, but the test set does not. Most Kaggle contest datasets have this unlabeled test set.</p> <p>Splits:</p> <ul> <li>train: Data to train on. Required, must have labels.</li> <li>validation: Subset of dataset to evaluate while training. Optional, must have labels.</li> <li>test: Held out from model development, used for later testing. Optional, may not be labeled.</li> </ul>"},{"location":"user_guide/datasets/dataset_zoo/#zoo-datasets","title":"Zoo Datasets","text":"<p>Here is the list of the currently available datasets:</p> Dataset Hosted On Description adult_census_income archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/adult. Whether a person makes over $50K a year or not. allstate_claims_severity Kaggle https://www.kaggle.com/c/allstate-claims-severity amazon_employee_access_challenge Kaggle https://www.kaggle.com/c/amazon-employee-access-challenge agnews Github https://search.r-project.org/CRAN/refmans/textdata/html/dataset_ag_news.html allstate_claims_severity Kaggle https://www.kaggle.com/c/allstate-claims-severity amazon_employee_access_challenge Kaggle https://www.kaggle.com/c/amazon-employee-access-challenge amazon_review_polarity S3 https://paperswithcode.com/sota/sentiment-analysis-on-amazon-review-polarity amazon_reviews S3 https://s3.amazonaws.com/amazon-reviews-pds/readme.html ames_housing Kaggle https://www.kaggle.com/c/ames-housing-data bbc_news Kaggle https://www.kaggle.com/c/learn-ai-bbc bnp_claims_management Kaggle https://www.kaggle.com/c/bnp-paribas-cardif-claims-management connect4 Kaggle https://www.kaggle.com/c/connectx/discussion/124397 creditcard_fraud Kaggle https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud dbpedia S3 https://paperswithcode.com/dataset/dbpedia electricity S3 Predict electricity demand from day of week and outside temperature. ethos_binary Github https://github.com/huggingface/datasets/blob/master/datasets/ethos/README.md fever S3 https://arxiv.org/abs/1803.05355 flickr8k Github https://www.kaggle.com/adityajn105/flickr8k forest_cover archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/covertype goemotions Github https://arxiv.org/abs/2005.00547 higgs archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/HIGGS ieee_fraud Kaggle https://www.kaggle.com/c/ieee-fraud-detection imbalanced_insurance Kaggle https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice imdb Kaggle https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews insurance_lite Kaggle https://www.kaggle.com/infernape/fast-furious-and-insured iris archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/iris irony Github https://github.com/bwallace/ACL-2014-irony kdd_appetency kdd.org https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data kdd_churn kdd.org https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data kdd_upselling kdd.org https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data mnist yann.lecun.com http://yann.lecun.com/exdb/mnist/ mushroom_edibility archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/mushroom naval archive.ics.uci.edu https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/24098 noshow_appointments Kaggle https://www.kaggle.com/datasets/joniarroba/noshowappointments numerai28pt6 Kaggle https://www.kaggle.com/numerai/encrypted-stock-market-data-from-numerai ohsumed_7400 Kaggle https://www.kaggle.com/datasets/weipengfei/ohr8r52 ohsumed_cmu boston.lti.cs.cmu.edu http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW2/ otto_group_product Kaggle https://www.kaggle.com/c/otto-group-product-classification-challenge poker_hand archive.ics.uci.edu https://archive.ics.uci.edu/ml/datasets/Poker+Hand porto_seguro_safe_driver Kaggle https://www.kaggle.com/c/porto-seguro-safe-driver-prediction protein archive.ics.uci.edu https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2932-0 reuters_cmu boston.lti.cs.cmu.edu http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW2/ reuters_r8 Kaggle Reuters R8 subset of Reuters 21578 dataset from Kaggle. rossmann_store_sales Kaggle https://www.kaggle.com/c/rossmann-store-sales santander_customer_satisfaction Kaggle https://www.kaggle.com/c/santander-customer-satisfaction santander_customer_transaction_prediction Kaggle https://www.kaggle.com/c/santander-customer-transaction-prediction santander_value_prediction Kaggle https://www.kaggle.com/c/santander-value-prediction-challenge sarcos gaussianprocess.org http://www.gaussianprocess.org/gpml/data/ sst2 nlp.stanford.edu https://paperswithcode.com/dataset/sst sst3 nlp.stanford.edu Merging very negative and negative, and very positive and positive classes. sst5 nlp.stanford.edu https://paperswithcode.com/dataset/sst synthetic_fraud Kaggle https://www.kaggle.com/ealaxi/paysim1 temperature Kaggle https://www.kaggle.com/selfishgene/historical-hourly-weather-data titanic Kaggle https://www.kaggle.com/c/titanic walmart_recruiting Kaggle https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting wmt15 Kaggle https://www.kaggle.com/dhruvildave/en-fr-translation-dataset yahoo_answers S3 Question classification. yelp_review_polarity S3 https://www.yelp.com/dataset. Predict the polarity or sentiment of a yelp review. yelp_reviews S3 https://www.yelp.com/dataset yosemite Github https://github.com/ourownstory/neural_prophet Yosemite temperatures dataset."},{"location":"user_guide/datasets/dataset_zoo/#adding-datasets","title":"Adding datasets","text":"<p>To add a dataset to the Ludwig Dataset Zoo, see Add a Dataset.</p>"},{"location":"user_guide/datasets/supported_formats/","title":"Supported Formats","text":"<p>Ludwig is able to read UTF-8 encoded data from 14 file formats. Supported formats are:</p> <ul> <li>Comma Separated Values (<code>csv</code>)</li> <li>Excel Workbooks (<code>excel</code>)</li> <li>Feather (<code>feather</code>)</li> <li>Fixed Width Format (<code>fwf</code>)</li> <li>Hierarchical Data Format 5 (<code>hdf5</code>)</li> <li>Hypertext Markup Language (<code>html</code>) Note: limited to single table in the file.</li> <li>JavaScript Object Notation (<code>json</code> and <code>jsonl</code>)</li> <li>Parquet (<code>parquet</code>)</li> <li>Pickled Pandas DataFrame (<code>pickle</code>)</li> <li>SAS data sets in XPORT or SAS7BDAT format (<code>sas</code>)</li> <li>SPSS file (<code>spss</code>)</li> <li>Stata file (<code>stata</code>)</li> <li>Tab Separated Values (<code>tsv</code>)</li> </ul> <p>Ludwig uses Pandas and Dask under the hood to read the UTF-8 encoded dataset files, which allows support for CSV, Excel, Feather, fwf, HDF5, HTML (containing a <code>&lt;table&gt;</code>), JSON, JSONL, Parquet, pickle (pickled Pandas DataFrame), SAS, SPSS, Stata and TSV formats. Ludwig tries to automatically identify the format by the extension.</p> <p>In case a *SV file is provided, Ludwig tries to identify the separator (generally <code>,</code>) from the data. The default escape character is <code>\\</code>. For example, if <code>,</code> is the column separator and one of your data columns has a <code>,</code> in it, Pandas would fail to load the data properly. To handle such cases, we expect the values in the columns to be escaped with backslashes (replace <code>,</code> in the data with <code>\\,</code>).</p>"},{"location":"user_guide/distributed_training/","title":"Distributed Training","text":"<p>For large datasets, training on a single machine storing the entire dataset in memory can be prohibitively expensive. As such, Ludwig supports distributing the preprocessing, training, and prediction steps across multiple machines and GPUs to operate on separate partitions of the data in parallel.</p> <p></p> <p>Ludwig supports two different distributed execution backends: Ray and Horovod / MPI. In most cases, we recommend using Ray (supporting both distributed data processing and distributed training at once), but native Horovod execution is also supported, particularly for users accustomed to running with MPI.</p>"},{"location":"user_guide/distributed_training/#ray","title":"Ray","text":"<p>Ray is a framework for distributed computing that makes it easy to scale up code that runs on your local machine to execute in parallel across a cluster.</p> <p>Ludwig has native integration with Ray for both hyperparameter search and distributed training.</p> <p>Running with Ray has several advantages over local execution:</p> <ul> <li>Ray enables you to provision a cluster of machines in a single command through its cluster launcher.</li> <li>Horovod on Ray allows you to do distributed training without needing to configure MPI in your environment.</li> <li>Dask on Ray allows you to process large datasets that don't fit in memory on a single machine.</li> <li>Ray Tune allows you to easily run distributed hyperparameter search across many machines in parallel.</li> <li>Ray provides easy access to high performance instances like high memory or GPU machines in the cloud.</li> </ul> <p>All of this comes for free without changing a single line of code in Ludwig. When Ludwig detects that you're running within a Ray cluster, the Ray backend will be enabled automatically. You can also enable the Ray backend explicitly either through the command line:</p> <pre><code>ludwig train ... --backend ray\n</code></pre> <p>Or in the Ludwig config:</p> <pre><code>backend:\ntype: ray\nprocessor:\ntype: dask\ntrainer:\nstrategy: horovod\n</code></pre>"},{"location":"user_guide/distributed_training/#running-ludwig-with-ray","title":"Running Ludwig with Ray","text":"<p>To use the Ray with Ludwig, you will need to have a running Ray cluster. The simplest way to start a Ray cluster is to use the Ray cluster launcher, which can be installed locally with <code>pip</code>:</p> <pre><code>pip install ray\n</code></pre> <p>Starting a Ray cluster requires that you have access to a cloud instance provider like AWS EC2 or Kubernetes.</p> <p>Here's an example of a partial Ray cluster configuration YAML file you can use to create your Ludwig Ray cluster:</p> <pre><code>cluster_name: ludwig-ray-gpu-latest\n\nmin_workers: 4\nmax_workers: 4\n\ndocker:\nimage: \"ludwigai/ludwig-ray-gpu:latest\"\ncontainer_name: \"ray_container\"\n\nhead_node:\nInstanceType: m5.2xlarge\nImageId: latest_dlami\n\nworker_nodes:\nInstanceType: g4dn.2xlarge\nImageId: latest_dlami\n</code></pre> <p>This configuration runs on AWS EC2 instances, with a CPU head node and 4 GPU (Nvidia T4) worker nodes. Every worker runs within a Docker image that provides Ludwig and its dependencies, including Ray, Dask, Horovod, etc. You can use one of these pre-built Docker images as the parent image for your cluster. Ludwig provides both CPU and GPU images ready for use with Ray.</p> <p>Once your Ray cluster is configured, you can start the cluster and submit your existing <code>ludwig</code> commands or Python files to Ray for distributed execution:</p> <pre><code>ray up cluster.yaml\nray submit cluster.yaml \\\nludwig train --config config.yaml --dataset s3://mybucket/dataset.parquet\n</code></pre>"},{"location":"user_guide/distributed_training/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/distributed_training/#cloud-storage","title":"Cloud Storage","text":"<p>In order for Ray to preprocess the input <code>dataset</code>, the dataset file path must be readable from every worker. There are a few ways to achieve this:</p> <ul> <li>Replicate the input dataset to the local filesystem of every worker (suitable for small datasets).</li> <li>Use a network mounted filesystem like NFS.</li> <li>Use an object storage system like Amazon S3.</li> </ul> <p>In most cases, we recommend using an object storage system such as S3 (AWS), GCS (GCP), or ADLS (Azure).</p> <p>To connect to one of these systems from Ludwig you need two things:</p> <ol> <li> <p>Install the appropriate filesystem driver package into your Python environment:</p> <pre><code>s3fs   # S3\nadlfs  # Azure Storage\ngcsfs  # GCS\n</code></pre> </li> <li> <p>Mount your credentials file or set the correct environment variables (example: S3) in your container.</p> </li> </ol> <p>See Cloud Storage for more detailed instructions for each major filesystem.</p>"},{"location":"user_guide/distributed_training/#autoscaling-clusters","title":"Autoscaling Clusters","text":"<p>By default, Ludwig on Ray will attempt to use all available GPUs for distributed training. However, if running in an autoscaling clusters there may not be any GPUs in the cluster at the time Ludwig performs its check. In such cases, we recommend setting the number of GPU workers explicitly in the config.</p> <p>For example, to train with 4 GPUs:</p> <pre><code>backend:\ntrainer:\nuse_gpu: true\nnum_workers: 4\n</code></pre> <p>When using Hyperopt in an autoscaling cluster, you should set <code>max_concurrent_trials</code> and <code>gpu_resources_per_trial</code>, otherwise Ludwig will similarly underestimate how many trials can fit in the fully autoscaled cluster at a time:</p> <pre><code>hyperopt:\nexecutor:\nmax_concurrent_trials: 4\ngpu_resources_per_trial: 1\n</code></pre>"},{"location":"user_guide/distributed_training/#horovod-mpi","title":"Horovod / MPI","text":"<p>You can distribute the training and prediction of your models using Horovod, which supports training on a single machine with multiple GPUs as well as on multiple machines with multiple GPUs.</p> <p>In order to use distributed training you have to install Horovod as detailed in Horovod's installation instructions. If you wish to use MPI, be sure to install OpenMPI or another implementation before installing Horovod:</p> <pre><code>pip install horovod mpi4py\n</code></pre> <p>Horovod works, in practice, by increasing the batch size and distributing a part of each batch to a different node and collecting the gradients from all the nodes in a smart and scalable way. It also adjusts the learning rate to balance the increase in the batch size. The advantage is that training speed scales almost linearly with the number of nodes.</p> <p><code>experiment</code>, <code>train</code> and <code>predict</code> commands accept a <code>--backend=horovod</code> argument that instructs the model building, training and prediction phases to be conducted using Horovod in a distributed way. A <code>horovodrun</code> command specifying which machines and / or GPUs to use, together with a few more parameters, must be provided before the call to Ludwig's command. For example, to train a Ludwig model on a local machine with four GPUs one you can run:</p> <pre><code>horovodrun -np 4 \\\n    ludwig train ...other Ludwig parameters...\n</code></pre> <p>To train on four remote machines with four GPUs each you can run:</p> <pre><code>horovodrun -np 16 \\\n    -H server1:4,server2:4,server3:4,server4:4 \\\n    ludwig train ...other Ludwig parameters...\n</code></pre> <p>The same applies to <code>experiment</code>, <code>predict</code> and <code>test</code>.</p> <p>More details on Horovod installation and run parameters can be found in Horovod's documentation.</p>"},{"location":"user_guide/distributed_training/finetuning/","title":"Fine-Tuning Pretrained Models","text":"<p>Fine-tuning is the process of taking a model previously trained one dataset, and adapting it to a more specialized dataset / task. Typically the original dataset is very large and very general (for example: a crawl of a large portion of the public Internet), and consequently the models are very large in order to reason about all this information (billions of parameters or more).</p> <p>Libraries like HuggingFace's transformers provide acess to state-of-the-art pretrained models that can be used as input feature encoders in Ludwig, allowing you to take advantage of these large pretrained models and adapt them to solve your specific tasks, combining them with other domain-specific features like tabular metadata to create powerful multi-modal model architectures.</p> <p>Ludwig's default configuration is designed to be fast and flexible, and as such, there are a few adjustments to the default configuration parameters we suggest making when fine-tuning a pretraine model. The sections below show examples of configurations we've found to give good results, along with the rationale behind each overridden parameter.</p>"},{"location":"user_guide/distributed_training/finetuning/#suggested-configuration","title":"Suggested Configuration","text":"<p>The below partial configuration shows the \"full fine-tuning configuration\" including trainable weights, batch size set to maximize throughput, and learning rate warmup / decay:</p> <pre><code>defaults:\ntext:\nencoder:\ntype: bert\ntrainable: true\ntrainer:\nepochs: 5\nbatch_size: auto\nlearning_rate: 0.00001\nlearning_rate_scheduler:\nwarmup_fraction: 0.2\ndecay: linear\noptimizer:\ntype: adamw\nuse_mixed_precision: true\n</code></pre> <p>If you're looking to get the best performance you can out of the model, and are insensitive to training time, this is a good place to start. In the sections below, we'll also cover options that tradeoff some potential performance in favor of large speedups to the training throughput.</p>"},{"location":"user_guide/distributed_training/finetuning/#feature-encoder-and-preprocessing","title":"Feature Encoder and Preprocessing","text":""},{"location":"user_guide/distributed_training/finetuning/#features-supporting-fine-tuning","title":"Features Supporting Fine-Tuning","text":""},{"location":"user_guide/distributed_training/finetuning/#text-encoders","title":"Text Encoders","text":"<pre><code>type: text\nencoder:\ntype: bert\nuse_pretrained: true\n</code></pre> <p>All of the HuggingFace encoders in Ludwig can be used for fine-tuning when <code>use_pretrained=true</code> in the encoder config (default). If there is a specific model you want to use, but don't see it listed, you can use the <code>auto_transformer</code> encoder in conjunction with providing the model name in the <code>pretrained_model_name_or_path</code> parameter.</p>"},{"location":"user_guide/distributed_training/finetuning/#image-encoders","title":"Image Encoders","text":"<pre><code>type: image\nencoder:\ntype: resnet\nuse_pretrained: true\n</code></pre> <p>All of the Torchvision pretrained models in Ludwig can be used for fine-tuning when <code>use_pretrained-true</code> in the encoder config (default).</p>"},{"location":"user_guide/distributed_training/finetuning/#trainable","title":"Trainable","text":"<pre><code>encoder:\ntrainable: true\n</code></pre> <p>Ludwig currently supports two variations on fine-tuning, configured via the <code>trainable</code> encoder parameter:</p> <ol> <li>Modifying the weights of the pretrained encoder to adapt them to the downstream task (<code>trainable=true</code>).</li> <li>Keeping the pretrained encoder weights fixed and training a stack of dense layers that sit downstream as the combiner and decoder modules (<code>trainable=false</code>, default). This is sometimes distinguished as transfer learning.</li> </ol> <p>Allowing the weights to be modified by setting <code>trainable=true</code> can significantly improve performance on the downstream task, but will take significantly longer to train (due to the additional backward passes over the pretrained model parameters). Additionally, more care needs to be taken when selecting hyperparameters when <code>trainable=true</code> to prevent catastrophic forgettng, whereby the model forgets all of the valuable information it learned during pretraining. We cover some useful techniques to help prevent this in the Trainer section below, but in summary:</p> <ul> <li>Select a low <code>learning_rate</code>.</li> <li>Use learning rate warmup by setting <code>warmup_fraction</code>.</li> <li>Use learning rate decay by setting <code>decay</code>.</li> <li>Limit the total number of training <code>epochs</code> to 3 to 10.</li> </ul> <p>Keeping the weights frozen will speed up training considerably, particularly when caching encoder embeddings (see below).</p>"},{"location":"user_guide/distributed_training/finetuning/#cache-encoder-embeddings","title":"Cache Encoder Embeddings","text":"<pre><code>name: sentence\ntype: text\nencoder:\ntype: bert\ntrainable: false\npreprocessing:\ncache_encoder_embeddings: true\n</code></pre> <p>If you choose to set <code>trainable=false</code> to keep the weights fixed during fine-tuning, we highly recommend setting <code>cache_encoder_embeddings=true</code> in the preprocessing section of the feature config, which moves the forward pass of the encoder that generates text embeddings from tokenized inputs into the preprocessing portion of the Ludwig training workflow, removing this step entirely from training. In practice, this can speed up training during fine-tuning by over 50x.</p> <p></p> <p>When the embeddings are cached in the preprocessed data, we replace the respective encoder in the ECD model at training time with a shim \u201cSkip Encoder\u201d that passes the input tensor data directly into the downstream combiner for concatenation with other input features. At prediction time and when exporting the model to TorchScript, the Skip Encoder is replaced with the original encoder and its pretrained weights.</p> <p>While embedding caching is useful for a single model training run because it pays the cost of the forward pass on the large pretrained model for only a single epoch, it can be even more valuable in subsequent experiments. Because Ludwig caches preprocessed data and reuses it for new model training experiments (when preprocessing parameters are the same), any subsequent training runs for the same dataset will require no forward passes on the pretrained model whatsoever.</p>"},{"location":"user_guide/distributed_training/finetuning/#trainer","title":"Trainer","text":""},{"location":"user_guide/distributed_training/finetuning/#epochs","title":"Epochs","text":"<pre><code>trainer:\nepochs: 5\n</code></pre> <p>We recommend keeping the number of training epochs to between 3 and 10 when fine-tuning. If you set <code>trainable=true</code>, we recommend setting it closer to 3 to 5 epochs. This is to help prevent catastrophic forgetting from happening during the later epochs, as learning rate decay will kick in too slowly when the prescribed number of epochs is too high.</p> <p>Additionaly, training for the default 100 epochs could be prohibitively time consuming when training on a single GPU.</p>"},{"location":"user_guide/distributed_training/finetuning/#batch-size","title":"Batch Size","text":"<pre><code>trainer:\nbatch_size: auto\n</code></pre> <p>We recommend keeping the default <code>batch_size=auto</code> to maximize GPU utilization during training. If a lower batch size is selected, training will progress much slower and be more expensive, as GPU cycles will be wasted.</p> <p>In practice, pretrained models tend to be less senstive to training with larger batch sizes than many smaller model architectures. However, if you experience issues with convergence when training with larger batch sizes, we recommend enabling ghost batch normalization in the combiner or decoder:</p> <pre><code>combiner:\ntype: concat\nnorm: ghost\n</code></pre>"},{"location":"user_guide/distributed_training/finetuning/#learning-rate","title":"Learning Rate","text":""},{"location":"user_guide/distributed_training/finetuning/#base-learning-rate","title":"Base learning rate","text":"<pre><code>trainer:\nlearning_rate: auto\n</code></pre> <p>Use a very small larning rate when fine-tuning to avoid catastrophic forgetting of all the pretrained model's previously learned knowledge.</p> <ul> <li>When <code>trainable=true</code> we recommend starting as low as <code>learning_rate=0.00001</code>.</li> <li>When <code>trainable=false</code> we recommend starting with <code>learning_rate=0.00002</code>.</li> </ul> <p>Note that setting <code>learning_rate=auto</code> will automatically set the above defaults on your behalf based on the selected model architecture.</p>"},{"location":"user_guide/distributed_training/finetuning/#learning-rate-schedule","title":"Learning rate schedule","text":"<pre><code>trainer:\nepochs: 5\nlearning_rate_scheduler:\nwarmup_fraction: 0.2\ndecay: linear\n</code></pre> <p>It's important to both warmup the learning rate (particularly when using distributed training) and decay it to avoid catastrophic forgetting.</p> <p>A <code>warmup_fraction</code> of <code>0.2</code> will result in 20% of the total training steps being spent linearly scaling the learning rate up from 0 to the initial value provided in <code>trainer.learning_rate</code>. This is useful as otherwise the learning process may over-correct the weights of the pretrained model in the early stages of training.</p> <p>Using <code>linear</code> decay is a very aggressive decay strategy that linearly reduces the learning rate down to 0 as training approaches the final epoch. The decay will only start after the learning rate warmup period has finished and the learning rate is set to its initial value.</p> <p>Both warmup and decay are affected by the total <code>epochs</code> set in the <code>trainer</code> config, so it's important to make sure the <code>epochs</code> are set to a sufficiently low value for the warmup and decay to be effective. If <code>epochs</code> is left at the default value of <code>100</code>, then too much time will be spent in warmup and the decay will not be noticeable.</p>"},{"location":"user_guide/distributed_training/finetuning/#learning-rate-scaling","title":"Learning rate scaling","text":"<pre><code>trainer:\nlearning_rate_scaling: sqrt\n</code></pre> <p>The base <code>trainer.learning_rate</code> will be scaled up as the number of training workers increases for distributed training. By default the learning rate will scale linearly (<code>linear</code>), but this can be relaxed if you notice catastrophic forgetting is occurring, in which case a softer <code>learning_rate_scaling=sqrt</code> setting may be worth considering.</p>"},{"location":"user_guide/distributed_training/finetuning/#optimizer","title":"Optimizer","text":"<pre><code>trainer:\noptimizer:\ntype: adamw\n</code></pre> <p>We recommend using <code>adamw</code> as the optimizer for fine-tuning.</p> <p>AdamW is typically recommended over more traditional optimizers such as SGD or Adam due to its improved handling of weight decay, which is of particular importance during fine-tuning to avoid catastrophic forgetting.</p>"},{"location":"user_guide/distributed_training/finetuning/#mixed-precision","title":"Mixed Precision","text":"<pre><code>trainer:\nuse_mixed_precision: true\n</code></pre> <p>It's highly recommended to set <code>use_mixed_precision=true</code> when fine-tuning. Empirically, it can speedup training by about 2.5x witout loss of model quality.</p>"},{"location":"user_guide/distributed_training/finetuning/#backend","title":"Backend","text":"<p>Fine-tuning large pretrained models typically benefit from distributed training without requiring a lot of additional hyperparameter tuning. As such, we recommend using the Ray backend in order to take advantage of multi-GPU training and to scale to large datasets.</p> <p>In most cases, the <code>horovod</code> or <code>ddp</code> distributed strategy will work well, but if the model is too large for your GPU type, then you should try model parallelism as described below.</p>"},{"location":"user_guide/distributed_training/finetuning/#model-parallelism-for-llms","title":"Model Parallelism for LLMs","text":"<p>Some large language models have billions of parameters and are too large to train even on the most powerful single GPUs. Some examples include the large variants of:</p> <ul> <li>BLOOM</li> <li>GPT2</li> <li>GPT-J</li> <li>OPT</li> <li>FLAN-T5</li> </ul> <p>For these models, it's recommended to enable the <code>fsdp</code> distributed strategy in a multi-GPU Ray cluster.</p> <p>Example:</p> <pre><code>defaults:\ntext:\nencoder:\ntype: auto_transformer\npretrained_model_name_or_path: bigscience/bloom-3b\ntrainable: true\nbackend:\ntype: ray\ntrainer:\nstrategy: fsdp\n</code></pre>"}]}